{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e99a0f95",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-26T21:42:51.302396Z",
     "iopub.status.busy": "2022-11-26T21:42:51.301571Z",
     "iopub.status.idle": "2022-11-26T21:42:56.783589Z",
     "shell.execute_reply": "2022-11-26T21:42:56.782271Z"
    },
    "papermill": {
     "duration": 5.492303,
     "end_time": "2022-11-26T21:42:56.786756",
     "exception": false,
     "start_time": "2022-11-26T21:42:51.294453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r ../input/recsys-repo/RecSys_Course_AT_PoliMi-master/* ./\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile, os\n",
    "import scipy.sparse as sps\n",
    "import matplotlib.pyplot as pyplot\n",
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample\n",
    "## In order to evaluate put it in a recommender class\n",
    "from Recommenders.BaseRecommender import BaseRecommender\n",
    "from Recommenders.Recommender_utils import check_matrix\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from Recommenders.BaseSimilarityMatrixRecommender import BaseItemSimilarityMatrixRecommender\n",
    "from Utils.seconds_to_biggest_unit import seconds_to_biggest_unit\n",
    "import time, sys\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from lightfm import LightFM\n",
    "URM_path = \"../input/urm-true-binary/URM_True_Binary.csv\"\n",
    "URM_all_dataframe = pd.read_csv(filepath_or_buffer=URM_path, \n",
    "                                sep=\",\",\n",
    "                                dtype={0:int, 1:int, 2:float},\n",
    "                                header=0)\n",
    "URM_all_dataframe.columns = [\"UserID\", \"ItemID\", \"Data\"]\n",
    "URM_all = sps.coo_matrix((URM_all_dataframe[\"Data\"].values, \n",
    "                          (URM_all_dataframe[\"UserID\"].values, URM_all_dataframe[\"ItemID\"].values)))\n",
    "URM_all = URM_all.tocsr() # to obtain fast access to rows (users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "923ae274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T21:42:56.798650Z",
     "iopub.status.busy": "2022-11-26T21:42:56.796305Z",
     "iopub.status.idle": "2022-11-26T21:45:49.199280Z",
     "shell.execute_reply": "2022-11-26T21:45:49.198123Z"
    },
    "papermill": {
     "duration": 172.411071,
     "end_time": "2022-11-26T21:45:49.202073",
     "exception": false,
     "start_time": "2022-11-26T21:42:56.791002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_compile_all_cython: Found 10 Cython files in 4 folders...\r\n",
      "run_compile_all_cython: All files will be compiled using your current python environment: '/opt/conda/bin/python'\r\n",
      "Compiling [1/10]: MatrixFactorizationImpressions_Cython_Epoch.pyx... \r\n",
      "In file included from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[KMatrixFactorizationImpressions_Cython_Epoch.c:746\u001b[m\u001b[K:\r\n",
      "\u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\r\n",
      "   17 | #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\r\n",
      "      |  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\r\n",
      "\u001b[01m\u001b[KMatrixFactorizationImpressions_Cython_Epoch.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[K__pyx_f_43MatrixFactorizationImpressions_Cython_Epoch_32MatrixFactorization_Cython_Epoch_sampleBPR_Cython\u001b[m\u001b[K’:\r\n",
      "\u001b[01m\u001b[KMatrixFactorizationImpressions_Cython_Epoch.c:12758:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K__pyx_v_start_pos_impression_items\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\r\n",
      "12758 |       \u001b[01;35m\u001b[K__pyx_t_4 = (__pyx_v_start_pos_impression_items + __pyx_v_index)\u001b[m\u001b[K;\r\n",
      "      |       \u001b[01;35m\u001b[K~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\r\n",
      "\u001b[01m\u001b[KMatrixFactorizationImpressions_Cython_Epoch.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[K__pyx_pf_43MatrixFactorizationImpressions_Cython_Epoch_32MatrixFactorization_Cython_Epoch_10epochIteration_Cython_ASY_SVD_SGD\u001b[m\u001b[K’:\r\n",
      "\u001b[01m\u001b[KMatrixFactorizationImpressions_Cython_Epoch.c:8736:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K__pyx_v_end_pos_seen_items\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\r\n",
      " 8736 |       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K (__pyx_t_19 = __pyx_v_start_pos_seen_items; __pyx_t_19 < __pyx_t_18; __pyx_t_19+=1) {\r\n",
      "      |       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\r\n",
      "\u001b[01m\u001b[KMatrixFactorizationImpressions_Cython_Epoch.c:8736:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K__pyx_v_start_pos_seen_items\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\r\n",
      "/opt/conda/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /kaggle/working/Recommenders/MatrixFactorization/Cython/MatrixFactorizationImpressions_Cython_Epoch.pyx\r\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\r\n",
      "Compiling [1/10]: MatrixFactorizationImpressions_Cython_Epoch.pyx... PASS\r\n",
      "\r\n",
      "Compiling [2/10]: MatrixFactorization_Cython_Epoch.pyx... \r\n",
      "In file included from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[KMatrixFactorization_Cython_Epoch.c:746\u001b[m\u001b[K:\r\n",
      "\u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\r\n",
      "   17 | #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\r\n",
      "      |  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\r\n",
      "\u001b[01m\u001b[KMatrixFactorization_Cython_Epoch.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[K__pyx_pf_32MatrixFactorization_Cython_Epoch_32MatrixFactorization_Cython_Epoch_10epochIteration_Cython_ASY_SVD_SGD\u001b[m\u001b[K’:\r\n",
      "\u001b[01m\u001b[KMatrixFactorization_Cython_Epoch.c:8669:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K__pyx_v_end_pos_seen_items\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\r\n",
      " 8669 |         \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K (__pyx_t_19 = __pyx_v_start_pos_seen_items; __pyx_t_19 < __pyx_t_18; __pyx_t_19+=1) {\r\n",
      "      |         \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\r\n",
      "\u001b[01m\u001b[KMatrixFactorization_Cython_Epoch.c:8669:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K__pyx_v_start_pos_seen_items\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\r\n",
      "/opt/conda/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /kaggle/working/Recommenders/MatrixFactorization/Cython/MatrixFactorization_Cython_Epoch.pyx\r\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\r\n",
      "Compiling [2/10]: MatrixFactorization_Cython_Epoch.pyx... PASS\r\n",
      "\r\n",
      "Compiling [3/10]: Compute_Similarity_Cython.pyx... \r\n",
      "In file included from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[KCompute_Similarity_Cython.c:746\u001b[m\u001b[K:\r\n",
      "\u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\r\n",
      "   17 | #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\r\n",
      "      |  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\r\n",
      "/opt/conda/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /kaggle/working/Recommenders/Similarity/Cython/Compute_Similarity_Cython.pyx\r\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\r\n",
      "Compiling [3/10]: Compute_Similarity_Cython.pyx... PASS\r\n",
      "\r\n",
      "Compiling [4/10]: SLIM_BPR_Cython_Epoch.pyx... \r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:633:34: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:633:66: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:818:35: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:818:52: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:917:35: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:917:53: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:917:69: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:1052:35: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:1052:42: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:1053:35: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:1053:53: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "In file included from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[KSLIM_BPR_Cython_Epoch.c:746\u001b[m\u001b[K:\r\n",
      "\u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\r\n",
      "   17 | #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\r\n",
      "      |  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\r\n",
      "\u001b[01m\u001b[KSLIM_BPR_Cython_Epoch.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[K__pyx_f_21SLIM_BPR_Cython_Epoch_22Sparse_Matrix_Tree_CSR_test_list_tee_conversion\u001b[m\u001b[K’:\r\n",
      "\u001b[01m\u001b[KSLIM_BPR_Cython_Epoch.c:10848:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K__pyx_v_previous_element\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\r\n",
      "10848 |     \u001b[01;35m\u001b[K__pyx_t_7 = __pyx_v_current_element->lower\u001b[m\u001b[K;\r\n",
      "      |     \u001b[01;35m\u001b[K~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\r\n",
      "/opt/conda/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /kaggle/working/Recommenders/SLIM/Cython/SLIM_BPR_Cython_Epoch.pyx\r\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:633:34: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:633:66: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:818:35: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:818:52: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:917:35: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:917:53: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:917:69: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:1052:35: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:1052:42: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:1053:35: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: SLIM_BPR_Cython_Epoch.pyx:1053:53: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "Compiling [4/10]: SLIM_BPR_Cython_Epoch.pyx... PASS\r\n",
      "\r\n",
      "Compiling [5/10]: Triangular_Matrix.pyx... \r\n",
      "In file included from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[KTriangular_Matrix.c:746\u001b[m\u001b[K:\r\n",
      "\u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\r\n",
      "   17 | #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\r\n",
      "      |  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\r\n",
      "/opt/conda/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /kaggle/working/Recommenders/SLIM/Cython/Triangular_Matrix.pyx\r\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\r\n",
      "Compiling [5/10]: Triangular_Matrix.pyx... PASS\r\n",
      "\r\n",
      "Compiling [6/10]: Sparse_Matrix_Tree_CSR.pyx... \r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:132:34: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:132:66: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:343:35: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:343:52: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:442:35: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:442:53: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:442:69: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:577:35: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:577:42: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:578:35: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:578:53: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "In file included from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[KSparse_Matrix_Tree_CSR.c:746\u001b[m\u001b[K:\r\n",
      "\u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\r\n",
      "   17 | #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\r\n",
      "      |  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\r\n",
      "\u001b[01m\u001b[KSparse_Matrix_Tree_CSR.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[K__pyx_f_22Sparse_Matrix_Tree_CSR_22Sparse_Matrix_Tree_CSR_test_list_tree_conversion\u001b[m\u001b[K’:\r\n",
      "\u001b[01m\u001b[KSparse_Matrix_Tree_CSR.c:5844:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K__pyx_v_previous_element\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\r\n",
      " 5844 |     \u001b[01;35m\u001b[K__pyx_t_7 = __pyx_v_current_element->lower\u001b[m\u001b[K;\r\n",
      "      |     \u001b[01;35m\u001b[K~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\r\n",
      "/opt/conda/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /kaggle/working/Recommenders/SLIM/Cython/Sparse_Matrix_Tree_CSR.pyx\r\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:132:34: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:132:66: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:343:35: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:343:52: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:442:35: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:442:53: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:442:69: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:577:35: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:577:42: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:578:35: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "warning: Sparse_Matrix_Tree_CSR.pyx:578:53: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\r\n",
      "Compiling [6/10]: Sparse_Matrix_Tree_CSR.pyx... PASS\r\n",
      "\r\n",
      "Compiling [7/10]: CFW_D_Similarity_Cython_SGD.pyx... \r\n",
      "In file included from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[KCFW_D_Similarity_Cython_SGD.c:746\u001b[m\u001b[K:\r\n",
      "\u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\r\n",
      "   17 | #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\r\n",
      "      |  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\r\n",
      "\u001b[01m\u001b[KCFW_D_Similarity_Cython_SGD.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[K__pyx_pf_27CFW_D_Similarity_Cython_SGD_27CFW_D_Similarity_Cython_SGD_6fit\u001b[m\u001b[K’:\r\n",
      "\u001b[01m\u001b[KCFW_D_Similarity_Cython_SGD.c:6056:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K__pyx_v_sample_num\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\r\n",
      " 6056 |   __pyx_t_3 = PyFloat_FromDouble((__pyx_v_cum_loss / \u001b[01;35m\u001b[K((double)__pyx_v_sample_num)\u001b[m\u001b[K)); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 290, __pyx_L1_error)\r\n",
      "      |                                                      \u001b[01;35m\u001b[K~^~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\r\n",
      "/opt/conda/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /kaggle/working/Recommenders/FeatureWeighting/Cython/CFW_D_Similarity_Cython_SGD.pyx\r\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\r\n",
      "Compiling [7/10]: CFW_D_Similarity_Cython_SGD.pyx... PASS\r\n",
      "\r\n",
      "Compiling [8/10]: HP3_Similarity_Cython_SGD.pyx... \r\n",
      "warning: HP3_Similarity_Cython_SGD.pyx:113:40: Index should be typed for more efficient access\r\n",
      "In file included from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[KHP3_Similarity_Cython_SGD.c:746\u001b[m\u001b[K:\r\n",
      "\u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\r\n",
      "   17 | #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\r\n",
      "      |  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\r\n",
      "\u001b[01m\u001b[KHP3_Similarity_Cython_SGD.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[K__pyx_pf_25HP3_Similarity_Cython_SGD_25HP3_Similarity_Cython_SGD_4fit\u001b[m\u001b[K’:\r\n",
      "\u001b[01m\u001b[KHP3_Similarity_Cython_SGD.c:6303:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K__pyx_v_sample_num\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\r\n",
      " 6303 |   __pyx_t_1 = PyFloat_FromDouble((__pyx_v_cum_loss / \u001b[01;35m\u001b[K((double)__pyx_v_sample_num)\u001b[m\u001b[K)); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 291, __pyx_L1_error)\r\n",
      "      |                                                      \u001b[01;35m\u001b[K~^~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\r\n",
      "/opt/conda/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /kaggle/working/Recommenders/FeatureWeighting/Cython/HP3_Similarity_Cython_SGD.pyx\r\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\r\n",
      "warning: HP3_Similarity_Cython_SGD.pyx:113:40: Index should be typed for more efficient access\r\n",
      "Compiling [8/10]: HP3_Similarity_Cython_SGD.pyx... PASS\r\n",
      "\r\n",
      "Compiling [9/10]: FBSM_Rating_Cython_SGD.pyx... \r\n",
      "In file included from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[KFBSM_Rating_Cython_SGD.c:746\u001b[m\u001b[K:\r\n",
      "\u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\r\n",
      "   17 | #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\r\n",
      "      |  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\r\n",
      "\u001b[01m\u001b[KFBSM_Rating_Cython_SGD.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[K__pyx_pf_22FBSM_Rating_Cython_SGD_22FBSM_Rating_Cython_SGD_2fit\u001b[m\u001b[K’:\r\n",
      "\u001b[01m\u001b[KFBSM_Rating_Cython_SGD.c:9031:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K__pyx_v_num_sample\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\r\n",
      " 9031 |   __pyx_t_5 = PyFloat_FromDouble((__pyx_v_cum_loss / \u001b[01;35m\u001b[K((double)__pyx_v_num_sample)\u001b[m\u001b[K)); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 551, __pyx_L1_error)\r\n",
      "      |                                                      \u001b[01;35m\u001b[K~^~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\r\n",
      "/opt/conda/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /kaggle/working/Recommenders/FeatureWeighting/Cython/FBSM_Rating_Cython_SGD.pyx\r\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\r\n",
      "Compiling [9/10]: FBSM_Rating_Cython_SGD.pyx... PASS\r\n",
      "\r\n",
      "Compiling [10/10]: CFW_DVV_Similarity_Cython_SGD.pyx... \r\n",
      "In file included from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\r\n",
      "                 from \u001b[01m\u001b[KCFW_DVV_Similarity_Cython_SGD.c:746\u001b[m\u001b[K:\r\n",
      "\u001b[01m\u001b[K/opt/conda/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\r\n",
      "   17 | #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\r\n",
      "      |  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\r\n",
      "/opt/conda/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /kaggle/working/Recommenders/FeatureWeighting/Cython/CFW_DVV_Similarity_Cython_SGD.pyx\r\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\r\n",
      "Compiling [10/10]: CFW_DVV_Similarity_Cython_SGD.pyx... PASS\r\n",
      "\r\n",
      "run_compile_all_cython: Compilation finished. SUCCESS.\r\n",
      "Compilation log can be found here: './result_experiments/run_compile_all_cython.txt'\r\n"
     ]
    }
   ],
   "source": [
    "!python run_compile_all_cython.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9baf889d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T21:45:49.218764Z",
     "iopub.status.busy": "2022-11-26T21:45:49.218289Z",
     "iopub.status.idle": "2022-11-26T21:45:52.484169Z",
     "shell.execute_reply": "2022-11-26T21:45:52.483127Z"
    },
    "papermill": {
     "duration": 3.277839,
     "end_time": "2022-11-26T21:45:52.487138",
     "exception": false,
     "start_time": "2022-11-26T21:45:49.209299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: W_sparse.npz (deflated 0%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r -j /kaggle/working/SLIMElasticNetRecommender.zip /kaggle/input/slimurmbinary/W_sparse.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d699ed11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T21:45:52.504091Z",
     "iopub.status.busy": "2022-11-26T21:45:52.503142Z",
     "iopub.status.idle": "2022-11-26T21:45:57.774148Z",
     "shell.execute_reply": "2022-11-26T21:45:57.773251Z"
    },
    "papermill": {
     "duration": 5.282094,
     "end_time": "2022-11-26T21:45:57.776562",
     "exception": false,
     "start_time": "2022-11-26T21:45:52.494468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 319 (0.77 %) of 41629 users have no sampled items\n",
      "Warning: 1 (0.00 %) of 41629 users have no train items\n",
      "Warning: 779 (1.87 %) of 41629 users have no sampled items\n",
      "EvaluatorHoldout: Ignoring 779 ( 1.9%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 319 ( 0.8%) Users that have less than 1 test interactions\n"
     ]
    }
   ],
   "source": [
    "URM_train_validation, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.8)\n",
    "URM_train, URM_validation = split_train_in_two_percentage_global_sample(URM_train_validation, train_percentage = 0.8)\n",
    "\n",
    "evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=[10])\n",
    "evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a16c9c44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T21:45:57.793851Z",
     "iopub.status.busy": "2022-11-26T21:45:57.793383Z",
     "iopub.status.idle": "2022-11-26T21:45:57.804676Z",
     "shell.execute_reply": "2022-11-26T21:45:57.803378Z"
    },
    "papermill": {
     "duration": 0.023104,
     "end_time": "2022-11-26T21:45:57.807330",
     "exception": false,
     "start_time": "2022-11-26T21:45:57.784226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([41, 15, 64, ..., 17, 31, 19], dtype=int32), (41629,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_length = np.ediff1d(sps.csr_matrix(URM_train).indptr)\n",
    "profile_length, profile_length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb2a672a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T21:45:57.824683Z",
     "iopub.status.busy": "2022-11-26T21:45:57.824239Z",
     "iopub.status.idle": "2022-11-26T21:45:57.831338Z",
     "shell.execute_reply": "2022-11-26T21:45:57.830498Z"
    },
    "papermill": {
     "duration": 0.018148,
     "end_time": "2022-11-26T21:45:57.833310",
     "exception": false,
     "start_time": "2022-11-26T21:45:57.815162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4162"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = int(len(profile_length)*0.10)\n",
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c8461cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T21:45:57.851269Z",
     "iopub.status.busy": "2022-11-26T21:45:57.850371Z",
     "iopub.status.idle": "2022-11-26T21:45:57.859569Z",
     "shell.execute_reply": "2022-11-26T21:45:57.858695Z"
    },
    "papermill": {
     "duration": 0.02051,
     "end_time": "2022-11-26T21:45:57.861633",
     "exception": false,
     "start_time": "2022-11-26T21:45:57.841123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25061, 28744, 36572, ..., 19407,  8693, 12454])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_users = np.argsort(profile_length)\n",
    "sorted_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1172cd6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T21:45:57.881444Z",
     "iopub.status.busy": "2022-11-26T21:45:57.880255Z",
     "iopub.status.idle": "2022-11-26T21:45:57.893578Z",
     "shell.execute_reply": "2022-11-26T21:45:57.891793Z"
    },
    "papermill": {
     "duration": 0.024994,
     "end_time": "2022-11-26T21:45:57.896072",
     "exception": false,
     "start_time": "2022-11-26T21:45:57.871078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0, #users in group 4162, average p.len 8.47, median 9.0, min 0, max 10\n",
      "Group 1, #users in group 4162, average p.len 11.47, median 12.0, min 10, max 13\n",
      "Group 2, #users in group 4162, average p.len 13.48, median 13.0, min 13, max 14\n",
      "Group 3, #users in group 4162, average p.len 15.34, median 15.0, min 14, max 16\n",
      "Group 4, #users in group 4162, average p.len 17.32, median 17.0, min 16, max 18\n",
      "Group 5, #users in group 4162, average p.len 19.74, median 20.0, min 18, max 21\n",
      "Group 6, #users in group 4162, average p.len 22.84, median 23.0, min 21, max 25\n",
      "Group 7, #users in group 4162, average p.len 27.41, median 27.0, min 25, max 31\n",
      "Group 8, #users in group 4162, average p.len 35.56, median 35.0, min 31, max 42\n",
      "Group 9, #users in group 4162, average p.len 66.30, median 55.0, min 42, max 380\n"
     ]
    }
   ],
   "source": [
    "for group_id in range(0, 10):\n",
    "    start_pos = group_id * block_size\n",
    "    end_pos = min((group_id+1) * block_size, len(profile_length))\n",
    "    \n",
    "    users_in_group = sorted_users[start_pos:end_pos]\n",
    "    \n",
    "    users_in_group_p_len = profile_length[users_in_group]\n",
    "    \n",
    "    print(\"Group {}, #users in group {}, average p.len {:.2f}, median {}, min {}, max {}\".format(\n",
    "        group_id, \n",
    "        users_in_group.shape[0],\n",
    "        users_in_group_p_len.mean(),\n",
    "        np.median(users_in_group_p_len),\n",
    "        users_in_group_p_len.min(),\n",
    "        users_in_group_p_len.max()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02057543",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T21:45:57.914989Z",
     "iopub.status.busy": "2022-11-26T21:45:57.914539Z",
     "iopub.status.idle": "2022-11-26T21:45:57.938601Z",
     "shell.execute_reply": "2022-11-26T21:45:57.937401Z"
    },
    "papermill": {
     "duration": 0.037165,
     "end_time": "2022-11-26T21:45:57.941345",
     "exception": false,
     "start_time": "2022-11-26T21:45:57.904180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SLIMElasticNetRecommender(BaseItemSimilarityMatrixRecommender):\n",
    "    \"\"\"\n",
    "    Train a Sparse Linear Methods (SLIM) item similarity model.\n",
    "    NOTE: ElasticNet solver is parallel, a single intance of SLIM_ElasticNet will\n",
    "          make use of half the cores available\n",
    "    See:\n",
    "        Efficient Top-N Recommendation by Linear Regression,\n",
    "        M. Levy and K. Jack, LSRS workshop at RecSys 2013.\n",
    "        SLIM: Sparse linear methods for top-n recommender systems,\n",
    "        X. Ning and G. Karypis, ICDM 2011.\n",
    "        http://glaros.dtc.umn.edu/gkhome/fetch/papers/SLIM2011icdm.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    RECOMMENDER_NAME = \"SLIMElasticNetRecommender\"\n",
    "\n",
    "    def __init__(self, URM_train, verbose = True):\n",
    "        super(SLIMElasticNetRecommender, self).__init__(URM_train, verbose = verbose)\n",
    "\n",
    "    @ignore_warnings(category=ConvergenceWarning)\n",
    "    def fit(self, l1_ratio=0.1, alpha = 1.0, positive_only=True, topK = 100,**earlystopping_kwargs):\n",
    "\n",
    "        assert l1_ratio>= 0 and l1_ratio<=1, \"{}: l1_ratio must be between 0 and 1, provided value was {}\".format(self.RECOMMENDER_NAME, l1_ratio)\n",
    "\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.positive_only = positive_only\n",
    "        self.topK = topK\n",
    "\n",
    "\n",
    "        # initialize the ElasticNet model\n",
    "        self.model = ElasticNet(alpha=alpha,\n",
    "                                l1_ratio=self.l1_ratio,\n",
    "                                positive=self.positive_only,\n",
    "                                fit_intercept=False,\n",
    "                                copy_X=False,\n",
    "                                precompute=True,\n",
    "                                selection='random',\n",
    "                                max_iter=100,\n",
    "                                tol=1e-4)\n",
    "\n",
    "        URM_train = check_matrix(self.URM_train, 'csc', dtype=np.float32)\n",
    "\n",
    "        n_items = URM_train.shape[1]\n",
    "\n",
    "        # Use array as it reduces memory requirements compared to lists\n",
    "        dataBlock = 10000000\n",
    "\n",
    "        rows = np.zeros(dataBlock, dtype=np.int32)\n",
    "        cols = np.zeros(dataBlock, dtype=np.int32)\n",
    "        values = np.zeros(dataBlock, dtype=np.float32)\n",
    "\n",
    "        numCells = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_time_printBatch = start_time\n",
    "\n",
    "        # fit each item's factors sequentially (not in parallel)\n",
    "        for currentItem in range(n_items):\n",
    "\n",
    "            # get the target column\n",
    "            y = URM_train[:, currentItem].toarray()\n",
    "\n",
    "            # set the j-th column of X to zero\n",
    "            start_pos = URM_train.indptr[currentItem]\n",
    "            end_pos = URM_train.indptr[currentItem + 1]\n",
    "\n",
    "            current_item_data_backup = URM_train.data[start_pos: end_pos].copy()\n",
    "            URM_train.data[start_pos: end_pos] = 0.0\n",
    "\n",
    "            # fit one ElasticNet model per column\n",
    "            self.model.fit(URM_train, y)\n",
    "\n",
    "            # self.model.coef_ contains the coefficient of the ElasticNet model\n",
    "            # let's keep only the non-zero values\n",
    "\n",
    "            # Select topK values\n",
    "            # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "            # - Partition the data to extract the set of relevant items\n",
    "            # - Sort only the relevant items\n",
    "            # - Get the original item index\n",
    "\n",
    "            nonzero_model_coef_index = self.model.sparse_coef_.indices\n",
    "            nonzero_model_coef_value = self.model.sparse_coef_.data\n",
    "\n",
    "            local_topK = min(len(nonzero_model_coef_value)-1, self.topK)\n",
    "\n",
    "            relevant_items_partition = (-nonzero_model_coef_value).argpartition(local_topK)[0:local_topK]\n",
    "            relevant_items_partition_sorting = np.argsort(-nonzero_model_coef_value[relevant_items_partition])\n",
    "            ranking = relevant_items_partition[relevant_items_partition_sorting]\n",
    "\n",
    "            for index in range(len(ranking)):\n",
    "\n",
    "                if numCells == len(rows):\n",
    "                    rows = np.concatenate((rows, np.zeros(dataBlock, dtype=np.int32)))\n",
    "                    cols = np.concatenate((cols, np.zeros(dataBlock, dtype=np.int32)))\n",
    "                    values = np.concatenate((values, np.zeros(dataBlock, dtype=np.float32)))\n",
    "\n",
    "\n",
    "                rows[numCells] = nonzero_model_coef_index[ranking[index]]\n",
    "                cols[numCells] = currentItem\n",
    "                values[numCells] = nonzero_model_coef_value[ranking[index]]\n",
    "\n",
    "                numCells += 1\n",
    "\n",
    "            # finally, replace the original values of the j-th column\n",
    "            URM_train.data[start_pos:end_pos] = current_item_data_backup\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            new_time_value, new_time_unit = seconds_to_biggest_unit(elapsed_time)\n",
    "\n",
    "\n",
    "            if time.time() - start_time_printBatch > 300 or currentItem == n_items-1:\n",
    "                self._print(\"Processed {} ({:4.1f}%) in {:.2f} {}. Items per second: {:.2f}\".format(\n",
    "                    currentItem+1,\n",
    "                    100.0* float(currentItem+1)/n_items,\n",
    "                    new_time_value,\n",
    "                    new_time_unit,\n",
    "                    float(currentItem)/elapsed_time))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time_printBatch = time.time()\n",
    "\n",
    "        # generate the sparse weight matrix\n",
    "        self.W_sparse = sps.csr_matrix((values[:numCells], (rows[:numCells], cols[:numCells])),\n",
    "                                       shape=(n_items, n_items), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82e3183f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T21:45:57.959889Z",
     "iopub.status.busy": "2022-11-26T21:45:57.959171Z",
     "iopub.status.idle": "2022-11-26T21:59:35.346156Z",
     "shell.execute_reply": "2022-11-26T21:59:35.344510Z"
    },
    "papermill": {
     "duration": 817.39947,
     "end_time": "2022-11-26T21:59:35.348982",
     "exception": false,
     "start_time": "2022-11-26T21:45:57.949512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLIMElasticNetRecommender: URM Detected 1 ( 0.0%) users with no interactions.\n",
      "SLIMElasticNetRecommender: Loading model from file '/kaggle/workingSLIMElasticNetRecommender'\n",
      "SLIMElasticNetRecommender: Loading complete\n",
      "TopPopRecommender: URM Detected 1 ( 0.0%) users with no interactions.\n",
      "UserKNNCFRecommender: URM Detected 1 ( 0.0%) users with no interactions.\n",
      "Similarity column 41629 (100.0%), 2039.72 column/sec. Elapsed time 20.41 sec\n",
      "ItemKNNCFRecommender: URM Detected 1 ( 0.0%) users with no interactions.\n",
      "Similarity column 24507 (100.0%), 4557.15 column/sec. Elapsed time 5.38 sec\n",
      "P3alphaRecommender: URM Detected 1 ( 0.0%) users with no interactions.\n",
      "P3alphaRecommender: Similarity column 24507 (100.0%), 2342.60 column/sec. Elapsed time 10.46 sec\n",
      "RP3betaRecommender: URM Detected 1 ( 0.0%) users with no interactions.\n",
      "RP3betaRecommender: Similarity column 24507 (100.0%), 2115.69 column/sec. Elapsed time 11.58 sec\n",
      "PureSVDRecommender: URM Detected 1 ( 0.0%) users with no interactions.\n",
      "PureSVDRecommender: Computing SVD decomposition...\n",
      "PureSVDRecommender: Computing SVD decomposition... done in 3.80 sec\n",
      "NMFRecommender: URM Detected 1 ( 0.0%) users with no interactions.\n",
      "NMFRecommender: Computing NMF decomposition...\n",
      "NMFRecommender: Computing NMF decomposition... done in 2.47 min\n",
      "MatrixFactorization_FunkSVD_Cython_Recommender: URM Detected 1 ( 0.0%) users with no interactions.\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.40 sec. MSE loss 4.35E-01. Sample per second: 415022\n",
      "FUNK_SVD: Epoch 1 of 300. Elapsed time 1.63 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.01 sec. MSE loss 6.02E-02. Sample per second: 495970\n",
      "FUNK_SVD: Epoch 2 of 300. Elapsed time 3.24 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.63 sec. MSE loss 9.10E-03. Sample per second: 610701\n",
      "FUNK_SVD: Epoch 3 of 300. Elapsed time 4.86 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.22 sec. MSE loss 2.11E-03. Sample per second: 447373\n",
      "FUNK_SVD: Epoch 4 of 300. Elapsed time 6.46 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.84 sec. MSE loss 1.16E-03. Sample per second: 541592\n",
      "FUNK_SVD: Epoch 5 of 300. Elapsed time 8.07 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.40 sec. MSE loss 1.03E-03. Sample per second: 415318\n",
      "FUNK_SVD: Epoch 6 of 300. Elapsed time 9.63 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.97 sec. MSE loss 1.01E-03. Sample per second: 504276\n",
      "FUNK_SVD: Epoch 7 of 300. Elapsed time 11.20 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.62 sec. MSE loss 1.01E-03. Sample per second: 379914\n",
      "FUNK_SVD: Epoch 8 of 300. Elapsed time 12.85 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.34 sec. MSE loss 1.01E-03. Sample per second: 424799\n",
      "FUNK_SVD: Epoch 9 of 300. Elapsed time 14.58 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.03 sec. MSE loss 1.01E-03. Sample per second: 490185\n",
      "FUNK_SVD: Epoch 10 of 300. Elapsed time 16.26 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.68 sec. MSE loss 1.01E-03. Sample per second: 592080\n",
      "FUNK_SVD: Epoch 11 of 300. Elapsed time 17.91 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.31 sec. MSE loss 1.01E-03. Sample per second: 430524\n",
      "FUNK_SVD: Epoch 12 of 300. Elapsed time 19.54 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.97 sec. MSE loss 1.01E-03. Sample per second: 506186\n",
      "FUNK_SVD: Epoch 13 of 300. Elapsed time 21.20 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.67 sec. MSE loss 1.01E-03. Sample per second: 372655\n",
      "FUNK_SVD: Epoch 14 of 300. Elapsed time 22.90 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.50 sec. MSE loss 1.01E-03. Sample per second: 398325\n",
      "FUNK_SVD: Epoch 15 of 300. Elapsed time 24.73 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.19 sec. MSE loss 1.01E-03. Sample per second: 455267\n",
      "FUNK_SVD: Epoch 16 of 300. Elapsed time 26.42 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.77 sec. MSE loss 1.01E-03. Sample per second: 560569\n",
      "FUNK_SVD: Epoch 17 of 300. Elapsed time 28.01 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.40 sec. MSE loss 1.01E-03. Sample per second: 415108\n",
      "FUNK_SVD: Epoch 18 of 300. Elapsed time 29.63 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.03 sec. MSE loss 1.01E-03. Sample per second: 490710\n",
      "FUNK_SVD: Epoch 19 of 300. Elapsed time 31.26 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.62 sec. MSE loss 1.01E-03. Sample per second: 615174\n",
      "FUNK_SVD: Epoch 20 of 300. Elapsed time 32.85 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.30 sec. MSE loss 1.00E-03. Sample per second: 432682\n",
      "FUNK_SVD: Epoch 21 of 300. Elapsed time 34.53 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.92 sec. MSE loss 1.01E-03. Sample per second: 516932\n",
      "FUNK_SVD: Epoch 22 of 300. Elapsed time 36.16 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.53 sec. MSE loss 1.01E-03. Sample per second: 392501\n",
      "FUNK_SVD: Epoch 23 of 300. Elapsed time 37.77 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.18 sec. MSE loss 1.00E-03. Sample per second: 456864\n",
      "FUNK_SVD: Epoch 24 of 300. Elapsed time 39.41 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.84 sec. MSE loss 1.00E-03. Sample per second: 541203\n",
      "FUNK_SVD: Epoch 25 of 300. Elapsed time 41.07 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.49 sec. MSE loss 1.01E-03. Sample per second: 399566\n",
      "FUNK_SVD: Epoch 26 of 300. Elapsed time 42.72 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.16 sec. MSE loss 1.01E-03. Sample per second: 461354\n",
      "FUNK_SVD: Epoch 27 of 300. Elapsed time 44.39 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.79 sec. MSE loss 1.01E-03. Sample per second: 557171\n",
      "FUNK_SVD: Epoch 28 of 300. Elapsed time 46.02 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.36 sec. MSE loss 1.00E-03. Sample per second: 422437\n",
      "FUNK_SVD: Epoch 29 of 300. Elapsed time 47.59 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.98 sec. MSE loss 1.01E-03. Sample per second: 501392\n",
      "FUNK_SVD: Epoch 30 of 300. Elapsed time 49.22 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.59 sec. MSE loss 1.01E-03. Sample per second: 384239\n",
      "FUNK_SVD: Epoch 31 of 300. Elapsed time 50.82 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.22 sec. MSE loss 1.01E-03. Sample per second: 448771\n",
      "FUNK_SVD: Epoch 32 of 300. Elapsed time 52.45 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.93 sec. MSE loss 1.01E-03. Sample per second: 516565\n",
      "FUNK_SVD: Epoch 33 of 300. Elapsed time 54.16 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.68 sec. MSE loss 1.01E-03. Sample per second: 371453\n",
      "FUNK_SVD: Epoch 34 of 300. Elapsed time 55.91 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.39 sec. MSE loss 1.00E-03. Sample per second: 417154\n",
      "FUNK_SVD: Epoch 35 of 300. Elapsed time 57.62 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.99 sec. MSE loss 1.01E-03. Sample per second: 501160\n",
      "FUNK_SVD: Epoch 36 of 300. Elapsed time 59.22 sec\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.56 sec. MSE loss 1.01E-03. Sample per second: 389150\n",
      "FUNK_SVD: Epoch 37 of 300. Elapsed time 1.01 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.14 sec. MSE loss 1.01E-03. Sample per second: 465716\n",
      "FUNK_SVD: Epoch 38 of 300. Elapsed time 1.04 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.75 sec. MSE loss 1.00E-03. Sample per second: 569399\n",
      "FUNK_SVD: Epoch 39 of 300. Elapsed time 1.07 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.33 sec. MSE loss 1.01E-03. Sample per second: 426368\n",
      "FUNK_SVD: Epoch 40 of 300. Elapsed time 1.09 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.93 sec. MSE loss 1.01E-03. Sample per second: 516755\n",
      "FUNK_SVD: Epoch 41 of 300. Elapsed time 1.12 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.51 sec. MSE loss 1.01E-03. Sample per second: 396563\n",
      "FUNK_SVD: Epoch 42 of 300. Elapsed time 1.15 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.09 sec. MSE loss 1.01E-03. Sample per second: 476473\n",
      "FUNK_SVD: Epoch 43 of 300. Elapsed time 1.17 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.71 sec. MSE loss 1.01E-03. Sample per second: 580836\n",
      "FUNK_SVD: Epoch 44 of 300. Elapsed time 1.20 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.35 sec. MSE loss 1.01E-03. Sample per second: 423961\n",
      "FUNK_SVD: Epoch 45 of 300. Elapsed time 1.23 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.99 sec. MSE loss 1.01E-03. Sample per second: 499796\n",
      "FUNK_SVD: Epoch 46 of 300. Elapsed time 1.25 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.61 sec. MSE loss 1.01E-03. Sample per second: 381038\n",
      "FUNK_SVD: Epoch 47 of 300. Elapsed time 1.28 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.25 sec. MSE loss 1.01E-03. Sample per second: 441532\n",
      "FUNK_SVD: Epoch 48 of 300. Elapsed time 1.31 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.88 sec. MSE loss 1.01E-03. Sample per second: 527939\n",
      "FUNK_SVD: Epoch 49 of 300. Elapsed time 1.34 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.48 sec. MSE loss 1.01E-03. Sample per second: 400675\n",
      "FUNK_SVD: Epoch 50 of 300. Elapsed time 1.36 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.09 sec. MSE loss 1.01E-03. Sample per second: 475199\n",
      "FUNK_SVD: Epoch 51 of 300. Elapsed time 1.39 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.87 sec. MSE loss 1.01E-03. Sample per second: 532825\n",
      "FUNK_SVD: Epoch 52 of 300. Elapsed time 1.42 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.61 sec. MSE loss 1.01E-03. Sample per second: 381400\n",
      "FUNK_SVD: Epoch 53 of 300. Elapsed time 1.45 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.30 sec. MSE loss 1.01E-03. Sample per second: 432424\n",
      "FUNK_SVD: Epoch 54 of 300. Elapsed time 1.48 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.96 sec. MSE loss 1.00E-03. Sample per second: 508606\n",
      "FUNK_SVD: Epoch 55 of 300. Elapsed time 1.50 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.51 sec. MSE loss 1.01E-03. Sample per second: 395797\n",
      "FUNK_SVD: Epoch 56 of 300. Elapsed time 1.53 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.12 sec. MSE loss 1.01E-03. Sample per second: 468743\n",
      "FUNK_SVD: Epoch 57 of 300. Elapsed time 1.56 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.76 sec. MSE loss 1.01E-03. Sample per second: 565497\n",
      "FUNK_SVD: Epoch 58 of 300. Elapsed time 1.58 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.35 sec. MSE loss 1.00E-03. Sample per second: 423711\n",
      "FUNK_SVD: Epoch 59 of 300. Elapsed time 1.61 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.95 sec. MSE loss 1.01E-03. Sample per second: 509212\n",
      "FUNK_SVD: Epoch 60 of 300. Elapsed time 1.64 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.59 sec. MSE loss 1.01E-03. Sample per second: 384120\n",
      "FUNK_SVD: Epoch 61 of 300. Elapsed time 1.66 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.24 sec. MSE loss 1.01E-03. Sample per second: 443388\n",
      "FUNK_SVD: Epoch 62 of 300. Elapsed time 1.69 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.90 sec. MSE loss 1.01E-03. Sample per second: 522626\n",
      "FUNK_SVD: Epoch 63 of 300. Elapsed time 1.72 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.58 sec. MSE loss 1.00E-03. Sample per second: 385773\n",
      "FUNK_SVD: Epoch 64 of 300. Elapsed time 1.75 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.16 sec. MSE loss 1.01E-03. Sample per second: 461180\n",
      "FUNK_SVD: Epoch 65 of 300. Elapsed time 1.77 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.78 sec. MSE loss 1.01E-03. Sample per second: 560033\n",
      "FUNK_SVD: Epoch 66 of 300. Elapsed time 1.80 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.43 sec. MSE loss 1.01E-03. Sample per second: 409795\n",
      "FUNK_SVD: Epoch 67 of 300. Elapsed time 1.83 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.05 sec. MSE loss 1.01E-03. Sample per second: 484366\n",
      "FUNK_SVD: Epoch 68 of 300. Elapsed time 1.85 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.63 sec. MSE loss 1.01E-03. Sample per second: 611178\n",
      "FUNK_SVD: Epoch 69 of 300. Elapsed time 1.88 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.27 sec. MSE loss 1.00E-03. Sample per second: 437905\n",
      "FUNK_SVD: Epoch 70 of 300. Elapsed time 1.91 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.94 sec. MSE loss 1.01E-03. Sample per second: 513474\n",
      "FUNK_SVD: Epoch 71 of 300. Elapsed time 1.94 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.53 sec. MSE loss 1.01E-03. Sample per second: 393318\n",
      "FUNK_SVD: Epoch 72 of 300. Elapsed time 1.96 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.12 sec. MSE loss 1.01E-03. Sample per second: 470025\n",
      "FUNK_SVD: Epoch 73 of 300. Elapsed time 1.99 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.70 sec. MSE loss 1.00E-03. Sample per second: 583491\n",
      "FUNK_SVD: Epoch 74 of 300. Elapsed time 2.02 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.34 sec. MSE loss 1.01E-03. Sample per second: 425679\n",
      "FUNK_SVD: Epoch 75 of 300. Elapsed time 2.04 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.02 sec. MSE loss 1.01E-03. Sample per second: 493180\n",
      "FUNK_SVD: Epoch 76 of 300. Elapsed time 2.07 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.68 sec. MSE loss 1.01E-03. Sample per second: 593723\n",
      "FUNK_SVD: Epoch 77 of 300. Elapsed time 2.10 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.30 sec. MSE loss 1.00E-03. Sample per second: 433237\n",
      "FUNK_SVD: Epoch 78 of 300. Elapsed time 2.13 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.90 sec. MSE loss 1.01E-03. Sample per second: 524827\n",
      "FUNK_SVD: Epoch 79 of 300. Elapsed time 2.15 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.54 sec. MSE loss 1.01E-03. Sample per second: 392390\n",
      "FUNK_SVD: Epoch 80 of 300. Elapsed time 2.18 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.13 sec. MSE loss 1.01E-03. Sample per second: 467136\n",
      "FUNK_SVD: Epoch 81 of 300. Elapsed time 2.21 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.78 sec. MSE loss 1.01E-03. Sample per second: 560344\n",
      "FUNK_SVD: Epoch 82 of 300. Elapsed time 2.23 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.42 sec. MSE loss 1.01E-03. Sample per second: 410583\n",
      "FUNK_SVD: Epoch 83 of 300. Elapsed time 2.26 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.04 sec. MSE loss 1.00E-03. Sample per second: 487197\n",
      "FUNK_SVD: Epoch 84 of 300. Elapsed time 2.29 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.69 sec. MSE loss 1.01E-03. Sample per second: 589716\n",
      "FUNK_SVD: Epoch 85 of 300. Elapsed time 2.32 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.26 sec. MSE loss 1.01E-03. Sample per second: 439705\n",
      "FUNK_SVD: Epoch 86 of 300. Elapsed time 2.34 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.85 sec. MSE loss 1.01E-03. Sample per second: 537304\n",
      "FUNK_SVD: Epoch 87 of 300. Elapsed time 2.37 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.46 sec. MSE loss 1.01E-03. Sample per second: 404067\n",
      "FUNK_SVD: Epoch 88 of 300. Elapsed time 2.39 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.08 sec. MSE loss 1.01E-03. Sample per second: 478100\n",
      "FUNK_SVD: Epoch 89 of 300. Elapsed time 2.42 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.77 sec. MSE loss 1.01E-03. Sample per second: 563595\n",
      "FUNK_SVD: Epoch 90 of 300. Elapsed time 2.45 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.46 sec. MSE loss 1.01E-03. Sample per second: 404565\n",
      "FUNK_SVD: Epoch 91 of 300. Elapsed time 2.48 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.21 sec. MSE loss 1.01E-03. Sample per second: 450585\n",
      "FUNK_SVD: Epoch 92 of 300. Elapsed time 2.51 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.92 sec. MSE loss 1.01E-03. Sample per second: 517679\n",
      "FUNK_SVD: Epoch 93 of 300. Elapsed time 2.54 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.64 sec. MSE loss 1.00E-03. Sample per second: 376387\n",
      "FUNK_SVD: Epoch 94 of 300. Elapsed time 2.56 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.27 sec. MSE loss 1.01E-03. Sample per second: 437908\n",
      "FUNK_SVD: Epoch 95 of 300. Elapsed time 2.59 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.86 sec. MSE loss 1.01E-03. Sample per second: 534788\n",
      "FUNK_SVD: Epoch 96 of 300. Elapsed time 2.62 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.44 sec. MSE loss 1.00E-03. Sample per second: 407590\n",
      "FUNK_SVD: Epoch 97 of 300. Elapsed time 2.64 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.03 sec. MSE loss 1.01E-03. Sample per second: 491010\n",
      "FUNK_SVD: Epoch 98 of 300. Elapsed time 2.67 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.65 sec. MSE loss 1.00E-03. Sample per second: 601666\n",
      "FUNK_SVD: Epoch 99 of 300. Elapsed time 2.70 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.31 sec. MSE loss 1.01E-03. Sample per second: 430064\n",
      "FUNK_SVD: Epoch 100 of 300. Elapsed time 2.73 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.93 sec. MSE loss 1.01E-03. Sample per second: 514831\n",
      "FUNK_SVD: Epoch 101 of 300. Elapsed time 2.75 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.57 sec. MSE loss 1.01E-03. Sample per second: 386964\n",
      "FUNK_SVD: Epoch 102 of 300. Elapsed time 2.78 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.26 sec. MSE loss 1.01E-03. Sample per second: 439808\n",
      "FUNK_SVD: Epoch 103 of 300. Elapsed time 2.81 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.90 sec. MSE loss 1.00E-03. Sample per second: 522430\n",
      "FUNK_SVD: Epoch 104 of 300. Elapsed time 2.84 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.47 sec. MSE loss 1.00E-03. Sample per second: 403011\n",
      "FUNK_SVD: Epoch 105 of 300. Elapsed time 2.86 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.10 sec. MSE loss 1.01E-03. Sample per second: 474009\n",
      "FUNK_SVD: Epoch 106 of 300. Elapsed time 2.89 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.75 sec. MSE loss 1.01E-03. Sample per second: 569566\n",
      "FUNK_SVD: Epoch 107 of 300. Elapsed time 2.92 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.38 sec. MSE loss 1.01E-03. Sample per second: 417488\n",
      "FUNK_SVD: Epoch 108 of 300. Elapsed time 2.94 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.09 sec. MSE loss 1.01E-03. Sample per second: 476988\n",
      "FUNK_SVD: Epoch 109 of 300. Elapsed time 2.97 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.71 sec. MSE loss 1.01E-03. Sample per second: 580556\n",
      "FUNK_SVD: Epoch 110 of 300. Elapsed time 3.00 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.34 sec. MSE loss 1.00E-03. Sample per second: 424302\n",
      "FUNK_SVD: Epoch 111 of 300. Elapsed time 3.03 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.98 sec. MSE loss 1.00E-03. Sample per second: 501666\n",
      "FUNK_SVD: Epoch 112 of 300. Elapsed time 3.05 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.58 sec. MSE loss 1.01E-03. Sample per second: 384916\n",
      "FUNK_SVD: Epoch 113 of 300. Elapsed time 3.08 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.16 sec. MSE loss 1.00E-03. Sample per second: 460490\n",
      "FUNK_SVD: Epoch 114 of 300. Elapsed time 3.11 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.74 sec. MSE loss 1.01E-03. Sample per second: 572743\n",
      "FUNK_SVD: Epoch 115 of 300. Elapsed time 3.13 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.35 sec. MSE loss 1.01E-03. Sample per second: 423450\n",
      "FUNK_SVD: Epoch 116 of 300. Elapsed time 3.16 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.91 sec. MSE loss 1.01E-03. Sample per second: 519741\n",
      "FUNK_SVD: Epoch 117 of 300. Elapsed time 3.19 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.53 sec. MSE loss 1.01E-03. Sample per second: 392844\n",
      "FUNK_SVD: Epoch 118 of 300. Elapsed time 3.21 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.19 sec. MSE loss 1.01E-03. Sample per second: 454200\n",
      "FUNK_SVD: Epoch 119 of 300. Elapsed time 3.24 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.84 sec. MSE loss 1.01E-03. Sample per second: 539273\n",
      "FUNK_SVD: Epoch 120 of 300. Elapsed time 3.27 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.47 sec. MSE loss 1.01E-03. Sample per second: 403165\n",
      "FUNK_SVD: Epoch 121 of 300. Elapsed time 3.29 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.13 sec. MSE loss 1.01E-03. Sample per second: 467882\n",
      "FUNK_SVD: Epoch 122 of 300. Elapsed time 3.32 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.76 sec. MSE loss 1.00E-03. Sample per second: 566000\n",
      "FUNK_SVD: Epoch 123 of 300. Elapsed time 3.35 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.38 sec. MSE loss 1.00E-03. Sample per second: 418795\n",
      "FUNK_SVD: Epoch 124 of 300. Elapsed time 3.38 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.03 sec. MSE loss 1.01E-03. Sample per second: 489712\n",
      "FUNK_SVD: Epoch 125 of 300. Elapsed time 3.40 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.69 sec. MSE loss 1.00E-03. Sample per second: 589737\n",
      "FUNK_SVD: Epoch 126 of 300. Elapsed time 3.43 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.36 sec. MSE loss 1.01E-03. Sample per second: 421901\n",
      "FUNK_SVD: Epoch 127 of 300. Elapsed time 3.46 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.02 sec. MSE loss 1.00E-03. Sample per second: 492578\n",
      "FUNK_SVD: Epoch 128 of 300. Elapsed time 3.49 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.77 sec. MSE loss 1.01E-03. Sample per second: 563089\n",
      "FUNK_SVD: Epoch 129 of 300. Elapsed time 3.52 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.44 sec. MSE loss 1.01E-03. Sample per second: 407527\n",
      "FUNK_SVD: Epoch 130 of 300. Elapsed time 3.54 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.09 sec. MSE loss 1.01E-03. Sample per second: 475863\n",
      "FUNK_SVD: Epoch 131 of 300. Elapsed time 3.57 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.69 sec. MSE loss 1.01E-03. Sample per second: 587439\n",
      "FUNK_SVD: Epoch 132 of 300. Elapsed time 3.60 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.26 sec. MSE loss 1.01E-03. Sample per second: 439364\n",
      "FUNK_SVD: Epoch 133 of 300. Elapsed time 3.62 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.91 sec. MSE loss 1.00E-03. Sample per second: 520617\n",
      "FUNK_SVD: Epoch 134 of 300. Elapsed time 3.65 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.53 sec. MSE loss 1.01E-03. Sample per second: 392765\n",
      "FUNK_SVD: Epoch 135 of 300. Elapsed time 3.68 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.30 sec. MSE loss 1.00E-03. Sample per second: 432794\n",
      "FUNK_SVD: Epoch 136 of 300. Elapsed time 3.71 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.96 sec. MSE loss 1.01E-03. Sample per second: 508540\n",
      "FUNK_SVD: Epoch 137 of 300. Elapsed time 3.74 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.65 sec. MSE loss 1.00E-03. Sample per second: 375748\n",
      "FUNK_SVD: Epoch 138 of 300. Elapsed time 3.76 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.26 sec. MSE loss 1.00E-03. Sample per second: 440270\n",
      "FUNK_SVD: Epoch 139 of 300. Elapsed time 3.79 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.87 sec. MSE loss 1.00E-03. Sample per second: 531511\n",
      "FUNK_SVD: Epoch 140 of 300. Elapsed time 3.82 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.47 sec. MSE loss 1.01E-03. Sample per second: 402552\n",
      "FUNK_SVD: Epoch 141 of 300. Elapsed time 3.85 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.13 sec. MSE loss 1.01E-03. Sample per second: 466918\n",
      "FUNK_SVD: Epoch 142 of 300. Elapsed time 3.87 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.77 sec. MSE loss 1.00E-03. Sample per second: 562568\n",
      "FUNK_SVD: Epoch 143 of 300. Elapsed time 3.90 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.40 sec. MSE loss 1.01E-03. Sample per second: 414940\n",
      "FUNK_SVD: Epoch 144 of 300. Elapsed time 3.93 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.01 sec. MSE loss 1.01E-03. Sample per second: 494474\n",
      "FUNK_SVD: Epoch 145 of 300. Elapsed time 3.95 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.58 sec. MSE loss 1.01E-03. Sample per second: 627820\n",
      "FUNK_SVD: Epoch 146 of 300. Elapsed time 3.98 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.25 sec. MSE loss 1.00E-03. Sample per second: 442455\n",
      "FUNK_SVD: Epoch 147 of 300. Elapsed time 4.01 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.93 sec. MSE loss 1.01E-03. Sample per second: 516695\n",
      "FUNK_SVD: Epoch 148 of 300. Elapsed time 4.04 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.67 sec. MSE loss 1.00E-03. Sample per second: 372859\n",
      "FUNK_SVD: Epoch 149 of 300. Elapsed time 4.06 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.37 sec. MSE loss 1.00E-03. Sample per second: 420204\n",
      "FUNK_SVD: Epoch 150 of 300. Elapsed time 4.09 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.09 sec. MSE loss 1.01E-03. Sample per second: 476492\n",
      "FUNK_SVD: Epoch 151 of 300. Elapsed time 4.12 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.69 sec. MSE loss 1.01E-03. Sample per second: 587467\n",
      "FUNK_SVD: Epoch 152 of 300. Elapsed time 4.15 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.30 sec. MSE loss 1.00E-03. Sample per second: 432152\n",
      "FUNK_SVD: Epoch 153 of 300. Elapsed time 4.18 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.00 sec. MSE loss 1.00E-03. Sample per second: 498236\n",
      "FUNK_SVD: Epoch 154 of 300. Elapsed time 4.20 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.65 sec. MSE loss 1.01E-03. Sample per second: 602967\n",
      "FUNK_SVD: Epoch 155 of 300. Elapsed time 4.23 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.35 sec. MSE loss 1.00E-03. Sample per second: 423148\n",
      "FUNK_SVD: Epoch 156 of 300. Elapsed time 4.26 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.94 sec. MSE loss 1.00E-03. Sample per second: 511621\n",
      "FUNK_SVD: Epoch 157 of 300. Elapsed time 4.29 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.56 sec. MSE loss 1.01E-03. Sample per second: 388116\n",
      "FUNK_SVD: Epoch 158 of 300. Elapsed time 4.31 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.18 sec. MSE loss 1.01E-03. Sample per second: 457302\n",
      "FUNK_SVD: Epoch 159 of 300. Elapsed time 4.34 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.85 sec. MSE loss 1.01E-03. Sample per second: 537302\n",
      "FUNK_SVD: Epoch 160 of 300. Elapsed time 4.37 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.55 sec. MSE loss 1.00E-03. Sample per second: 390395\n",
      "FUNK_SVD: Epoch 161 of 300. Elapsed time 4.40 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.21 sec. MSE loss 1.00E-03. Sample per second: 449409\n",
      "FUNK_SVD: Epoch 162 of 300. Elapsed time 4.42 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.82 sec. MSE loss 1.01E-03. Sample per second: 547189\n",
      "FUNK_SVD: Epoch 163 of 300. Elapsed time 4.45 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.40 sec. MSE loss 1.00E-03. Sample per second: 413836\n",
      "FUNK_SVD: Epoch 164 of 300. Elapsed time 4.48 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.00 sec. MSE loss 1.01E-03. Sample per second: 497005\n",
      "FUNK_SVD: Epoch 165 of 300. Elapsed time 4.50 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.72 sec. MSE loss 1.01E-03. Sample per second: 577987\n",
      "FUNK_SVD: Epoch 166 of 300. Elapsed time 4.53 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.53 sec. MSE loss 1.01E-03. Sample per second: 393129\n",
      "FUNK_SVD: Epoch 167 of 300. Elapsed time 4.56 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.25 sec. MSE loss 1.00E-03. Sample per second: 441650\n",
      "FUNK_SVD: Epoch 168 of 300. Elapsed time 4.59 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.96 sec. MSE loss 1.00E-03. Sample per second: 506459\n",
      "FUNK_SVD: Epoch 169 of 300. Elapsed time 4.62 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.61 sec. MSE loss 1.01E-03. Sample per second: 381792\n",
      "FUNK_SVD: Epoch 170 of 300. Elapsed time 4.65 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.24 sec. MSE loss 1.01E-03. Sample per second: 444984\n",
      "FUNK_SVD: Epoch 171 of 300. Elapsed time 4.67 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.86 sec. MSE loss 1.01E-03. Sample per second: 533500\n",
      "FUNK_SVD: Epoch 172 of 300. Elapsed time 4.70 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.47 sec. MSE loss 1.00E-03. Sample per second: 402486\n",
      "FUNK_SVD: Epoch 173 of 300. Elapsed time 4.73 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.06 sec. MSE loss 1.00E-03. Sample per second: 481946\n",
      "FUNK_SVD: Epoch 174 of 300. Elapsed time 4.75 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.69 sec. MSE loss 1.01E-03. Sample per second: 589502\n",
      "FUNK_SVD: Epoch 175 of 300. Elapsed time 4.78 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.37 sec. MSE loss 1.01E-03. Sample per second: 419868\n",
      "FUNK_SVD: Epoch 176 of 300. Elapsed time 4.81 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.97 sec. MSE loss 1.01E-03. Sample per second: 505085\n",
      "FUNK_SVD: Epoch 177 of 300. Elapsed time 4.84 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.64 sec. MSE loss 1.00E-03. Sample per second: 376556\n",
      "FUNK_SVD: Epoch 178 of 300. Elapsed time 4.86 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.30 sec. MSE loss 1.01E-03. Sample per second: 432960\n",
      "FUNK_SVD: Epoch 179 of 300. Elapsed time 4.89 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.89 sec. MSE loss 1.01E-03. Sample per second: 527607\n",
      "FUNK_SVD: Epoch 180 of 300. Elapsed time 4.92 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.49 sec. MSE loss 1.00E-03. Sample per second: 400234\n",
      "FUNK_SVD: Epoch 181 of 300. Elapsed time 4.95 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.13 sec. MSE loss 1.00E-03. Sample per second: 466128\n",
      "FUNK_SVD: Epoch 182 of 300. Elapsed time 4.97 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.74 sec. MSE loss 1.00E-03. Sample per second: 572505\n",
      "FUNK_SVD: Epoch 183 of 300. Elapsed time 5.00 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.35 sec. MSE loss 1.00E-03. Sample per second: 422607\n",
      "FUNK_SVD: Epoch 184 of 300. Elapsed time 5.03 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.99 sec. MSE loss 1.00E-03. Sample per second: 500829\n",
      "FUNK_SVD: Epoch 185 of 300. Elapsed time 5.05 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.62 sec. MSE loss 1.01E-03. Sample per second: 379812\n",
      "FUNK_SVD: Epoch 186 of 300. Elapsed time 5.08 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.21 sec. MSE loss 1.00E-03. Sample per second: 449472\n",
      "FUNK_SVD: Epoch 187 of 300. Elapsed time 5.11 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.83 sec. MSE loss 1.00E-03. Sample per second: 544092\n",
      "FUNK_SVD: Epoch 188 of 300. Elapsed time 5.13 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.56 sec. MSE loss 1.01E-03. Sample per second: 388595\n",
      "FUNK_SVD: Epoch 189 of 300. Elapsed time 5.16 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.28 sec. MSE loss 1.00E-03. Sample per second: 436506\n",
      "FUNK_SVD: Epoch 190 of 300. Elapsed time 5.19 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.92 sec. MSE loss 1.00E-03. Sample per second: 517509\n",
      "FUNK_SVD: Epoch 191 of 300. Elapsed time 5.22 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.56 sec. MSE loss 1.01E-03. Sample per second: 389193\n",
      "FUNK_SVD: Epoch 192 of 300. Elapsed time 5.25 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.17 sec. MSE loss 1.00E-03. Sample per second: 459126\n",
      "FUNK_SVD: Epoch 193 of 300. Elapsed time 5.27 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.79 sec. MSE loss 1.00E-03. Sample per second: 555126\n",
      "FUNK_SVD: Epoch 194 of 300. Elapsed time 5.30 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.41 sec. MSE loss 1.01E-03. Sample per second: 412137\n",
      "FUNK_SVD: Epoch 195 of 300. Elapsed time 5.33 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.12 sec. MSE loss 1.01E-03. Sample per second: 469006\n",
      "FUNK_SVD: Epoch 196 of 300. Elapsed time 5.36 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.77 sec. MSE loss 1.00E-03. Sample per second: 560600\n",
      "FUNK_SVD: Epoch 197 of 300. Elapsed time 5.38 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.37 sec. MSE loss 1.01E-03. Sample per second: 419051\n",
      "FUNK_SVD: Epoch 198 of 300. Elapsed time 5.41 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.96 sec. MSE loss 1.01E-03. Sample per second: 507474\n",
      "FUNK_SVD: Epoch 199 of 300. Elapsed time 5.44 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.56 sec. MSE loss 1.01E-03. Sample per second: 387937\n",
      "FUNK_SVD: Epoch 200 of 300. Elapsed time 5.46 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.19 sec. MSE loss 1.00E-03. Sample per second: 454177\n",
      "FUNK_SVD: Epoch 201 of 300. Elapsed time 5.49 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.83 sec. MSE loss 1.01E-03. Sample per second: 543502\n",
      "FUNK_SVD: Epoch 202 of 300. Elapsed time 5.52 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.52 sec. MSE loss 1.01E-03. Sample per second: 394587\n",
      "FUNK_SVD: Epoch 203 of 300. Elapsed time 5.55 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.21 sec. MSE loss 1.00E-03. Sample per second: 450728\n",
      "FUNK_SVD: Epoch 204 of 300. Elapsed time 5.57 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.92 sec. MSE loss 1.00E-03. Sample per second: 518239\n",
      "FUNK_SVD: Epoch 205 of 300. Elapsed time 5.60 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.57 sec. MSE loss 1.01E-03. Sample per second: 386821\n",
      "FUNK_SVD: Epoch 206 of 300. Elapsed time 5.63 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.24 sec. MSE loss 1.01E-03. Sample per second: 443796\n",
      "FUNK_SVD: Epoch 207 of 300. Elapsed time 5.66 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.96 sec. MSE loss 1.00E-03. Sample per second: 507988\n",
      "FUNK_SVD: Epoch 208 of 300. Elapsed time 5.69 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.64 sec. MSE loss 1.01E-03. Sample per second: 376420\n",
      "FUNK_SVD: Epoch 209 of 300. Elapsed time 5.71 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.30 sec. MSE loss 1.01E-03. Sample per second: 433147\n",
      "FUNK_SVD: Epoch 210 of 300. Elapsed time 5.74 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.93 sec. MSE loss 1.00E-03. Sample per second: 516600\n",
      "FUNK_SVD: Epoch 211 of 300. Elapsed time 5.77 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.59 sec. MSE loss 1.00E-03. Sample per second: 384414\n",
      "FUNK_SVD: Epoch 212 of 300. Elapsed time 5.80 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.22 sec. MSE loss 1.00E-03. Sample per second: 447196\n",
      "FUNK_SVD: Epoch 213 of 300. Elapsed time 5.82 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.83 sec. MSE loss 1.00E-03. Sample per second: 543571\n",
      "FUNK_SVD: Epoch 214 of 300. Elapsed time 5.85 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.45 sec. MSE loss 1.00E-03. Sample per second: 406203\n",
      "FUNK_SVD: Epoch 215 of 300. Elapsed time 5.88 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.10 sec. MSE loss 1.01E-03. Sample per second: 474180\n",
      "FUNK_SVD: Epoch 216 of 300. Elapsed time 5.91 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.68 sec. MSE loss 1.00E-03. Sample per second: 591053\n",
      "FUNK_SVD: Epoch 217 of 300. Elapsed time 5.93 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.30 sec. MSE loss 1.00E-03. Sample per second: 431877\n",
      "FUNK_SVD: Epoch 218 of 300. Elapsed time 5.96 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.89 sec. MSE loss 1.00E-03. Sample per second: 525025\n",
      "FUNK_SVD: Epoch 219 of 300. Elapsed time 5.99 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.56 sec. MSE loss 1.00E-03. Sample per second: 388056\n",
      "FUNK_SVD: Epoch 220 of 300. Elapsed time 6.01 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.23 sec. MSE loss 1.00E-03. Sample per second: 446235\n",
      "FUNK_SVD: Epoch 221 of 300. Elapsed time 6.04 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.91 sec. MSE loss 1.00E-03. Sample per second: 521099\n",
      "FUNK_SVD: Epoch 222 of 300. Elapsed time 6.07 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.56 sec. MSE loss 1.00E-03. Sample per second: 387997\n",
      "FUNK_SVD: Epoch 223 of 300. Elapsed time 6.10 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.26 sec. MSE loss 1.00E-03. Sample per second: 439900\n",
      "FUNK_SVD: Epoch 224 of 300. Elapsed time 6.12 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.86 sec. MSE loss 1.00E-03. Sample per second: 533601\n",
      "FUNK_SVD: Epoch 225 of 300. Elapsed time 6.15 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.48 sec. MSE loss 1.00E-03. Sample per second: 401636\n",
      "FUNK_SVD: Epoch 226 of 300. Elapsed time 6.18 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.12 sec. MSE loss 1.00E-03. Sample per second: 470079\n",
      "FUNK_SVD: Epoch 227 of 300. Elapsed time 6.21 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.77 sec. MSE loss 1.01E-03. Sample per second: 560763\n",
      "FUNK_SVD: Epoch 228 of 300. Elapsed time 6.23 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.50 sec. MSE loss 1.01E-03. Sample per second: 397933\n",
      "FUNK_SVD: Epoch 229 of 300. Elapsed time 6.26 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.14 sec. MSE loss 1.00E-03. Sample per second: 464586\n",
      "FUNK_SVD: Epoch 230 of 300. Elapsed time 6.29 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.77 sec. MSE loss 1.00E-03. Sample per second: 562377\n",
      "FUNK_SVD: Epoch 231 of 300. Elapsed time 6.32 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.42 sec. MSE loss 1.00E-03. Sample per second: 411348\n",
      "FUNK_SVD: Epoch 232 of 300. Elapsed time 6.34 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.05 sec. MSE loss 1.00E-03. Sample per second: 486223\n",
      "FUNK_SVD: Epoch 233 of 300. Elapsed time 6.37 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.69 sec. MSE loss 1.00E-03. Sample per second: 587272\n",
      "FUNK_SVD: Epoch 234 of 300. Elapsed time 6.40 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.32 sec. MSE loss 1.00E-03. Sample per second: 429278\n",
      "FUNK_SVD: Epoch 235 of 300. Elapsed time 6.43 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.94 sec. MSE loss 1.00E-03. Sample per second: 512778\n",
      "FUNK_SVD: Epoch 236 of 300. Elapsed time 6.45 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.59 sec. MSE loss 1.00E-03. Sample per second: 383515\n",
      "FUNK_SVD: Epoch 237 of 300. Elapsed time 6.48 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.31 sec. MSE loss 1.00E-03. Sample per second: 430191\n",
      "FUNK_SVD: Epoch 238 of 300. Elapsed time 6.51 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.98 sec. MSE loss 1.00E-03. Sample per second: 503022\n",
      "FUNK_SVD: Epoch 239 of 300. Elapsed time 6.54 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.63 sec. MSE loss 1.00E-03. Sample per second: 378599\n",
      "FUNK_SVD: Epoch 240 of 300. Elapsed time 6.56 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.24 sec. MSE loss 1.00E-03. Sample per second: 444592\n",
      "FUNK_SVD: Epoch 241 of 300. Elapsed time 6.59 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.91 sec. MSE loss 1.01E-03. Sample per second: 521058\n",
      "FUNK_SVD: Epoch 242 of 300. Elapsed time 6.62 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.62 sec. MSE loss 1.00E-03. Sample per second: 379647\n",
      "FUNK_SVD: Epoch 243 of 300. Elapsed time 6.65 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.24 sec. MSE loss 1.00E-03. Sample per second: 443455\n",
      "FUNK_SVD: Epoch 244 of 300. Elapsed time 6.67 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.96 sec. MSE loss 1.00E-03. Sample per second: 508542\n",
      "FUNK_SVD: Epoch 245 of 300. Elapsed time 6.70 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.60 sec. MSE loss 1.00E-03. Sample per second: 382223\n",
      "FUNK_SVD: Epoch 246 of 300. Elapsed time 6.73 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.24 sec. MSE loss 1.00E-03. Sample per second: 444608\n",
      "FUNK_SVD: Epoch 247 of 300. Elapsed time 6.76 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.91 sec. MSE loss 1.00E-03. Sample per second: 522048\n",
      "FUNK_SVD: Epoch 248 of 300. Elapsed time 6.79 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.62 sec. MSE loss 1.00E-03. Sample per second: 380214\n",
      "FUNK_SVD: Epoch 249 of 300. Elapsed time 6.81 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.26 sec. MSE loss 1.00E-03. Sample per second: 440130\n",
      "FUNK_SVD: Epoch 250 of 300. Elapsed time 6.84 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.93 sec. MSE loss 1.01E-03. Sample per second: 515215\n",
      "FUNK_SVD: Epoch 251 of 300. Elapsed time 6.87 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.59 sec. MSE loss 1.00E-03. Sample per second: 384052\n",
      "FUNK_SVD: Epoch 252 of 300. Elapsed time 6.90 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.26 sec. MSE loss 1.00E-03. Sample per second: 441167\n",
      "FUNK_SVD: Epoch 253 of 300. Elapsed time 6.92 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.01 sec. MSE loss 1.01E-03. Sample per second: 494669\n",
      "FUNK_SVD: Epoch 254 of 300. Elapsed time 6.95 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.68 sec. MSE loss 1.01E-03. Sample per second: 592418\n",
      "FUNK_SVD: Epoch 255 of 300. Elapsed time 6.98 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.39 sec. MSE loss 1.00E-03. Sample per second: 417028\n",
      "FUNK_SVD: Epoch 256 of 300. Elapsed time 7.01 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.05 sec. MSE loss 1.00E-03. Sample per second: 484136\n",
      "FUNK_SVD: Epoch 257 of 300. Elapsed time 7.04 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.72 sec. MSE loss 1.00E-03. Sample per second: 578876\n",
      "FUNK_SVD: Epoch 258 of 300. Elapsed time 7.07 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.34 sec. MSE loss 1.01E-03. Sample per second: 425254\n",
      "FUNK_SVD: Epoch 259 of 300. Elapsed time 7.09 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.95 sec. MSE loss 1.00E-03. Sample per second: 510926\n",
      "FUNK_SVD: Epoch 260 of 300. Elapsed time 7.12 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.59 sec. MSE loss 1.01E-03. Sample per second: 384189\n",
      "FUNK_SVD: Epoch 261 of 300. Elapsed time 7.15 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.28 sec. MSE loss 1.01E-03. Sample per second: 436617\n",
      "FUNK_SVD: Epoch 262 of 300. Elapsed time 7.18 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.97 sec. MSE loss 1.00E-03. Sample per second: 504560\n",
      "FUNK_SVD: Epoch 263 of 300. Elapsed time 7.20 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.66 sec. MSE loss 1.00E-03. Sample per second: 373864\n",
      "FUNK_SVD: Epoch 264 of 300. Elapsed time 7.23 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.33 sec. MSE loss 1.00E-03. Sample per second: 426415\n",
      "FUNK_SVD: Epoch 265 of 300. Elapsed time 7.26 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.96 sec. MSE loss 1.00E-03. Sample per second: 508585\n",
      "FUNK_SVD: Epoch 266 of 300. Elapsed time 7.29 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.60 sec. MSE loss 1.00E-03. Sample per second: 383309\n",
      "FUNK_SVD: Epoch 267 of 300. Elapsed time 7.31 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.28 sec. MSE loss 1.01E-03. Sample per second: 436611\n",
      "FUNK_SVD: Epoch 268 of 300. Elapsed time 7.34 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.97 sec. MSE loss 9.99E-04. Sample per second: 504293\n",
      "FUNK_SVD: Epoch 269 of 300. Elapsed time 7.37 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.57 sec. MSE loss 1.00E-03. Sample per second: 386355\n",
      "FUNK_SVD: Epoch 270 of 300. Elapsed time 7.40 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.22 sec. MSE loss 1.00E-03. Sample per second: 449000\n",
      "FUNK_SVD: Epoch 271 of 300. Elapsed time 7.42 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.90 sec. MSE loss 1.00E-03. Sample per second: 524407\n",
      "FUNK_SVD: Epoch 272 of 300. Elapsed time 7.45 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.56 sec. MSE loss 1.01E-03. Sample per second: 388663\n",
      "FUNK_SVD: Epoch 273 of 300. Elapsed time 7.48 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.31 sec. MSE loss 1.00E-03. Sample per second: 431453\n",
      "FUNK_SVD: Epoch 274 of 300. Elapsed time 7.51 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.05 sec. MSE loss 1.00E-03. Sample per second: 485858\n",
      "FUNK_SVD: Epoch 275 of 300. Elapsed time 7.54 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.73 sec. MSE loss 1.00E-03. Sample per second: 576352\n",
      "FUNK_SVD: Epoch 276 of 300. Elapsed time 7.57 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.34 sec. MSE loss 1.00E-03. Sample per second: 425123\n",
      "FUNK_SVD: Epoch 277 of 300. Elapsed time 7.59 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.98 sec. MSE loss 1.00E-03. Sample per second: 501917\n",
      "FUNK_SVD: Epoch 278 of 300. Elapsed time 7.62 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.61 sec. MSE loss 1.00E-03. Sample per second: 380847\n",
      "FUNK_SVD: Epoch 279 of 300. Elapsed time 7.65 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.32 sec. MSE loss 1.00E-03. Sample per second: 428459\n",
      "FUNK_SVD: Epoch 280 of 300. Elapsed time 7.68 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.02 sec. MSE loss 1.00E-03. Sample per second: 492867\n",
      "FUNK_SVD: Epoch 281 of 300. Elapsed time 7.70 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.68 sec. MSE loss 1.00E-03. Sample per second: 593779\n",
      "FUNK_SVD: Epoch 282 of 300. Elapsed time 7.73 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.38 sec. MSE loss 1.00E-03. Sample per second: 418232\n",
      "FUNK_SVD: Epoch 283 of 300. Elapsed time 7.76 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.02 sec. MSE loss 1.00E-03. Sample per second: 491829\n",
      "FUNK_SVD: Epoch 284 of 300. Elapsed time 7.79 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.70 sec. MSE loss 1.01E-03. Sample per second: 583599\n",
      "FUNK_SVD: Epoch 285 of 300. Elapsed time 7.82 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.35 sec. MSE loss 1.00E-03. Sample per second: 423022\n",
      "FUNK_SVD: Epoch 286 of 300. Elapsed time 7.84 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.99 sec. MSE loss 1.00E-03. Sample per second: 499204\n",
      "FUNK_SVD: Epoch 287 of 300. Elapsed time 7.87 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.61 sec. MSE loss 1.00E-03. Sample per second: 381746\n",
      "FUNK_SVD: Epoch 288 of 300. Elapsed time 7.90 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.22 sec. MSE loss 1.00E-03. Sample per second: 448891\n",
      "FUNK_SVD: Epoch 289 of 300. Elapsed time 7.92 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.80 sec. MSE loss 1.00E-03. Sample per second: 551731\n",
      "FUNK_SVD: Epoch 290 of 300. Elapsed time 7.95 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.40 sec. MSE loss 1.00E-03. Sample per second: 415360\n",
      "FUNK_SVD: Epoch 291 of 300. Elapsed time 7.98 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.07 sec. MSE loss 1.00E-03. Sample per second: 480042\n",
      "FUNK_SVD: Epoch 292 of 300. Elapsed time 8.01 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.73 sec. MSE loss 1.00E-03. Sample per second: 575290\n",
      "FUNK_SVD: Epoch 293 of 300. Elapsed time 8.03 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.39 sec. MSE loss 1.00E-03. Sample per second: 416756\n",
      "FUNK_SVD: Epoch 294 of 300. Elapsed time 8.06 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.08 sec. MSE loss 1.00E-03. Sample per second: 477627\n",
      "FUNK_SVD: Epoch 295 of 300. Elapsed time 8.09 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.73 sec. MSE loss 1.00E-03. Sample per second: 575479\n",
      "FUNK_SVD: Epoch 296 of 300. Elapsed time 8.12 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.32 sec. MSE loss 1.00E-03. Sample per second: 427942\n",
      "FUNK_SVD: Epoch 297 of 300. Elapsed time 8.14 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 1.98 sec. MSE loss 1.00E-03. Sample per second: 502444\n",
      "FUNK_SVD: Epoch 298 of 300. Elapsed time 8.17 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.65 sec. MSE loss 1.00E-03. Sample per second: 375541\n",
      "FUNK_SVD: Epoch 299 of 300. Elapsed time 8.20 min\n",
      "FUNK_SVD: Processed 995000 (100.0%) in 2.27 sec. MSE loss 1.00E-03. Sample per second: 437953\n",
      "FUNK_SVD: Epoch 300 of 300. Elapsed time 8.23 min\n",
      "FUNK_SVD: Terminating at epoch 300. Elapsed time 8.23 min\n",
      "SLIM_BPR_Recommender: URM Detected 1 ( 0.0%) users with no interactions.\n",
      "SLIM_BPR_Recommender: Automatic selection of fastest train mode. Available RAM is 30619.00 MB (95.36%) of 32110.00 MB, required is 2402.37 MB. Using dense matrix.\n",
      "Processed 41629 (100.0%) in 0.63 sec. BPR loss is 3.08E-07. Sample per second: 65613\n",
      "SLIM_BPR_Recommender: Epoch 1 of 300. Elapsed time 0.18 sec\n",
      "Processed 41629 (100.0%) in 0.86 sec. BPR loss is 9.48E-07. Sample per second: 48591\n",
      "SLIM_BPR_Recommender: Epoch 2 of 300. Elapsed time 0.40 sec\n",
      "Processed 41629 (100.0%) in 1.05 sec. BPR loss is 1.57E-06. Sample per second: 39823\n",
      "SLIM_BPR_Recommender: Epoch 3 of 300. Elapsed time 0.59 sec\n",
      "Processed 41629 (100.0%) in 0.24 sec. BPR loss is 2.34E-06. Sample per second: 176462\n",
      "SLIM_BPR_Recommender: Epoch 4 of 300. Elapsed time 0.78 sec\n",
      "Processed 41629 (100.0%) in 0.43 sec. BPR loss is 2.84E-06. Sample per second: 96802\n",
      "SLIM_BPR_Recommender: Epoch 5 of 300. Elapsed time 0.98 sec\n",
      "Processed 41629 (100.0%) in 0.62 sec. BPR loss is 3.63E-06. Sample per second: 67368\n",
      "SLIM_BPR_Recommender: Epoch 6 of 300. Elapsed time 1.16 sec\n",
      "Processed 41629 (100.0%) in 0.82 sec. BPR loss is 4.24E-06. Sample per second: 50817\n",
      "SLIM_BPR_Recommender: Epoch 7 of 300. Elapsed time 1.37 sec\n",
      "Processed 41629 (100.0%) in 1.02 sec. BPR loss is 4.86E-06. Sample per second: 40916\n",
      "SLIM_BPR_Recommender: Epoch 8 of 300. Elapsed time 1.56 sec\n",
      "Processed 41629 (100.0%) in 0.21 sec. BPR loss is 5.96E-06. Sample per second: 199121\n",
      "SLIM_BPR_Recommender: Epoch 9 of 300. Elapsed time 1.76 sec\n",
      "Processed 41629 (100.0%) in 0.41 sec. BPR loss is 6.42E-06. Sample per second: 101967\n",
      "SLIM_BPR_Recommender: Epoch 10 of 300. Elapsed time 1.96 sec\n",
      "Processed 41629 (100.0%) in 0.60 sec. BPR loss is 7.04E-06. Sample per second: 68876\n",
      "SLIM_BPR_Recommender: Epoch 11 of 300. Elapsed time 2.15 sec\n",
      "Processed 41629 (100.0%) in 0.80 sec. BPR loss is 7.64E-06. Sample per second: 51995\n",
      "SLIM_BPR_Recommender: Epoch 12 of 300. Elapsed time 2.35 sec\n",
      "Processed 41629 (100.0%) in 0.99 sec. BPR loss is 8.10E-06. Sample per second: 41873\n",
      "SLIM_BPR_Recommender: Epoch 13 of 300. Elapsed time 2.54 sec\n",
      "Processed 41629 (100.0%) in 1.19 sec. BPR loss is 8.64E-06. Sample per second: 34981\n",
      "SLIM_BPR_Recommender: Epoch 14 of 300. Elapsed time 2.74 sec\n",
      "Processed 41629 (100.0%) in 0.39 sec. BPR loss is 9.69E-06. Sample per second: 107224\n",
      "SLIM_BPR_Recommender: Epoch 15 of 300. Elapsed time 2.94 sec\n",
      "Processed 41629 (100.0%) in 0.58 sec. BPR loss is 1.04E-05. Sample per second: 71906\n",
      "SLIM_BPR_Recommender: Epoch 16 of 300. Elapsed time 3.13 sec\n",
      "Processed 41629 (100.0%) in 0.77 sec. BPR loss is 1.06E-05. Sample per second: 53813\n",
      "SLIM_BPR_Recommender: Epoch 17 of 300. Elapsed time 3.32 sec\n",
      "Processed 41629 (100.0%) in 0.96 sec. BPR loss is 1.20E-05. Sample per second: 43167\n",
      "SLIM_BPR_Recommender: Epoch 18 of 300. Elapsed time 3.51 sec\n",
      "Processed 41629 (100.0%) in 1.16 sec. BPR loss is 1.21E-05. Sample per second: 35794\n",
      "SLIM_BPR_Recommender: Epoch 19 of 300. Elapsed time 3.71 sec\n",
      "Processed 41629 (100.0%) in 0.36 sec. BPR loss is 1.29E-05. Sample per second: 116806\n",
      "SLIM_BPR_Recommender: Epoch 20 of 300. Elapsed time 3.90 sec\n",
      "Processed 41629 (100.0%) in 0.55 sec. BPR loss is 1.40E-05. Sample per second: 76203\n",
      "SLIM_BPR_Recommender: Epoch 21 of 300. Elapsed time 4.09 sec\n",
      "Processed 41629 (100.0%) in 0.74 sec. BPR loss is 1.49E-05. Sample per second: 56193\n",
      "SLIM_BPR_Recommender: Epoch 22 of 300. Elapsed time 4.29 sec\n",
      "Processed 41629 (100.0%) in 0.93 sec. BPR loss is 1.56E-05. Sample per second: 44677\n",
      "SLIM_BPR_Recommender: Epoch 23 of 300. Elapsed time 4.48 sec\n",
      "Processed 41629 (100.0%) in 1.12 sec. BPR loss is 1.62E-05. Sample per second: 37146\n",
      "SLIM_BPR_Recommender: Epoch 24 of 300. Elapsed time 4.67 sec\n",
      "Processed 41629 (100.0%) in 0.31 sec. BPR loss is 1.70E-05. Sample per second: 132709\n",
      "SLIM_BPR_Recommender: Epoch 25 of 300. Elapsed time 4.86 sec\n",
      "Processed 41629 (100.0%) in 0.51 sec. BPR loss is 1.60E-05. Sample per second: 82010\n",
      "SLIM_BPR_Recommender: Epoch 26 of 300. Elapsed time 5.05 sec\n",
      "Processed 41629 (100.0%) in 0.70 sec. BPR loss is 1.77E-05. Sample per second: 59470\n",
      "SLIM_BPR_Recommender: Epoch 27 of 300. Elapsed time 5.25 sec\n",
      "Processed 41629 (100.0%) in 0.89 sec. BPR loss is 1.82E-05. Sample per second: 46797\n",
      "SLIM_BPR_Recommender: Epoch 28 of 300. Elapsed time 5.44 sec\n",
      "Processed 41629 (100.0%) in 1.08 sec. BPR loss is 1.92E-05. Sample per second: 38517\n",
      "SLIM_BPR_Recommender: Epoch 29 of 300. Elapsed time 5.63 sec\n",
      "Processed 41629 (100.0%) in 0.28 sec. BPR loss is 1.96E-05. Sample per second: 148078\n",
      "SLIM_BPR_Recommender: Epoch 30 of 300. Elapsed time 5.83 sec\n",
      "Processed 41629 (100.0%) in 0.47 sec. BPR loss is 2.05E-05. Sample per second: 87843\n",
      "SLIM_BPR_Recommender: Epoch 31 of 300. Elapsed time 6.02 sec\n",
      "Processed 41629 (100.0%) in 0.67 sec. BPR loss is 2.15E-05. Sample per second: 62125\n",
      "SLIM_BPR_Recommender: Epoch 32 of 300. Elapsed time 6.22 sec\n",
      "Processed 41629 (100.0%) in 0.87 sec. BPR loss is 2.17E-05. Sample per second: 47934\n",
      "SLIM_BPR_Recommender: Epoch 33 of 300. Elapsed time 6.42 sec\n",
      "Processed 41629 (100.0%) in 1.06 sec. BPR loss is 2.29E-05. Sample per second: 39166\n",
      "SLIM_BPR_Recommender: Epoch 34 of 300. Elapsed time 6.61 sec\n",
      "Processed 41629 (100.0%) in 0.25 sec. BPR loss is 2.36E-05. Sample per second: 167940\n",
      "SLIM_BPR_Recommender: Epoch 35 of 300. Elapsed time 6.80 sec\n",
      "Processed 41629 (100.0%) in 0.43 sec. BPR loss is 2.51E-05. Sample per second: 96145\n",
      "SLIM_BPR_Recommender: Epoch 36 of 300. Elapsed time 6.98 sec\n",
      "Processed 41629 (100.0%) in 0.62 sec. BPR loss is 2.63E-05. Sample per second: 67162\n",
      "SLIM_BPR_Recommender: Epoch 37 of 300. Elapsed time 7.17 sec\n",
      "Processed 41629 (100.0%) in 0.80 sec. BPR loss is 2.58E-05. Sample per second: 51753\n",
      "SLIM_BPR_Recommender: Epoch 38 of 300. Elapsed time 7.35 sec\n",
      "Processed 41629 (100.0%) in 0.98 sec. BPR loss is 2.67E-05. Sample per second: 42374\n",
      "SLIM_BPR_Recommender: Epoch 39 of 300. Elapsed time 7.53 sec\n",
      "Processed 41629 (100.0%) in 1.16 sec. BPR loss is 2.66E-05. Sample per second: 35984\n",
      "SLIM_BPR_Recommender: Epoch 40 of 300. Elapsed time 7.71 sec\n",
      "Processed 41629 (100.0%) in 0.34 sec. BPR loss is 2.78E-05. Sample per second: 122758\n",
      "SLIM_BPR_Recommender: Epoch 41 of 300. Elapsed time 7.89 sec\n",
      "Processed 41629 (100.0%) in 0.52 sec. BPR loss is 2.93E-05. Sample per second: 79965\n",
      "SLIM_BPR_Recommender: Epoch 42 of 300. Elapsed time 8.07 sec\n",
      "Processed 41629 (100.0%) in 0.70 sec. BPR loss is 2.82E-05. Sample per second: 59680\n",
      "SLIM_BPR_Recommender: Epoch 43 of 300. Elapsed time 8.24 sec\n",
      "Processed 41629 (100.0%) in 0.88 sec. BPR loss is 2.95E-05. Sample per second: 47342\n",
      "SLIM_BPR_Recommender: Epoch 44 of 300. Elapsed time 8.43 sec\n",
      "Processed 41629 (100.0%) in 1.06 sec. BPR loss is 3.06E-05. Sample per second: 39221\n",
      "SLIM_BPR_Recommender: Epoch 45 of 300. Elapsed time 8.61 sec\n",
      "Processed 41629 (100.0%) in 0.24 sec. BPR loss is 3.20E-05. Sample per second: 170309\n",
      "SLIM_BPR_Recommender: Epoch 46 of 300. Elapsed time 8.79 sec\n",
      "Processed 41629 (100.0%) in 0.42 sec. BPR loss is 3.21E-05. Sample per second: 98327\n",
      "SLIM_BPR_Recommender: Epoch 47 of 300. Elapsed time 8.97 sec\n",
      "Processed 41629 (100.0%) in 0.61 sec. BPR loss is 3.24E-05. Sample per second: 68542\n",
      "SLIM_BPR_Recommender: Epoch 48 of 300. Elapsed time 9.15 sec\n",
      "Processed 41629 (100.0%) in 0.79 sec. BPR loss is 3.27E-05. Sample per second: 52943\n",
      "SLIM_BPR_Recommender: Epoch 49 of 300. Elapsed time 9.33 sec\n",
      "Processed 41629 (100.0%) in 0.97 sec. BPR loss is 3.26E-05. Sample per second: 42971\n",
      "SLIM_BPR_Recommender: Epoch 50 of 300. Elapsed time 9.52 sec\n",
      "Processed 41629 (100.0%) in 1.15 sec. BPR loss is 3.42E-05. Sample per second: 36219\n",
      "SLIM_BPR_Recommender: Epoch 51 of 300. Elapsed time 9.70 sec\n",
      "Processed 41629 (100.0%) in 0.33 sec. BPR loss is 3.47E-05. Sample per second: 125083\n",
      "SLIM_BPR_Recommender: Epoch 52 of 300. Elapsed time 9.88 sec\n",
      "Processed 41629 (100.0%) in 0.52 sec. BPR loss is 3.74E-05. Sample per second: 79825\n",
      "SLIM_BPR_Recommender: Epoch 53 of 300. Elapsed time 10.07 sec\n",
      "Processed 41629 (100.0%) in 0.70 sec. BPR loss is 3.59E-05. Sample per second: 59676\n",
      "SLIM_BPR_Recommender: Epoch 54 of 300. Elapsed time 10.24 sec\n",
      "Processed 41629 (100.0%) in 0.88 sec. BPR loss is 3.66E-05. Sample per second: 47474\n",
      "SLIM_BPR_Recommender: Epoch 55 of 300. Elapsed time 10.42 sec\n",
      "Processed 41629 (100.0%) in 1.06 sec. BPR loss is 3.67E-05. Sample per second: 39168\n",
      "SLIM_BPR_Recommender: Epoch 56 of 300. Elapsed time 10.61 sec\n",
      "Processed 41629 (100.0%) in 0.24 sec. BPR loss is 3.98E-05. Sample per second: 173646\n",
      "SLIM_BPR_Recommender: Epoch 57 of 300. Elapsed time 10.79 sec\n",
      "Processed 41629 (100.0%) in 0.42 sec. BPR loss is 3.86E-05. Sample per second: 99021\n",
      "SLIM_BPR_Recommender: Epoch 58 of 300. Elapsed time 10.97 sec\n",
      "Processed 41629 (100.0%) in 0.60 sec. BPR loss is 3.94E-05. Sample per second: 68889\n",
      "SLIM_BPR_Recommender: Epoch 59 of 300. Elapsed time 11.15 sec\n",
      "Processed 41629 (100.0%) in 0.80 sec. BPR loss is 4.20E-05. Sample per second: 52268\n",
      "SLIM_BPR_Recommender: Epoch 60 of 300. Elapsed time 11.34 sec\n",
      "Processed 41629 (100.0%) in 1.00 sec. BPR loss is 4.01E-05. Sample per second: 41635\n",
      "SLIM_BPR_Recommender: Epoch 61 of 300. Elapsed time 11.55 sec\n",
      "Processed 41629 (100.0%) in 0.18 sec. BPR loss is 4.17E-05. Sample per second: 225745\n",
      "SLIM_BPR_Recommender: Epoch 62 of 300. Elapsed time 11.73 sec\n",
      "Processed 41629 (100.0%) in 0.37 sec. BPR loss is 4.08E-05. Sample per second: 113852\n",
      "SLIM_BPR_Recommender: Epoch 63 of 300. Elapsed time 11.91 sec\n",
      "Processed 41629 (100.0%) in 0.55 sec. BPR loss is 4.20E-05. Sample per second: 76166\n",
      "SLIM_BPR_Recommender: Epoch 64 of 300. Elapsed time 12.09 sec\n",
      "Processed 41629 (100.0%) in 0.73 sec. BPR loss is 4.37E-05. Sample per second: 57026\n",
      "SLIM_BPR_Recommender: Epoch 65 of 300. Elapsed time 12.28 sec\n",
      "Processed 41629 (100.0%) in 0.91 sec. BPR loss is 4.74E-05. Sample per second: 45629\n",
      "SLIM_BPR_Recommender: Epoch 66 of 300. Elapsed time 12.46 sec\n",
      "Processed 41629 (100.0%) in 1.10 sec. BPR loss is 4.54E-05. Sample per second: 37930\n",
      "SLIM_BPR_Recommender: Epoch 67 of 300. Elapsed time 12.64 sec\n",
      "Processed 41629 (100.0%) in 0.29 sec. BPR loss is 4.57E-05. Sample per second: 145546\n",
      "SLIM_BPR_Recommender: Epoch 68 of 300. Elapsed time 12.83 sec\n",
      "Processed 41629 (100.0%) in 0.47 sec. BPR loss is 4.65E-05. Sample per second: 88630\n",
      "SLIM_BPR_Recommender: Epoch 69 of 300. Elapsed time 13.02 sec\n",
      "Processed 41629 (100.0%) in 0.67 sec. BPR loss is 4.95E-05. Sample per second: 62337\n",
      "SLIM_BPR_Recommender: Epoch 70 of 300. Elapsed time 13.21 sec\n",
      "Processed 41629 (100.0%) in 0.86 sec. BPR loss is 4.97E-05. Sample per second: 48557\n",
      "SLIM_BPR_Recommender: Epoch 71 of 300. Elapsed time 13.40 sec\n",
      "Processed 41629 (100.0%) in 1.04 sec. BPR loss is 4.79E-05. Sample per second: 39897\n",
      "SLIM_BPR_Recommender: Epoch 72 of 300. Elapsed time 13.59 sec\n",
      "Processed 41629 (100.0%) in 0.23 sec. BPR loss is 4.78E-05. Sample per second: 180453\n",
      "SLIM_BPR_Recommender: Epoch 73 of 300. Elapsed time 13.78 sec\n",
      "Processed 41629 (100.0%) in 0.42 sec. BPR loss is 4.87E-05. Sample per second: 99340\n",
      "SLIM_BPR_Recommender: Epoch 74 of 300. Elapsed time 13.97 sec\n",
      "Processed 41629 (100.0%) in 0.60 sec. BPR loss is 5.08E-05. Sample per second: 68981\n",
      "SLIM_BPR_Recommender: Epoch 75 of 300. Elapsed time 14.15 sec\n",
      "Processed 41629 (100.0%) in 0.79 sec. BPR loss is 5.13E-05. Sample per second: 52522\n",
      "SLIM_BPR_Recommender: Epoch 76 of 300. Elapsed time 14.34 sec\n",
      "Processed 41629 (100.0%) in 0.97 sec. BPR loss is 5.29E-05. Sample per second: 42690\n",
      "SLIM_BPR_Recommender: Epoch 77 of 300. Elapsed time 14.52 sec\n",
      "Processed 41629 (100.0%) in 1.16 sec. BPR loss is 5.29E-05. Sample per second: 35900\n",
      "SLIM_BPR_Recommender: Epoch 78 of 300. Elapsed time 14.71 sec\n",
      "Processed 41629 (100.0%) in 0.34 sec. BPR loss is 5.37E-05. Sample per second: 121453\n",
      "SLIM_BPR_Recommender: Epoch 79 of 300. Elapsed time 14.89 sec\n",
      "Processed 41629 (100.0%) in 0.53 sec. BPR loss is 5.33E-05. Sample per second: 79181\n",
      "SLIM_BPR_Recommender: Epoch 80 of 300. Elapsed time 15.07 sec\n",
      "Processed 41629 (100.0%) in 0.71 sec. BPR loss is 5.40E-05. Sample per second: 58743\n",
      "SLIM_BPR_Recommender: Epoch 81 of 300. Elapsed time 15.26 sec\n",
      "Processed 41629 (100.0%) in 0.89 sec. BPR loss is 5.52E-05. Sample per second: 46618\n",
      "SLIM_BPR_Recommender: Epoch 82 of 300. Elapsed time 15.44 sec\n",
      "Processed 41629 (100.0%) in 1.07 sec. BPR loss is 5.66E-05. Sample per second: 38810\n",
      "SLIM_BPR_Recommender: Epoch 83 of 300. Elapsed time 15.62 sec\n",
      "Processed 41629 (100.0%) in 0.27 sec. BPR loss is 5.92E-05. Sample per second: 153741\n",
      "SLIM_BPR_Recommender: Epoch 84 of 300. Elapsed time 15.82 sec\n",
      "Processed 41629 (100.0%) in 0.47 sec. BPR loss is 5.94E-05. Sample per second: 88994\n",
      "SLIM_BPR_Recommender: Epoch 85 of 300. Elapsed time 16.02 sec\n",
      "Processed 41629 (100.0%) in 0.67 sec. BPR loss is 5.83E-05. Sample per second: 62213\n",
      "SLIM_BPR_Recommender: Epoch 86 of 300. Elapsed time 16.22 sec\n",
      "Processed 41629 (100.0%) in 0.86 sec. BPR loss is 5.55E-05. Sample per second: 48457\n",
      "SLIM_BPR_Recommender: Epoch 87 of 300. Elapsed time 16.41 sec\n",
      "Processed 41629 (100.0%) in 1.04 sec. BPR loss is 6.01E-05. Sample per second: 39940\n",
      "SLIM_BPR_Recommender: Epoch 88 of 300. Elapsed time 16.59 sec\n",
      "Processed 41629 (100.0%) in 0.23 sec. BPR loss is 5.78E-05. Sample per second: 179395\n",
      "SLIM_BPR_Recommender: Epoch 89 of 300. Elapsed time 16.78 sec\n",
      "Processed 41629 (100.0%) in 0.42 sec. BPR loss is 6.03E-05. Sample per second: 99554\n",
      "SLIM_BPR_Recommender: Epoch 90 of 300. Elapsed time 16.97 sec\n",
      "Processed 41629 (100.0%) in 0.60 sec. BPR loss is 5.96E-05. Sample per second: 68826\n",
      "SLIM_BPR_Recommender: Epoch 91 of 300. Elapsed time 17.15 sec\n",
      "Processed 41629 (100.0%) in 0.78 sec. BPR loss is 5.82E-05. Sample per second: 53068\n",
      "SLIM_BPR_Recommender: Epoch 92 of 300. Elapsed time 17.33 sec\n",
      "Processed 41629 (100.0%) in 0.97 sec. BPR loss is 6.50E-05. Sample per second: 43109\n",
      "SLIM_BPR_Recommender: Epoch 93 of 300. Elapsed time 17.51 sec\n",
      "Processed 41629 (100.0%) in 1.15 sec. BPR loss is 6.38E-05. Sample per second: 36100\n",
      "SLIM_BPR_Recommender: Epoch 94 of 300. Elapsed time 17.70 sec\n",
      "Processed 41629 (100.0%) in 0.34 sec. BPR loss is 6.47E-05. Sample per second: 123158\n",
      "SLIM_BPR_Recommender: Epoch 95 of 300. Elapsed time 17.89 sec\n",
      "Processed 41629 (100.0%) in 0.53 sec. BPR loss is 6.48E-05. Sample per second: 78598\n",
      "SLIM_BPR_Recommender: Epoch 96 of 300. Elapsed time 18.08 sec\n",
      "Processed 41629 (100.0%) in 0.71 sec. BPR loss is 6.49E-05. Sample per second: 58522\n",
      "SLIM_BPR_Recommender: Epoch 97 of 300. Elapsed time 18.26 sec\n",
      "Processed 41629 (100.0%) in 0.90 sec. BPR loss is 6.38E-05. Sample per second: 46346\n",
      "SLIM_BPR_Recommender: Epoch 98 of 300. Elapsed time 18.45 sec\n",
      "Processed 41629 (100.0%) in 1.09 sec. BPR loss is 6.45E-05. Sample per second: 38305\n",
      "SLIM_BPR_Recommender: Epoch 99 of 300. Elapsed time 18.63 sec\n",
      "Processed 41629 (100.0%) in 0.27 sec. BPR loss is 6.90E-05. Sample per second: 151277\n",
      "SLIM_BPR_Recommender: Epoch 100 of 300. Elapsed time 18.82 sec\n",
      "Processed 41629 (100.0%) in 0.46 sec. BPR loss is 6.99E-05. Sample per second: 91066\n",
      "SLIM_BPR_Recommender: Epoch 101 of 300. Elapsed time 19.01 sec\n",
      "Processed 41629 (100.0%) in 0.64 sec. BPR loss is 6.75E-05. Sample per second: 65011\n",
      "SLIM_BPR_Recommender: Epoch 102 of 300. Elapsed time 19.19 sec\n",
      "Processed 41629 (100.0%) in 0.82 sec. BPR loss is 6.74E-05. Sample per second: 50693\n",
      "SLIM_BPR_Recommender: Epoch 103 of 300. Elapsed time 19.37 sec\n",
      "Processed 41629 (100.0%) in 1.00 sec. BPR loss is 7.05E-05. Sample per second: 41421\n",
      "SLIM_BPR_Recommender: Epoch 104 of 300. Elapsed time 19.55 sec\n",
      "Processed 41629 (100.0%) in 0.19 sec. BPR loss is 7.18E-05. Sample per second: 216864\n",
      "SLIM_BPR_Recommender: Epoch 105 of 300. Elapsed time 19.74 sec\n",
      "Processed 41629 (100.0%) in 0.38 sec. BPR loss is 7.01E-05. Sample per second: 109062\n",
      "SLIM_BPR_Recommender: Epoch 106 of 300. Elapsed time 19.93 sec\n",
      "Processed 41629 (100.0%) in 0.57 sec. BPR loss is 7.46E-05. Sample per second: 73394\n",
      "SLIM_BPR_Recommender: Epoch 107 of 300. Elapsed time 20.11 sec\n",
      "Processed 41629 (100.0%) in 0.75 sec. BPR loss is 7.19E-05. Sample per second: 55286\n",
      "SLIM_BPR_Recommender: Epoch 108 of 300. Elapsed time 20.30 sec\n",
      "Processed 41629 (100.0%) in 0.94 sec. BPR loss is 7.65E-05. Sample per second: 44315\n",
      "SLIM_BPR_Recommender: Epoch 109 of 300. Elapsed time 20.49 sec\n",
      "Processed 41629 (100.0%) in 1.13 sec. BPR loss is 7.89E-05. Sample per second: 36825\n",
      "SLIM_BPR_Recommender: Epoch 110 of 300. Elapsed time 20.68 sec\n",
      "Processed 41629 (100.0%) in 0.32 sec. BPR loss is 7.69E-05. Sample per second: 130956\n",
      "SLIM_BPR_Recommender: Epoch 111 of 300. Elapsed time 20.86 sec\n",
      "Processed 41629 (100.0%) in 0.50 sec. BPR loss is 7.78E-05. Sample per second: 82772\n",
      "SLIM_BPR_Recommender: Epoch 112 of 300. Elapsed time 21.05 sec\n",
      "Processed 41629 (100.0%) in 0.69 sec. BPR loss is 7.81E-05. Sample per second: 60369\n",
      "SLIM_BPR_Recommender: Epoch 113 of 300. Elapsed time 21.24 sec\n",
      "Processed 41629 (100.0%) in 0.88 sec. BPR loss is 7.84E-05. Sample per second: 47554\n",
      "SLIM_BPR_Recommender: Epoch 114 of 300. Elapsed time 21.42 sec\n",
      "Processed 41629 (100.0%) in 1.06 sec. BPR loss is 7.46E-05. Sample per second: 39280\n",
      "SLIM_BPR_Recommender: Epoch 115 of 300. Elapsed time 21.61 sec\n",
      "Processed 41629 (100.0%) in 0.24 sec. BPR loss is 7.92E-05. Sample per second: 171672\n",
      "SLIM_BPR_Recommender: Epoch 116 of 300. Elapsed time 21.79 sec\n",
      "Processed 41629 (100.0%) in 0.43 sec. BPR loss is 8.01E-05. Sample per second: 97330\n",
      "SLIM_BPR_Recommender: Epoch 117 of 300. Elapsed time 21.97 sec\n",
      "Processed 41629 (100.0%) in 0.62 sec. BPR loss is 7.85E-05. Sample per second: 67670\n",
      "SLIM_BPR_Recommender: Epoch 118 of 300. Elapsed time 22.16 sec\n",
      "Processed 41629 (100.0%) in 0.81 sec. BPR loss is 7.92E-05. Sample per second: 51430\n",
      "SLIM_BPR_Recommender: Epoch 119 of 300. Elapsed time 22.36 sec\n",
      "Processed 41629 (100.0%) in 1.01 sec. BPR loss is 8.20E-05. Sample per second: 41022\n",
      "SLIM_BPR_Recommender: Epoch 120 of 300. Elapsed time 22.56 sec\n",
      "Processed 41629 (100.0%) in 0.21 sec. BPR loss is 8.10E-05. Sample per second: 202266\n",
      "SLIM_BPR_Recommender: Epoch 121 of 300. Elapsed time 22.75 sec\n",
      "Processed 41629 (100.0%) in 0.40 sec. BPR loss is 8.21E-05. Sample per second: 105214\n",
      "SLIM_BPR_Recommender: Epoch 122 of 300. Elapsed time 22.94 sec\n",
      "Processed 41629 (100.0%) in 0.58 sec. BPR loss is 8.61E-05. Sample per second: 71495\n",
      "SLIM_BPR_Recommender: Epoch 123 of 300. Elapsed time 23.13 sec\n",
      "Processed 41629 (100.0%) in 0.76 sec. BPR loss is 8.62E-05. Sample per second: 54405\n",
      "SLIM_BPR_Recommender: Epoch 124 of 300. Elapsed time 23.31 sec\n",
      "Processed 41629 (100.0%) in 0.95 sec. BPR loss is 8.40E-05. Sample per second: 43727\n",
      "SLIM_BPR_Recommender: Epoch 125 of 300. Elapsed time 23.50 sec\n",
      "Processed 41629 (100.0%) in 1.14 sec. BPR loss is 9.26E-05. Sample per second: 36512\n",
      "SLIM_BPR_Recommender: Epoch 126 of 300. Elapsed time 23.69 sec\n",
      "Processed 41629 (100.0%) in 0.33 sec. BPR loss is 8.04E-05. Sample per second: 125766\n",
      "SLIM_BPR_Recommender: Epoch 127 of 300. Elapsed time 23.88 sec\n",
      "Processed 41629 (100.0%) in 0.52 sec. BPR loss is 8.60E-05. Sample per second: 79991\n",
      "SLIM_BPR_Recommender: Epoch 128 of 300. Elapsed time 24.07 sec\n",
      "Processed 41629 (100.0%) in 0.71 sec. BPR loss is 8.87E-05. Sample per second: 58837\n",
      "SLIM_BPR_Recommender: Epoch 129 of 300. Elapsed time 24.25 sec\n",
      "Processed 41629 (100.0%) in 0.89 sec. BPR loss is 8.94E-05. Sample per second: 46615\n",
      "SLIM_BPR_Recommender: Epoch 130 of 300. Elapsed time 24.44 sec\n",
      "Processed 41629 (100.0%) in 1.08 sec. BPR loss is 8.61E-05. Sample per second: 38681\n",
      "SLIM_BPR_Recommender: Epoch 131 of 300. Elapsed time 24.62 sec\n",
      "Processed 41629 (100.0%) in 0.26 sec. BPR loss is 8.77E-05. Sample per second: 161017\n",
      "SLIM_BPR_Recommender: Epoch 132 of 300. Elapsed time 24.81 sec\n",
      "Processed 41629 (100.0%) in 0.44 sec. BPR loss is 9.28E-05. Sample per second: 94187\n",
      "SLIM_BPR_Recommender: Epoch 133 of 300. Elapsed time 24.99 sec\n",
      "Processed 41629 (100.0%) in 0.62 sec. BPR loss is 9.47E-05. Sample per second: 66722\n",
      "SLIM_BPR_Recommender: Epoch 134 of 300. Elapsed time 25.17 sec\n",
      "Processed 41629 (100.0%) in 0.80 sec. BPR loss is 9.28E-05. Sample per second: 51814\n",
      "SLIM_BPR_Recommender: Epoch 135 of 300. Elapsed time 25.35 sec\n",
      "Processed 41629 (100.0%) in 0.99 sec. BPR loss is 9.36E-05. Sample per second: 42051\n",
      "SLIM_BPR_Recommender: Epoch 136 of 300. Elapsed time 25.54 sec\n",
      "Processed 41629 (100.0%) in 1.18 sec. BPR loss is 9.39E-05. Sample per second: 35408\n",
      "SLIM_BPR_Recommender: Epoch 137 of 300. Elapsed time 25.72 sec\n",
      "Processed 41629 (100.0%) in 0.36 sec. BPR loss is 9.23E-05. Sample per second: 116391\n",
      "SLIM_BPR_Recommender: Epoch 138 of 300. Elapsed time 25.90 sec\n",
      "Processed 41629 (100.0%) in 0.54 sec. BPR loss is 9.13E-05. Sample per second: 77308\n",
      "SLIM_BPR_Recommender: Epoch 139 of 300. Elapsed time 26.09 sec\n",
      "Processed 41629 (100.0%) in 0.72 sec. BPR loss is 9.53E-05. Sample per second: 58111\n",
      "SLIM_BPR_Recommender: Epoch 140 of 300. Elapsed time 26.26 sec\n",
      "Processed 41629 (100.0%) in 0.91 sec. BPR loss is 9.37E-05. Sample per second: 45764\n",
      "SLIM_BPR_Recommender: Epoch 141 of 300. Elapsed time 26.46 sec\n",
      "Processed 41629 (100.0%) in 1.09 sec. BPR loss is 9.58E-05. Sample per second: 38072\n",
      "SLIM_BPR_Recommender: Epoch 142 of 300. Elapsed time 26.64 sec\n",
      "Processed 41629 (100.0%) in 0.27 sec. BPR loss is 9.70E-05. Sample per second: 152363\n",
      "SLIM_BPR_Recommender: Epoch 143 of 300. Elapsed time 26.82 sec\n",
      "Processed 41629 (100.0%) in 0.46 sec. BPR loss is 9.64E-05. Sample per second: 91252\n",
      "SLIM_BPR_Recommender: Epoch 144 of 300. Elapsed time 27.00 sec\n",
      "Processed 41629 (100.0%) in 0.64 sec. BPR loss is 9.82E-05. Sample per second: 65134\n",
      "SLIM_BPR_Recommender: Epoch 145 of 300. Elapsed time 27.19 sec\n",
      "Processed 41629 (100.0%) in 0.83 sec. BPR loss is 1.01E-04. Sample per second: 50378\n",
      "SLIM_BPR_Recommender: Epoch 146 of 300. Elapsed time 27.37 sec\n",
      "Processed 41629 (100.0%) in 1.01 sec. BPR loss is 1.04E-04. Sample per second: 41158\n",
      "SLIM_BPR_Recommender: Epoch 147 of 300. Elapsed time 27.56 sec\n",
      "Processed 41629 (100.0%) in 0.20 sec. BPR loss is 9.56E-05. Sample per second: 207897\n",
      "SLIM_BPR_Recommender: Epoch 148 of 300. Elapsed time 27.75 sec\n",
      "Processed 41629 (100.0%) in 0.38 sec. BPR loss is 1.03E-04. Sample per second: 108752\n",
      "SLIM_BPR_Recommender: Epoch 149 of 300. Elapsed time 27.93 sec\n",
      "Processed 41629 (100.0%) in 0.56 sec. BPR loss is 1.02E-04. Sample per second: 73694\n",
      "SLIM_BPR_Recommender: Epoch 150 of 300. Elapsed time 28.11 sec\n",
      "Processed 41629 (100.0%) in 0.74 sec. BPR loss is 1.05E-04. Sample per second: 55915\n",
      "SLIM_BPR_Recommender: Epoch 151 of 300. Elapsed time 28.29 sec\n",
      "Processed 41629 (100.0%) in 0.92 sec. BPR loss is 9.85E-05. Sample per second: 45040\n",
      "SLIM_BPR_Recommender: Epoch 152 of 300. Elapsed time 28.47 sec\n",
      "Processed 41629 (100.0%) in 1.11 sec. BPR loss is 1.05E-04. Sample per second: 37653\n",
      "SLIM_BPR_Recommender: Epoch 153 of 300. Elapsed time 28.65 sec\n",
      "Processed 41629 (100.0%) in 0.29 sec. BPR loss is 1.06E-04. Sample per second: 141023\n",
      "SLIM_BPR_Recommender: Epoch 154 of 300. Elapsed time 28.84 sec\n",
      "Processed 41629 (100.0%) in 0.48 sec. BPR loss is 1.04E-04. Sample per second: 87153\n",
      "SLIM_BPR_Recommender: Epoch 155 of 300. Elapsed time 29.02 sec\n",
      "Processed 41629 (100.0%) in 0.66 sec. BPR loss is 1.10E-04. Sample per second: 62855\n",
      "SLIM_BPR_Recommender: Epoch 156 of 300. Elapsed time 29.21 sec\n",
      "Processed 41629 (100.0%) in 0.85 sec. BPR loss is 1.06E-04. Sample per second: 48836\n",
      "SLIM_BPR_Recommender: Epoch 157 of 300. Elapsed time 29.40 sec\n",
      "Processed 41629 (100.0%) in 1.04 sec. BPR loss is 1.03E-04. Sample per second: 39974\n",
      "SLIM_BPR_Recommender: Epoch 158 of 300. Elapsed time 29.59 sec\n",
      "Processed 41629 (100.0%) in 0.23 sec. BPR loss is 1.05E-04. Sample per second: 183813\n",
      "SLIM_BPR_Recommender: Epoch 159 of 300. Elapsed time 29.77 sec\n",
      "Processed 41629 (100.0%) in 0.41 sec. BPR loss is 1.07E-04. Sample per second: 101193\n",
      "SLIM_BPR_Recommender: Epoch 160 of 300. Elapsed time 29.96 sec\n",
      "Processed 41629 (100.0%) in 0.60 sec. BPR loss is 1.05E-04. Sample per second: 69612\n",
      "SLIM_BPR_Recommender: Epoch 161 of 300. Elapsed time 30.14 sec\n",
      "Processed 41629 (100.0%) in 0.78 sec. BPR loss is 1.13E-04. Sample per second: 53254\n",
      "SLIM_BPR_Recommender: Epoch 162 of 300. Elapsed time 30.33 sec\n",
      "Processed 41629 (100.0%) in 0.97 sec. BPR loss is 1.12E-04. Sample per second: 42715\n",
      "SLIM_BPR_Recommender: Epoch 163 of 300. Elapsed time 30.52 sec\n",
      "Processed 41629 (100.0%) in 1.16 sec. BPR loss is 1.10E-04. Sample per second: 35838\n",
      "SLIM_BPR_Recommender: Epoch 164 of 300. Elapsed time 30.71 sec\n",
      "Processed 41629 (100.0%) in 0.35 sec. BPR loss is 1.14E-04. Sample per second: 120523\n",
      "SLIM_BPR_Recommender: Epoch 165 of 300. Elapsed time 30.89 sec\n",
      "Processed 41629 (100.0%) in 0.53 sec. BPR loss is 1.11E-04. Sample per second: 78610\n",
      "SLIM_BPR_Recommender: Epoch 166 of 300. Elapsed time 31.08 sec\n",
      "Processed 41629 (100.0%) in 0.71 sec. BPR loss is 1.14E-04. Sample per second: 58897\n",
      "SLIM_BPR_Recommender: Epoch 167 of 300. Elapsed time 31.25 sec\n",
      "Processed 41629 (100.0%) in 0.89 sec. BPR loss is 1.12E-04. Sample per second: 46837\n",
      "SLIM_BPR_Recommender: Epoch 168 of 300. Elapsed time 31.44 sec\n",
      "Processed 41629 (100.0%) in 1.07 sec. BPR loss is 1.16E-04. Sample per second: 38884\n",
      "SLIM_BPR_Recommender: Epoch 169 of 300. Elapsed time 31.62 sec\n",
      "Processed 41629 (100.0%) in 0.25 sec. BPR loss is 1.19E-04. Sample per second: 167120\n",
      "SLIM_BPR_Recommender: Epoch 170 of 300. Elapsed time 31.80 sec\n",
      "Processed 41629 (100.0%) in 0.44 sec. BPR loss is 1.18E-04. Sample per second: 95541\n",
      "SLIM_BPR_Recommender: Epoch 171 of 300. Elapsed time 31.98 sec\n",
      "Processed 41629 (100.0%) in 0.62 sec. BPR loss is 1.19E-04. Sample per second: 67020\n",
      "SLIM_BPR_Recommender: Epoch 172 of 300. Elapsed time 32.17 sec\n",
      "Processed 41629 (100.0%) in 0.81 sec. BPR loss is 1.13E-04. Sample per second: 51378\n",
      "SLIM_BPR_Recommender: Epoch 173 of 300. Elapsed time 32.36 sec\n",
      "Processed 41629 (100.0%) in 1.00 sec. BPR loss is 1.19E-04. Sample per second: 41657\n",
      "SLIM_BPR_Recommender: Epoch 174 of 300. Elapsed time 32.55 sec\n",
      "Processed 41629 (100.0%) in 0.19 sec. BPR loss is 1.26E-04. Sample per second: 217325\n",
      "SLIM_BPR_Recommender: Epoch 175 of 300. Elapsed time 32.74 sec\n",
      "Processed 41629 (100.0%) in 0.38 sec. BPR loss is 1.16E-04. Sample per second: 109678\n",
      "SLIM_BPR_Recommender: Epoch 176 of 300. Elapsed time 32.93 sec\n",
      "Processed 41629 (100.0%) in 0.56 sec. BPR loss is 1.21E-04. Sample per second: 73692\n",
      "SLIM_BPR_Recommender: Epoch 177 of 300. Elapsed time 33.11 sec\n",
      "Processed 41629 (100.0%) in 0.75 sec. BPR loss is 1.24E-04. Sample per second: 55515\n",
      "SLIM_BPR_Recommender: Epoch 178 of 300. Elapsed time 33.30 sec\n",
      "Processed 41629 (100.0%) in 0.96 sec. BPR loss is 1.27E-04. Sample per second: 43336\n",
      "SLIM_BPR_Recommender: Epoch 179 of 300. Elapsed time 33.51 sec\n",
      "Processed 41629 (100.0%) in 1.15 sec. BPR loss is 1.20E-04. Sample per second: 36189\n",
      "SLIM_BPR_Recommender: Epoch 180 of 300. Elapsed time 33.70 sec\n",
      "Processed 41629 (100.0%) in 0.34 sec. BPR loss is 1.24E-04. Sample per second: 124145\n",
      "SLIM_BPR_Recommender: Epoch 181 of 300. Elapsed time 33.88 sec\n",
      "Processed 41629 (100.0%) in 0.52 sec. BPR loss is 1.25E-04. Sample per second: 80188\n",
      "SLIM_BPR_Recommender: Epoch 182 of 300. Elapsed time 34.07 sec\n",
      "Processed 41629 (100.0%) in 0.70 sec. BPR loss is 1.27E-04. Sample per second: 59789\n",
      "SLIM_BPR_Recommender: Epoch 183 of 300. Elapsed time 34.24 sec\n",
      "Processed 41629 (100.0%) in 0.88 sec. BPR loss is 1.27E-04. Sample per second: 47350\n",
      "SLIM_BPR_Recommender: Epoch 184 of 300. Elapsed time 34.43 sec\n",
      "Processed 41629 (100.0%) in 1.06 sec. BPR loss is 1.26E-04. Sample per second: 39339\n",
      "SLIM_BPR_Recommender: Epoch 185 of 300. Elapsed time 34.61 sec\n",
      "Processed 41629 (100.0%) in 0.24 sec. BPR loss is 1.24E-04. Sample per second: 174471\n",
      "SLIM_BPR_Recommender: Epoch 186 of 300. Elapsed time 34.79 sec\n",
      "Processed 41629 (100.0%) in 0.42 sec. BPR loss is 1.29E-04. Sample per second: 99374\n",
      "SLIM_BPR_Recommender: Epoch 187 of 300. Elapsed time 34.97 sec\n",
      "Processed 41629 (100.0%) in 0.60 sec. BPR loss is 1.22E-04. Sample per second: 68993\n",
      "SLIM_BPR_Recommender: Epoch 188 of 300. Elapsed time 35.15 sec\n",
      "Processed 41629 (100.0%) in 0.79 sec. BPR loss is 1.32E-04. Sample per second: 52909\n",
      "SLIM_BPR_Recommender: Epoch 189 of 300. Elapsed time 35.33 sec\n",
      "Processed 41629 (100.0%) in 0.97 sec. BPR loss is 1.31E-04. Sample per second: 42813\n",
      "SLIM_BPR_Recommender: Epoch 190 of 300. Elapsed time 35.52 sec\n",
      "Processed 41629 (100.0%) in 1.16 sec. BPR loss is 1.29E-04. Sample per second: 35894\n",
      "SLIM_BPR_Recommender: Epoch 191 of 300. Elapsed time 35.71 sec\n",
      "Processed 41629 (100.0%) in 0.34 sec. BPR loss is 1.33E-04. Sample per second: 121538\n",
      "SLIM_BPR_Recommender: Epoch 192 of 300. Elapsed time 35.89 sec\n",
      "Processed 41629 (100.0%) in 0.53 sec. BPR loss is 1.35E-04. Sample per second: 78188\n",
      "SLIM_BPR_Recommender: Epoch 193 of 300. Elapsed time 36.08 sec\n",
      "Processed 41629 (100.0%) in 0.72 sec. BPR loss is 1.38E-04. Sample per second: 57876\n",
      "SLIM_BPR_Recommender: Epoch 194 of 300. Elapsed time 36.27 sec\n",
      "Processed 41629 (100.0%) in 0.91 sec. BPR loss is 1.30E-04. Sample per second: 45777\n",
      "SLIM_BPR_Recommender: Epoch 195 of 300. Elapsed time 36.46 sec\n",
      "Processed 41629 (100.0%) in 1.10 sec. BPR loss is 1.36E-04. Sample per second: 37679\n",
      "SLIM_BPR_Recommender: Epoch 196 of 300. Elapsed time 36.65 sec\n",
      "Processed 41629 (100.0%) in 0.30 sec. BPR loss is 1.36E-04. Sample per second: 140778\n",
      "SLIM_BPR_Recommender: Epoch 197 of 300. Elapsed time 36.84 sec\n",
      "Processed 41629 (100.0%) in 0.48 sec. BPR loss is 1.30E-04. Sample per second: 86484\n",
      "SLIM_BPR_Recommender: Epoch 198 of 300. Elapsed time 37.03 sec\n",
      "Processed 41629 (100.0%) in 0.67 sec. BPR loss is 1.36E-04. Sample per second: 62511\n",
      "SLIM_BPR_Recommender: Epoch 199 of 300. Elapsed time 37.21 sec\n",
      "Processed 41629 (100.0%) in 0.85 sec. BPR loss is 1.40E-04. Sample per second: 48875\n",
      "SLIM_BPR_Recommender: Epoch 200 of 300. Elapsed time 37.40 sec\n",
      "Processed 41629 (100.0%) in 1.03 sec. BPR loss is 1.34E-04. Sample per second: 40337\n",
      "SLIM_BPR_Recommender: Epoch 201 of 300. Elapsed time 37.58 sec\n",
      "Processed 41629 (100.0%) in 0.22 sec. BPR loss is 1.37E-04. Sample per second: 191461\n",
      "SLIM_BPR_Recommender: Epoch 202 of 300. Elapsed time 37.76 sec\n",
      "Processed 41629 (100.0%) in 0.40 sec. BPR loss is 1.38E-04. Sample per second: 103602\n",
      "SLIM_BPR_Recommender: Epoch 203 of 300. Elapsed time 37.95 sec\n",
      "Processed 41629 (100.0%) in 0.58 sec. BPR loss is 1.40E-04. Sample per second: 71353\n",
      "SLIM_BPR_Recommender: Epoch 204 of 300. Elapsed time 38.13 sec\n",
      "Processed 41629 (100.0%) in 0.77 sec. BPR loss is 1.37E-04. Sample per second: 54378\n",
      "SLIM_BPR_Recommender: Epoch 205 of 300. Elapsed time 38.31 sec\n",
      "Processed 41629 (100.0%) in 0.95 sec. BPR loss is 1.36E-04. Sample per second: 44002\n",
      "SLIM_BPR_Recommender: Epoch 206 of 300. Elapsed time 38.49 sec\n",
      "Processed 41629 (100.0%) in 1.13 sec. BPR loss is 1.34E-04. Sample per second: 36812\n",
      "SLIM_BPR_Recommender: Epoch 207 of 300. Elapsed time 38.68 sec\n",
      "Processed 41629 (100.0%) in 0.31 sec. BPR loss is 1.41E-04. Sample per second: 131889\n",
      "SLIM_BPR_Recommender: Epoch 208 of 300. Elapsed time 38.86 sec\n",
      "Processed 41629 (100.0%) in 0.50 sec. BPR loss is 1.46E-04. Sample per second: 83568\n",
      "SLIM_BPR_Recommender: Epoch 209 of 300. Elapsed time 39.04 sec\n",
      "Processed 41629 (100.0%) in 0.68 sec. BPR loss is 1.45E-04. Sample per second: 61497\n",
      "SLIM_BPR_Recommender: Epoch 210 of 300. Elapsed time 39.23 sec\n",
      "Processed 41629 (100.0%) in 0.86 sec. BPR loss is 1.40E-04. Sample per second: 48520\n",
      "SLIM_BPR_Recommender: Epoch 211 of 300. Elapsed time 39.40 sec\n",
      "Processed 41629 (100.0%) in 1.04 sec. BPR loss is 1.51E-04. Sample per second: 39979\n",
      "SLIM_BPR_Recommender: Epoch 212 of 300. Elapsed time 39.59 sec\n",
      "Processed 41629 (100.0%) in 0.22 sec. BPR loss is 1.41E-04. Sample per second: 186577\n",
      "SLIM_BPR_Recommender: Epoch 213 of 300. Elapsed time 39.77 sec\n",
      "Processed 41629 (100.0%) in 0.41 sec. BPR loss is 1.48E-04. Sample per second: 102494\n",
      "SLIM_BPR_Recommender: Epoch 214 of 300. Elapsed time 39.95 sec\n",
      "Processed 41629 (100.0%) in 0.59 sec. BPR loss is 1.47E-04. Sample per second: 70599\n",
      "SLIM_BPR_Recommender: Epoch 215 of 300. Elapsed time 40.14 sec\n",
      "Processed 41629 (100.0%) in 0.77 sec. BPR loss is 1.47E-04. Sample per second: 53853\n",
      "SLIM_BPR_Recommender: Epoch 216 of 300. Elapsed time 40.32 sec\n",
      "Processed 41629 (100.0%) in 0.96 sec. BPR loss is 1.44E-04. Sample per second: 43404\n",
      "SLIM_BPR_Recommender: Epoch 217 of 300. Elapsed time 40.51 sec\n",
      "Processed 41629 (100.0%) in 1.15 sec. BPR loss is 1.56E-04. Sample per second: 36162\n",
      "SLIM_BPR_Recommender: Epoch 218 of 300. Elapsed time 40.70 sec\n",
      "Processed 41629 (100.0%) in 0.34 sec. BPR loss is 1.51E-04. Sample per second: 123079\n",
      "SLIM_BPR_Recommender: Epoch 219 of 300. Elapsed time 40.88 sec\n",
      "Processed 41629 (100.0%) in 0.52 sec. BPR loss is 1.53E-04. Sample per second: 79429\n",
      "SLIM_BPR_Recommender: Epoch 220 of 300. Elapsed time 41.07 sec\n",
      "Processed 41629 (100.0%) in 0.71 sec. BPR loss is 1.47E-04. Sample per second: 58552\n",
      "SLIM_BPR_Recommender: Epoch 221 of 300. Elapsed time 41.26 sec\n",
      "Processed 41629 (100.0%) in 0.90 sec. BPR loss is 1.44E-04. Sample per second: 46431\n",
      "SLIM_BPR_Recommender: Epoch 222 of 300. Elapsed time 41.44 sec\n",
      "Processed 41629 (100.0%) in 1.08 sec. BPR loss is 1.48E-04. Sample per second: 38482\n",
      "SLIM_BPR_Recommender: Epoch 223 of 300. Elapsed time 41.63 sec\n",
      "Processed 41629 (100.0%) in 0.27 sec. BPR loss is 1.50E-04. Sample per second: 153716\n",
      "SLIM_BPR_Recommender: Epoch 224 of 300. Elapsed time 41.82 sec\n",
      "Processed 41629 (100.0%) in 0.46 sec. BPR loss is 1.62E-04. Sample per second: 91111\n",
      "SLIM_BPR_Recommender: Epoch 225 of 300. Elapsed time 42.00 sec\n",
      "Processed 41629 (100.0%) in 0.64 sec. BPR loss is 1.54E-04. Sample per second: 65024\n",
      "SLIM_BPR_Recommender: Epoch 226 of 300. Elapsed time 42.19 sec\n",
      "Processed 41629 (100.0%) in 0.83 sec. BPR loss is 1.55E-04. Sample per second: 50217\n",
      "SLIM_BPR_Recommender: Epoch 227 of 300. Elapsed time 42.38 sec\n",
      "Processed 41629 (100.0%) in 1.02 sec. BPR loss is 1.49E-04. Sample per second: 40822\n",
      "SLIM_BPR_Recommender: Epoch 228 of 300. Elapsed time 42.57 sec\n",
      "Processed 41629 (100.0%) in 0.21 sec. BPR loss is 1.57E-04. Sample per second: 200666\n",
      "SLIM_BPR_Recommender: Epoch 229 of 300. Elapsed time 42.75 sec\n",
      "Processed 41629 (100.0%) in 0.40 sec. BPR loss is 1.57E-04. Sample per second: 104820\n",
      "SLIM_BPR_Recommender: Epoch 230 of 300. Elapsed time 42.94 sec\n",
      "Processed 41629 (100.0%) in 0.59 sec. BPR loss is 1.53E-04. Sample per second: 71129\n",
      "SLIM_BPR_Recommender: Epoch 231 of 300. Elapsed time 43.13 sec\n",
      "Processed 41629 (100.0%) in 0.78 sec. BPR loss is 1.61E-04. Sample per second: 53544\n",
      "SLIM_BPR_Recommender: Epoch 232 of 300. Elapsed time 43.32 sec\n",
      "Processed 41629 (100.0%) in 0.97 sec. BPR loss is 1.56E-04. Sample per second: 42987\n",
      "SLIM_BPR_Recommender: Epoch 233 of 300. Elapsed time 43.52 sec\n",
      "Processed 41629 (100.0%) in 1.15 sec. BPR loss is 1.55E-04. Sample per second: 36098\n",
      "SLIM_BPR_Recommender: Epoch 234 of 300. Elapsed time 43.70 sec\n",
      "Processed 41629 (100.0%) in 0.34 sec. BPR loss is 1.59E-04. Sample per second: 121003\n",
      "SLIM_BPR_Recommender: Epoch 235 of 300. Elapsed time 43.89 sec\n",
      "Processed 41629 (100.0%) in 0.53 sec. BPR loss is 1.58E-04. Sample per second: 78872\n",
      "SLIM_BPR_Recommender: Epoch 236 of 300. Elapsed time 44.07 sec\n",
      "Processed 41629 (100.0%) in 0.71 sec. BPR loss is 1.62E-04. Sample per second: 58240\n",
      "SLIM_BPR_Recommender: Epoch 237 of 300. Elapsed time 44.26 sec\n",
      "Processed 41629 (100.0%) in 0.92 sec. BPR loss is 1.55E-04. Sample per second: 45043\n",
      "SLIM_BPR_Recommender: Epoch 238 of 300. Elapsed time 44.47 sec\n",
      "Processed 41629 (100.0%) in 1.11 sec. BPR loss is 1.74E-04. Sample per second: 37653\n",
      "SLIM_BPR_Recommender: Epoch 239 of 300. Elapsed time 44.65 sec\n",
      "Processed 41629 (100.0%) in 0.29 sec. BPR loss is 1.62E-04. Sample per second: 144460\n",
      "SLIM_BPR_Recommender: Epoch 240 of 300. Elapsed time 44.83 sec\n",
      "Processed 41629 (100.0%) in 0.47 sec. BPR loss is 1.63E-04. Sample per second: 88795\n",
      "SLIM_BPR_Recommender: Epoch 241 of 300. Elapsed time 45.02 sec\n",
      "Processed 41629 (100.0%) in 0.65 sec. BPR loss is 1.69E-04. Sample per second: 64070\n",
      "SLIM_BPR_Recommender: Epoch 242 of 300. Elapsed time 45.20 sec\n",
      "Processed 41629 (100.0%) in 0.83 sec. BPR loss is 1.77E-04. Sample per second: 50213\n",
      "SLIM_BPR_Recommender: Epoch 243 of 300. Elapsed time 45.38 sec\n",
      "Processed 41629 (100.0%) in 1.01 sec. BPR loss is 1.68E-04. Sample per second: 41213\n",
      "SLIM_BPR_Recommender: Epoch 244 of 300. Elapsed time 45.56 sec\n",
      "Processed 41629 (100.0%) in 0.19 sec. BPR loss is 1.71E-04. Sample per second: 219658\n",
      "SLIM_BPR_Recommender: Epoch 245 of 300. Elapsed time 45.74 sec\n",
      "Processed 41629 (100.0%) in 0.36 sec. BPR loss is 1.69E-04. Sample per second: 114412\n",
      "SLIM_BPR_Recommender: Epoch 246 of 300. Elapsed time 45.91 sec\n",
      "Processed 41629 (100.0%) in 0.54 sec. BPR loss is 1.73E-04. Sample per second: 76915\n",
      "SLIM_BPR_Recommender: Epoch 247 of 300. Elapsed time 46.09 sec\n",
      "Processed 41629 (100.0%) in 0.72 sec. BPR loss is 1.79E-04. Sample per second: 57882\n",
      "SLIM_BPR_Recommender: Epoch 248 of 300. Elapsed time 46.27 sec\n",
      "Processed 41629 (100.0%) in 0.90 sec. BPR loss is 1.76E-04. Sample per second: 46082\n",
      "SLIM_BPR_Recommender: Epoch 249 of 300. Elapsed time 46.45 sec\n",
      "Processed 41629 (100.0%) in 1.08 sec. BPR loss is 1.72E-04. Sample per second: 38426\n",
      "SLIM_BPR_Recommender: Epoch 250 of 300. Elapsed time 46.63 sec\n",
      "Processed 41629 (100.0%) in 0.26 sec. BPR loss is 1.68E-04. Sample per second: 159821\n",
      "SLIM_BPR_Recommender: Epoch 251 of 300. Elapsed time 46.81 sec\n",
      "Processed 41629 (100.0%) in 0.44 sec. BPR loss is 1.71E-04. Sample per second: 94115\n",
      "SLIM_BPR_Recommender: Epoch 252 of 300. Elapsed time 46.99 sec\n",
      "Processed 41629 (100.0%) in 0.65 sec. BPR loss is 1.76E-04. Sample per second: 64504\n",
      "SLIM_BPR_Recommender: Epoch 253 of 300. Elapsed time 47.19 sec\n",
      "Processed 41629 (100.0%) in 0.86 sec. BPR loss is 1.71E-04. Sample per second: 48165\n",
      "SLIM_BPR_Recommender: Epoch 254 of 300. Elapsed time 47.41 sec\n",
      "Processed 41629 (100.0%) in 1.07 sec. BPR loss is 1.73E-04. Sample per second: 39066\n",
      "SLIM_BPR_Recommender: Epoch 255 of 300. Elapsed time 47.61 sec\n",
      "Processed 41629 (100.0%) in 0.25 sec. BPR loss is 1.73E-04. Sample per second: 164246\n",
      "SLIM_BPR_Recommender: Epoch 256 of 300. Elapsed time 47.80 sec\n",
      "Processed 41629 (100.0%) in 0.45 sec. BPR loss is 1.81E-04. Sample per second: 92495\n",
      "SLIM_BPR_Recommender: Epoch 257 of 300. Elapsed time 48.00 sec\n",
      "Processed 41629 (100.0%) in 0.64 sec. BPR loss is 1.73E-04. Sample per second: 65276\n",
      "SLIM_BPR_Recommender: Epoch 258 of 300. Elapsed time 48.18 sec\n",
      "Processed 41629 (100.0%) in 0.83 sec. BPR loss is 1.77E-04. Sample per second: 50404\n",
      "SLIM_BPR_Recommender: Epoch 259 of 300. Elapsed time 48.37 sec\n",
      "Processed 41629 (100.0%) in 1.01 sec. BPR loss is 1.77E-04. Sample per second: 41027\n",
      "SLIM_BPR_Recommender: Epoch 260 of 300. Elapsed time 48.56 sec\n",
      "Processed 41629 (100.0%) in 0.20 sec. BPR loss is 1.78E-04. Sample per second: 204265\n",
      "SLIM_BPR_Recommender: Epoch 261 of 300. Elapsed time 48.75 sec\n",
      "Processed 41629 (100.0%) in 0.39 sec. BPR loss is 1.87E-04. Sample per second: 106797\n",
      "SLIM_BPR_Recommender: Epoch 262 of 300. Elapsed time 48.94 sec\n",
      "Processed 41629 (100.0%) in 0.57 sec. BPR loss is 1.68E-04. Sample per second: 72642\n",
      "SLIM_BPR_Recommender: Epoch 263 of 300. Elapsed time 49.12 sec\n",
      "Processed 41629 (100.0%) in 0.75 sec. BPR loss is 1.81E-04. Sample per second: 55133\n",
      "SLIM_BPR_Recommender: Epoch 264 of 300. Elapsed time 49.30 sec\n",
      "Processed 41629 (100.0%) in 0.94 sec. BPR loss is 1.83E-04. Sample per second: 44370\n",
      "SLIM_BPR_Recommender: Epoch 265 of 300. Elapsed time 49.49 sec\n",
      "Processed 41629 (100.0%) in 1.12 sec. BPR loss is 1.74E-04. Sample per second: 37186\n",
      "SLIM_BPR_Recommender: Epoch 266 of 300. Elapsed time 49.67 sec\n",
      "Processed 41629 (100.0%) in 0.30 sec. BPR loss is 1.77E-04. Sample per second: 139600\n",
      "SLIM_BPR_Recommender: Epoch 267 of 300. Elapsed time 49.84 sec\n",
      "Processed 41629 (100.0%) in 0.48 sec. BPR loss is 1.77E-04. Sample per second: 87312\n",
      "SLIM_BPR_Recommender: Epoch 268 of 300. Elapsed time 50.02 sec\n",
      "Processed 41629 (100.0%) in 0.66 sec. BPR loss is 1.77E-04. Sample per second: 63239\n",
      "SLIM_BPR_Recommender: Epoch 269 of 300. Elapsed time 50.20 sec\n",
      "Processed 41629 (100.0%) in 0.83 sec. BPR loss is 1.80E-04. Sample per second: 49901\n",
      "SLIM_BPR_Recommender: Epoch 270 of 300. Elapsed time 50.38 sec\n",
      "Processed 41629 (100.0%) in 1.01 sec. BPR loss is 1.80E-04. Sample per second: 41142\n",
      "SLIM_BPR_Recommender: Epoch 271 of 300. Elapsed time 50.56 sec\n",
      "Processed 41629 (100.0%) in 0.20 sec. BPR loss is 1.86E-04. Sample per second: 210416\n",
      "SLIM_BPR_Recommender: Epoch 272 of 300. Elapsed time 50.74 sec\n",
      "Processed 41629 (100.0%) in 0.38 sec. BPR loss is 1.91E-04. Sample per second: 110505\n",
      "SLIM_BPR_Recommender: Epoch 273 of 300. Elapsed time 50.92 sec\n",
      "Processed 41629 (100.0%) in 0.56 sec. BPR loss is 1.83E-04. Sample per second: 74694\n",
      "SLIM_BPR_Recommender: Epoch 274 of 300. Elapsed time 51.10 sec\n",
      "Processed 41629 (100.0%) in 0.74 sec. BPR loss is 1.91E-04. Sample per second: 56180\n",
      "SLIM_BPR_Recommender: Epoch 275 of 300. Elapsed time 51.29 sec\n",
      "Processed 41629 (100.0%) in 0.93 sec. BPR loss is 1.89E-04. Sample per second: 44957\n",
      "SLIM_BPR_Recommender: Epoch 276 of 300. Elapsed time 51.47 sec\n",
      "Processed 41629 (100.0%) in 1.11 sec. BPR loss is 1.90E-04. Sample per second: 37656\n",
      "SLIM_BPR_Recommender: Epoch 277 of 300. Elapsed time 51.65 sec\n",
      "Processed 41629 (100.0%) in 0.28 sec. BPR loss is 1.91E-04. Sample per second: 146727\n",
      "SLIM_BPR_Recommender: Epoch 278 of 300. Elapsed time 51.83 sec\n",
      "Processed 41629 (100.0%) in 0.46 sec. BPR loss is 2.02E-04. Sample per second: 89571\n",
      "SLIM_BPR_Recommender: Epoch 279 of 300. Elapsed time 52.01 sec\n",
      "Processed 41629 (100.0%) in 0.65 sec. BPR loss is 2.00E-04. Sample per second: 63721\n",
      "SLIM_BPR_Recommender: Epoch 280 of 300. Elapsed time 52.20 sec\n",
      "Processed 41629 (100.0%) in 0.84 sec. BPR loss is 1.90E-04. Sample per second: 49503\n",
      "SLIM_BPR_Recommender: Epoch 281 of 300. Elapsed time 52.39 sec\n",
      "Processed 41629 (100.0%) in 1.03 sec. BPR loss is 1.97E-04. Sample per second: 40403\n",
      "SLIM_BPR_Recommender: Epoch 282 of 300. Elapsed time 52.58 sec\n",
      "Processed 41629 (100.0%) in 0.22 sec. BPR loss is 1.86E-04. Sample per second: 189709\n",
      "SLIM_BPR_Recommender: Epoch 283 of 300. Elapsed time 52.77 sec\n",
      "Processed 41629 (100.0%) in 0.40 sec. BPR loss is 1.96E-04. Sample per second: 102904\n",
      "SLIM_BPR_Recommender: Epoch 284 of 300. Elapsed time 52.95 sec\n",
      "Processed 41629 (100.0%) in 0.59 sec. BPR loss is 1.93E-04. Sample per second: 70670\n",
      "SLIM_BPR_Recommender: Epoch 285 of 300. Elapsed time 53.14 sec\n",
      "Processed 41629 (100.0%) in 0.77 sec. BPR loss is 1.95E-04. Sample per second: 53893\n",
      "SLIM_BPR_Recommender: Epoch 286 of 300. Elapsed time 53.32 sec\n",
      "Processed 41629 (100.0%) in 0.96 sec. BPR loss is 2.04E-04. Sample per second: 43377\n",
      "SLIM_BPR_Recommender: Epoch 287 of 300. Elapsed time 53.51 sec\n",
      "Processed 41629 (100.0%) in 1.15 sec. BPR loss is 1.99E-04. Sample per second: 36212\n",
      "SLIM_BPR_Recommender: Epoch 288 of 300. Elapsed time 53.70 sec\n",
      "Processed 41629 (100.0%) in 0.33 sec. BPR loss is 2.03E-04. Sample per second: 124845\n",
      "SLIM_BPR_Recommender: Epoch 289 of 300. Elapsed time 53.88 sec\n",
      "Processed 41629 (100.0%) in 0.52 sec. BPR loss is 1.92E-04. Sample per second: 80400\n",
      "SLIM_BPR_Recommender: Epoch 290 of 300. Elapsed time 54.07 sec\n",
      "Processed 41629 (100.0%) in 0.70 sec. BPR loss is 2.00E-04. Sample per second: 59406\n",
      "SLIM_BPR_Recommender: Epoch 291 of 300. Elapsed time 54.25 sec\n",
      "Processed 41629 (100.0%) in 0.89 sec. BPR loss is 2.01E-04. Sample per second: 46840\n",
      "SLIM_BPR_Recommender: Epoch 292 of 300. Elapsed time 54.44 sec\n",
      "Processed 41629 (100.0%) in 1.07 sec. BPR loss is 1.94E-04. Sample per second: 38925\n",
      "SLIM_BPR_Recommender: Epoch 293 of 300. Elapsed time 54.62 sec\n",
      "Processed 41629 (100.0%) in 0.25 sec. BPR loss is 1.99E-04. Sample per second: 166080\n",
      "SLIM_BPR_Recommender: Epoch 294 of 300. Elapsed time 54.80 sec\n",
      "Processed 41629 (100.0%) in 0.44 sec. BPR loss is 1.97E-04. Sample per second: 94879\n",
      "SLIM_BPR_Recommender: Epoch 295 of 300. Elapsed time 54.99 sec\n",
      "Processed 41629 (100.0%) in 0.62 sec. BPR loss is 2.05E-04. Sample per second: 67208\n",
      "SLIM_BPR_Recommender: Epoch 296 of 300. Elapsed time 55.17 sec\n",
      "Processed 41629 (100.0%) in 0.81 sec. BPR loss is 2.01E-04. Sample per second: 51603\n",
      "SLIM_BPR_Recommender: Epoch 297 of 300. Elapsed time 55.35 sec\n",
      "Processed 41629 (100.0%) in 1.01 sec. BPR loss is 2.07E-04. Sample per second: 41406\n",
      "SLIM_BPR_Recommender: Epoch 298 of 300. Elapsed time 55.55 sec\n",
      "Processed 41629 (100.0%) in 0.19 sec. BPR loss is 1.95E-04. Sample per second: 221964\n",
      "SLIM_BPR_Recommender: Epoch 299 of 300. Elapsed time 55.73 sec\n",
      "Processed 41629 (100.0%) in 0.36 sec. BPR loss is 2.07E-04. Sample per second: 114053\n",
      "SLIM_BPR_Recommender: Epoch 300 of 300. Elapsed time 55.91 sec\n",
      "SLIM_BPR_Recommender: Terminating at epoch 300. Elapsed time 1.32 min\n",
      "Deallocating Cython objects\n"
     ]
    }
   ],
   "source": [
    "from Recommenders.NonPersonalizedRecommender import TopPop\n",
    "from Recommenders.KNN.UserKNNCFRecommender import UserKNNCFRecommender\n",
    "from Recommenders.KNN.ItemKNNCFRecommender import ItemKNNCFRecommender\n",
    "from Recommenders.KNN.ItemKNNCBFRecommender import ItemKNNCBFRecommender\n",
    "from Recommenders.KNN import ItemKNN_CFCBF_Hybrid_Recommender\n",
    "from Recommenders.SLIM.Cython.SLIM_BPR_Cython import SLIM_BPR_Cython\n",
    "from Recommenders.GraphBased.P3alphaRecommender import P3alphaRecommender\n",
    "from Recommenders.GraphBased.RP3betaRecommender import RP3betaRecommender\n",
    "from Recommenders.MatrixFactorization.Cython.MatrixFactorization_Cython import MatrixFactorization_BPR_Cython, MatrixFactorization_FunkSVD_Cython, MatrixFactorization_AsySVD_Cython\n",
    "from Recommenders.MatrixFactorization.PureSVDRecommender import PureSVDRecommender\n",
    "from Recommenders.MatrixFactorization.IALSRecommender import IALSRecommender\n",
    "from Recommenders.MatrixFactorization.NMFRecommender import NMFRecommender\n",
    "\n",
    "MAP_recommender_per_group = {}\n",
    "\n",
    "collaborative_recommender_class = {\n",
    "                                   \"SLIMElasticNetRecommender\": SLIMElasticNetRecommender,\n",
    "                                   \"TopPop\": TopPop,\n",
    "                                   \"UserKNNCF\": UserKNNCFRecommender,\n",
    "                                   \"ItemKNNCF\": ItemKNNCFRecommender,\n",
    "                                   \"P3alpha\": P3alphaRecommender,\n",
    "                                   \"RP3beta\": RP3betaRecommender,\n",
    "                                   \"PureSVD\": PureSVDRecommender,\n",
    "                                   \"NMF\": NMFRecommender,\n",
    "                                   \"FunkSVD\": MatrixFactorization_FunkSVD_Cython,\n",
    "                                   \"SLIMBPR\": SLIM_BPR_Cython,\n",
    "                                   }\n",
    "\n",
    "content_recommender_class = {\"ItemKNNCBF\": ItemKNNCBFRecommender,\n",
    "                             \"ItemKNNCFCBF\": ItemKNN_CFCBF_Hybrid_Recommender\n",
    "                            }\n",
    "\n",
    "recommender_object_dict = {}\n",
    "\n",
    "for label, recommender_class in collaborative_recommender_class.items():\n",
    "    recommender_object = recommender_class(URM_train)\n",
    "    \n",
    "    if isinstance(recommender_object, SLIMElasticNetRecommender):\n",
    "        recommender_object.load_model(\"/kaggle/working\")\n",
    "    else:\n",
    "         recommender_object.fit()\n",
    "            \n",
    "    recommender_object_dict[label] = recommender_object\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99259967",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T21:59:35.511321Z",
     "iopub.status.busy": "2022-11-26T21:59:35.510912Z",
     "iopub.status.idle": "2022-11-26T22:07:29.601642Z",
     "shell.execute_reply": "2022-11-26T22:07:29.600326Z"
    },
    "papermill": {
     "duration": 474.175665,
     "end_time": "2022-11-26T22:07:29.604337",
     "exception": false,
     "start_time": "2022-11-26T21:59:35.428672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0, #users in group 4162, average p.len 8.47, median 9.0, min 0, max 10\n",
      "EvaluatorHoldout: Ignoring 319 ( 0.8%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 37467 Users\n",
      "EvaluatorHoldout: Processed 4094 (100.0%) in 3.79 sec. Users per second: 1080\n",
      "EvaluatorHoldout: Processed 4094 (100.0%) in 3.84 sec. Users per second: 1066\n",
      "EvaluatorHoldout: Processed 4094 (100.0%) in 3.52 sec. Users per second: 1163\n",
      "EvaluatorHoldout: Processed 4094 (100.0%) in 3.66 sec. Users per second: 1117\n",
      "EvaluatorHoldout: Processed 4094 (100.0%) in 3.29 sec. Users per second: 1243\n",
      "EvaluatorHoldout: Processed 4094 (100.0%) in 3.35 sec. Users per second: 1221\n",
      "EvaluatorHoldout: Processed 4094 (100.0%) in 4.48 sec. Users per second: 913\n",
      "EvaluatorHoldout: Processed 4094 (100.0%) in 9.63 sec. Users per second: 425\n",
      "EvaluatorHoldout: Processed 4094 (100.0%) in 4.67 sec. Users per second: 876\n",
      "EvaluatorHoldout: Processed 4094 (100.0%) in 3.70 sec. Users per second: 1105\n",
      "Group 1, #users in group 4162, average p.len 11.47, median 12.0, min 10, max 13\n",
      "EvaluatorHoldout: Ignoring 319 ( 0.8%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 37467 Users\n",
      "EvaluatorHoldout: Processed 4092 (100.0%) in 3.99 sec. Users per second: 1025\n",
      "EvaluatorHoldout: Processed 4092 (100.0%) in 3.94 sec. Users per second: 1039\n",
      "EvaluatorHoldout: Processed 4092 (100.0%) in 3.29 sec. Users per second: 1244\n",
      "EvaluatorHoldout: Processed 4092 (100.0%) in 3.76 sec. Users per second: 1087\n",
      "EvaluatorHoldout: Processed 4092 (100.0%) in 3.29 sec. Users per second: 1243\n",
      "EvaluatorHoldout: Processed 4092 (100.0%) in 3.48 sec. Users per second: 1176\n",
      "EvaluatorHoldout: Processed 4092 (100.0%) in 4.58 sec. Users per second: 893\n",
      "EvaluatorHoldout: Processed 4092 (100.0%) in 10.04 sec. Users per second: 407\n",
      "EvaluatorHoldout: Processed 4092 (100.0%) in 4.56 sec. Users per second: 898\n",
      "EvaluatorHoldout: Processed 4092 (100.0%) in 3.71 sec. Users per second: 1102\n",
      "Group 2, #users in group 4162, average p.len 13.48, median 13.0, min 13, max 14\n",
      "EvaluatorHoldout: Ignoring 319 ( 0.8%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 37467 Users\n",
      "EvaluatorHoldout: Processed 4098 (100.0%) in 4.10 sec. Users per second: 999\n",
      "EvaluatorHoldout: Processed 4098 (100.0%) in 3.99 sec. Users per second: 1028\n",
      "EvaluatorHoldout: Processed 4098 (100.0%) in 3.19 sec. Users per second: 1285\n",
      "EvaluatorHoldout: Processed 4098 (100.0%) in 3.98 sec. Users per second: 1029\n",
      "EvaluatorHoldout: Processed 4098 (100.0%) in 3.35 sec. Users per second: 1223\n",
      "EvaluatorHoldout: Processed 4098 (100.0%) in 3.47 sec. Users per second: 1182\n",
      "EvaluatorHoldout: Processed 4098 (100.0%) in 4.47 sec. Users per second: 916\n",
      "EvaluatorHoldout: Processed 4098 (100.0%) in 9.93 sec. Users per second: 413\n",
      "EvaluatorHoldout: Processed 4098 (100.0%) in 4.61 sec. Users per second: 889\n",
      "EvaluatorHoldout: Processed 4098 (100.0%) in 3.73 sec. Users per second: 1099\n",
      "Group 3, #users in group 4162, average p.len 15.34, median 15.0, min 14, max 16\n",
      "EvaluatorHoldout: Ignoring 319 ( 0.8%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 37467 Users\n",
      "EvaluatorHoldout: Processed 4113 (100.0%) in 4.29 sec. Users per second: 959\n",
      "EvaluatorHoldout: Processed 4113 (100.0%) in 4.07 sec. Users per second: 1011\n",
      "EvaluatorHoldout: Processed 4113 (100.0%) in 3.30 sec. Users per second: 1246\n",
      "EvaluatorHoldout: Processed 4113 (100.0%) in 4.03 sec. Users per second: 1020\n",
      "EvaluatorHoldout: Processed 4113 (100.0%) in 3.38 sec. Users per second: 1219\n",
      "EvaluatorHoldout: Processed 4113 (100.0%) in 3.50 sec. Users per second: 1174\n",
      "EvaluatorHoldout: Processed 4113 (100.0%) in 4.54 sec. Users per second: 906\n",
      "EvaluatorHoldout: Processed 4113 (100.0%) in 10.46 sec. Users per second: 393\n",
      "EvaluatorHoldout: Processed 4113 (100.0%) in 4.57 sec. Users per second: 900\n",
      "EvaluatorHoldout: Processed 4113 (100.0%) in 3.75 sec. Users per second: 1098\n",
      "Group 4, #users in group 4162, average p.len 17.32, median 17.0, min 16, max 18\n",
      "EvaluatorHoldout: Ignoring 319 ( 0.8%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 37467 Users\n",
      "EvaluatorHoldout: Processed 4130 (100.0%) in 4.37 sec. Users per second: 944\n",
      "EvaluatorHoldout: Processed 4130 (100.0%) in 4.08 sec. Users per second: 1013\n",
      "EvaluatorHoldout: Processed 4130 (100.0%) in 3.30 sec. Users per second: 1253\n",
      "EvaluatorHoldout: Processed 4130 (100.0%) in 4.25 sec. Users per second: 971\n",
      "EvaluatorHoldout: Processed 4130 (100.0%) in 3.49 sec. Users per second: 1182\n",
      "EvaluatorHoldout: Processed 4130 (100.0%) in 3.55 sec. Users per second: 1165\n",
      "EvaluatorHoldout: Processed 4130 (100.0%) in 4.61 sec. Users per second: 897\n",
      "EvaluatorHoldout: Processed 4130 (100.0%) in 10.26 sec. Users per second: 402\n",
      "EvaluatorHoldout: Processed 4130 (100.0%) in 4.58 sec. Users per second: 902\n",
      "EvaluatorHoldout: Processed 4130 (100.0%) in 3.80 sec. Users per second: 1086\n",
      "Group 5, #users in group 4162, average p.len 19.74, median 20.0, min 18, max 21\n",
      "EvaluatorHoldout: Ignoring 319 ( 0.8%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 37467 Users\n",
      "EvaluatorHoldout: Processed 4142 (100.0%) in 4.61 sec. Users per second: 899\n",
      "EvaluatorHoldout: Processed 4142 (100.0%) in 4.04 sec. Users per second: 1026\n",
      "EvaluatorHoldout: Processed 4142 (100.0%) in 3.31 sec. Users per second: 1253\n",
      "EvaluatorHoldout: Processed 4142 (100.0%) in 4.45 sec. Users per second: 931\n",
      "EvaluatorHoldout: Processed 4142 (100.0%) in 3.43 sec. Users per second: 1207\n",
      "EvaluatorHoldout: Processed 4142 (100.0%) in 3.65 sec. Users per second: 1134\n",
      "EvaluatorHoldout: Processed 4142 (100.0%) in 4.60 sec. Users per second: 901\n",
      "EvaluatorHoldout: Processed 4142 (100.0%) in 10.37 sec. Users per second: 399\n",
      "EvaluatorHoldout: Processed 4142 (100.0%) in 4.63 sec. Users per second: 895\n",
      "EvaluatorHoldout: Processed 4142 (100.0%) in 3.91 sec. Users per second: 1058\n",
      "Group 6, #users in group 4162, average p.len 22.84, median 23.0, min 21, max 25\n",
      "EvaluatorHoldout: Ignoring 319 ( 0.8%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 37467 Users\n",
      "EvaluatorHoldout: Processed 4148 (100.0%) in 4.64 sec. Users per second: 893\n",
      "EvaluatorHoldout: Processed 4148 (100.0%) in 4.06 sec. Users per second: 1021\n",
      "EvaluatorHoldout: Processed 4148 (100.0%) in 3.39 sec. Users per second: 1223\n",
      "EvaluatorHoldout: Processed 4148 (100.0%) in 4.65 sec. Users per second: 892\n",
      "EvaluatorHoldout: Processed 4148 (100.0%) in 3.57 sec. Users per second: 1162\n",
      "EvaluatorHoldout: Processed 4148 (100.0%) in 3.83 sec. Users per second: 1084\n",
      "EvaluatorHoldout: Processed 4148 (100.0%) in 4.56 sec. Users per second: 909\n",
      "EvaluatorHoldout: Processed 4148 (100.0%) in 10.16 sec. Users per second: 408\n",
      "EvaluatorHoldout: Processed 4148 (100.0%) in 4.63 sec. Users per second: 895\n",
      "EvaluatorHoldout: Processed 4148 (100.0%) in 3.99 sec. Users per second: 1039\n",
      "Group 7, #users in group 4162, average p.len 27.41, median 27.0, min 25, max 31\n",
      "EvaluatorHoldout: Ignoring 319 ( 0.8%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 37467 Users\n",
      "EvaluatorHoldout: Processed 4160 (100.0%) in 4.93 sec. Users per second: 843\n",
      "EvaluatorHoldout: Processed 4160 (100.0%) in 4.25 sec. Users per second: 978\n",
      "EvaluatorHoldout: Processed 4160 (100.0%) in 3.52 sec. Users per second: 1181\n",
      "EvaluatorHoldout: Processed 4160 (100.0%) in 4.99 sec. Users per second: 833\n",
      "EvaluatorHoldout: Processed 4160 (100.0%) in 3.52 sec. Users per second: 1182\n",
      "EvaluatorHoldout: Processed 4160 (100.0%) in 3.80 sec. Users per second: 1094\n",
      "EvaluatorHoldout: Processed 4160 (100.0%) in 4.58 sec. Users per second: 909\n",
      "EvaluatorHoldout: Processed 4160 (100.0%) in 10.27 sec. Users per second: 405\n",
      "EvaluatorHoldout: Processed 4160 (100.0%) in 4.65 sec. Users per second: 895\n",
      "EvaluatorHoldout: Processed 4160 (100.0%) in 4.12 sec. Users per second: 1010\n",
      "Group 8, #users in group 4162, average p.len 35.56, median 35.0, min 31, max 42\n",
      "EvaluatorHoldout: Ignoring 319 ( 0.8%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 37467 Users\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 5.26 sec. Users per second: 792\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 4.20 sec. Users per second: 990\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 3.77 sec. Users per second: 1104\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 5.49 sec. Users per second: 758\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 3.60 sec. Users per second: 1155\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 4.01 sec. Users per second: 1037\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 4.60 sec. Users per second: 904\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 10.17 sec. Users per second: 409\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 4.66 sec. Users per second: 892\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 4.39 sec. Users per second: 948\n",
      "Group 9, #users in group 4162, average p.len 66.30, median 55.0, min 42, max 380\n",
      "EvaluatorHoldout: Ignoring 319 ( 0.8%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 37467 Users\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 6.17 sec. Users per second: 674\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 4.26 sec. Users per second: 977\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 4.37 sec. Users per second: 953\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 6.19 sec. Users per second: 673\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 3.78 sec. Users per second: 1100\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 4.44 sec. Users per second: 937\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 4.76 sec. Users per second: 875\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 10.11 sec. Users per second: 412\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 4.63 sec. Users per second: 899\n",
      "EvaluatorHoldout: Processed 4162 (100.0%) in 4.87 sec. Users per second: 855\n"
     ]
    }
   ],
   "source": [
    "cutoff = 10\n",
    "\n",
    "for group_id in range(0, 10):\n",
    "    \n",
    "    start_pos = group_id*block_size\n",
    "    end_pos = min((group_id+1)*block_size, len(profile_length))\n",
    "    \n",
    "    users_in_group = sorted_users[start_pos:end_pos]\n",
    "    \n",
    "    users_in_group_p_len = profile_length[users_in_group]\n",
    "    \n",
    "    print(\"Group {}, #users in group {}, average p.len {:.2f}, median {}, min {}, max {}\".format(\n",
    "        group_id, \n",
    "        users_in_group.shape[0],\n",
    "        users_in_group_p_len.mean(),\n",
    "        np.median(users_in_group_p_len),\n",
    "        users_in_group_p_len.min(),\n",
    "        users_in_group_p_len.max()))\n",
    "    \n",
    "    \n",
    "    users_not_in_group_flag = np.isin(sorted_users, users_in_group, invert=True)\n",
    "    users_not_in_group = sorted_users[users_not_in_group_flag]\n",
    "    \n",
    "    evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=[cutoff], ignore_users=users_not_in_group)\n",
    "    \n",
    "    for label, recommender in recommender_object_dict.items():\n",
    "        result_df, _ = evaluator_test.evaluateRecommender(recommender)\n",
    "        if label in MAP_recommender_per_group:\n",
    "            MAP_recommender_per_group[label].append(result_df.loc[cutoff][\"MAP\"])\n",
    "        else:\n",
    "            MAP_recommender_per_group[label] = [result_df.loc[cutoff][\"MAP\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdef276e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T22:07:29.783919Z",
     "iopub.status.busy": "2022-11-26T22:07:29.783205Z",
     "iopub.status.idle": "2022-11-26T22:07:30.448333Z",
     "shell.execute_reply": "2022-11-26T22:07:30.447183Z"
    },
    "papermill": {
     "duration": 0.758041,
     "end_time": "2022-11-26T22:07:30.450905",
     "exception": false,
     "start_time": "2022-11-26T22:07:29.692864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAAIWCAYAAABjkRHCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABo+ElEQVR4nO3de3xU1b3///fKPRAIQsCEi0IoYICEAAGKmgpEQb8UQfECpUWrPbZWj8ARqtQjTbVWWmgBrb9aerSIpaJF5CIqVMAWLwUCxCCXCMQohERuJpCQhFzW748kUwJBM0kme2byej4ePMh89mU+s88cyztr7bWNtVYAAAAAAPiqAKcbAAAAAACgMQi2AAAAAACfRrAFAAAAAPg0gi0AAAAAwKcRbAEAAAAAPo1gCwAAAADwaUFON9BUoqKibPfu3Z1uAwAAAADgATt27Dhhre1Y1za/Cbbdu3dXWlqa020AAAAAADzAGPP5pbYxFRkAAAAA4NMItgAAAAAAn0awBQAAAAD4NL+5x7YuZWVlOnLkiEpKSpxuBWgSYWFh6tq1q4KDg51uBQAAAPAafh1sjxw5ojZt2qh79+4yxjjdDtAo1lqdPHlSR44cUY8ePZxuBwAAAPAaHp2KbIy50RiTaYw5aIx5tI7t3zHG7DTGlBtjbrtg213GmAPVf+5qyPuXlJSoQ4cOhFr4BWOMOnTowAwEAAAA4AIeC7bGmEBJz0m6SVJfSZONMX0v2O0LSXdL+tsFx7aX9AtJwyQNlfQLY8xlDeyjIYcBXonvMwAAAHAxT47YDpV00FqbZa09J2m5pPHn72CtzbbWZkiqvODYMZL+Ya09Za39StI/JN3owV496qmnnlK/fv2UkJCgxMREbd26VSNGjLjoubvvvfeevvvd70qSlixZImOM3n33Xdf2VatWyRijFStWSJJGjBihPn36KDExUYmJibrttqpB79TUVM2fP9/tPtPT0/XWW2+5Xq9Zs0Zz58792mOMMXr44Yddr+fPn6/U1NSvPea9997Thx9+6HqdmpqqLl26KDExUX379tUrr7zidu++4vz/GwMAAABoGp4Mtl0kHT7v9ZHqmqeP9SofffSR3nzzTe3cuVMZGRl699131a1bt3odGx8fr+XLl7tev/LKKxowYECtfZYtW6b09HSlp6e7Am9DXRhsb775Zj366EUzyGsJDQ3VypUrdeLEiXq/z4XBVpJmzJih9PR0rV69Wj/+8Y9VVlbmXvN+qry83OkWAAAAAK/n04/7McbcZ4xJM8akHT9+vNHnW7UrR9fM3aQej67TNXM3adWunEafMzc3V1FRUQoNDZUkRUVFqXPnzvU6Njk5Wdu2bVNZWZkKCwt18OBBJSYmuvX+f/7znzVkyBANGDBAEydO1NmzZyVJf//739W/f38NGDBA3/nOd3Tu3DnNmTNHr776qhITE/Xqq69qyZIlevDBByVJX375pW655RYNGDBAAwYMcAXToKAg3XfffVqwYMFF7338+HFNnDhRQ4YM0ZAhQ/TBBx8oOztbzz//vBYsWKDExERt2bKl1jG9evVSq1at9NVXX0mS5s2bpyFDhighIUG/+MUvXPstXbpUCQkJGjBggH7wgx9IkrKzszVq1CglJCQoJSVFX3zxhSTp7rvv1v33369vf/vbio2N1Xvvvad77rlHcXFxuvvuu13njIiI0KxZs9SvXz9df/312rZtm0aMGKHY2FitWbNGklRRUaFZs2a5evrTn/4kqSqsjxgxQrfddpuuuuoqTZkyRdZaSdI777yjq666SoMGDdLKlStd71dUVKR77rlHQ4cO1cCBA7V69WpJVaP1N998s0aNGqWUlBS3/u8NAAAAtESeDLY5ks4fmuxaXWuyY621i621SdbapI4dOza4Uakq1M5euVs5+cWyknLyizV75e5Gh9vRo0fr8OHD6t27t37605/qn//8Z72PNcbo+uuv1/r167V69WrdfPPNF+0zZcoU11TkWbNmXbT91ltv1fbt2/Xxxx8rLi5OL7zwgiTpiSee0Pr16/Xxxx9rzZo1CgkJ0RNPPKE777xT6enpuvPOO2ud56GHHtJ1112njz/+WDt37lS/fv1c2x544AEtW7ZMBQUFtY6ZNm2aZsyYoe3bt+v111/Xj370I3Xv3l0/+clPXCO0ycnJtY7ZuXOnevXqpU6dOmnDhg06cOCAtm3bpvT0dO3YsUP/+te/tGfPHv3qV7/Spk2b9PHHH2vRokWSpP/+7//WXXfdpYyMDE2ZMkUPPfSQ67xfffWVPvroIy1YsEA333yzZsyYoT179mj37t1KT0+XVBU0R40apT179qhNmzb63//9X/3jH//QG2+8oTlz5kiSXnjhBUVGRmr79u3avn27/vznP+uzzz6TJO3atUsLFy7U3r17lZWVpQ8++EAlJSX6r//6L61du1Y7duxQXl6eq6ennnpKo0aN0rZt27R582bNmjVLRUVFruuwYsUKt74vAAAAQEvlycf9bJfUyxjTQ1WhdJKk79Xz2PWSfn3eglGjJc1u+hb/Y976TBWXVdSqFZdVaN76TE0Y2PBZ0BEREdqxY4e2bNmizZs368477/zG+1bPN2nSJD3zzDMqKCjQ7373O/3617+utX3ZsmVKSkq65PGffPKJ/vd//1f5+fkqLCzUmDFjJEnXXHON7r77bt1xxx269dZbv7GPTZs2aenSpZKkwMBARUZGura1bdtWU6dO1TPPPKPw8HBX/d1339XevXtdr0+fPq3CwsI6z79gwQL95S9/0aeffqq1a9dKkjZs2KANGzZo4MCBkqTCwkIdOHBAH3/8sW6//XZFRUVJktq3by+patp3zYjoD37wA/3sZz9znX/cuHEyxig+Pl6XX3654uPjJUn9+vVTdna2EhMTFRISohtvrLqVOz4+XqGhoQoODlZ8fLyys7NdPWVkZLimfRcUFOjAgQMKCQnR0KFD1bVrV0lSYmKisrOzFRERoR49eqhXr16SpO9///tavHix61xr1qxx3Q9dUlLiGmW+4YYbXJ8LAAAAwNfzWLC11pYbYx5UVUgNlPSitXaPMeYJSWnW2jXGmCGS3pB0maRxxphfWmv7WWtPGWOeVFU4lqQnrLWnPNWrJB3NL3ar7o7AwECNGDFCI0aMUHx8vF566aV6Hzt06FDt3r1brVq1Uu/evd1+77vvvlurVq3SgAEDtGTJEr333nuSpOeff15bt27VunXrNHjwYO3YscPtc59v+vTpGjRokH74wx+6apWVlfr3v/+tsLCwbzx+xowZmjlzptasWaN7771Xhw4dkrVWs2fP1o9//ONa+z777LNu91czFTwgIMD1c83rmvtYg4ODXasOn7/f+ftYa/Xss8+6fkFQ47333qt13sDAwG+8P9Zaq9dff119+vSpVd+6datat27t9mcEAAAAWiqP3mNrrX3LWtvbWtvTWvtUdW2OtXZN9c/brbVdrbWtrbUdrLX9zjv2RWvtt6r//MWTfUpS53bhbtXrKzMzUwcOHHC9Tk9P15VXXunWOebOnXvRSG19nTlzRjExMSorK9OyZctc9UOHDmnYsGF64okn1LFjRx0+fFht2rTRmTNn6jxPSkqK/vjHP0qqus/0wmnH7du31x133OGa6ixVTcM+P4TWTPn9uve5+eablZSUpJdeekljxozRiy++6BrlzcnJ0bFjxzRq1Cj9/e9/18mTJyVJp05V/c7j6quvdi22tWzZsoumOTeFMWPG6I9//KNrcatPP/3UNX24LldddZWys7N16NAhSaq14vOYMWP07LPPuu7F3bVrV5P3CwAAALQEPr14VFOaNaaPwoMDa9XCgwM1a0yfSxxRP4WFhbrrrrvUt29fJSQkaO/eva7H4YwdO1Zdu3ZV165ddfvtt1/yHDfddJNGjhxZ57bz77G9/vrrL9r+5JNPatiwYbrmmmt01VVXueqzZs1SfHy8+vfvr6uvvloDBgzQyJEjtXfvXtfiUedbtGiRNm/erPj4eA0ePLjWFOMaDz/8cK3VkZ955hmlpaUpISFBffv21fPPPy+palrwG2+8UefiUZI0Z84c/f73v9f111+v733vexo+fLji4+N122236cyZM+rXr58ee+wxXXfddRowYID+53/+R1LVSO5f/vIXJSQk6OWXX3bde9uUfvSjH6lv374aNGiQ+vfvrx//+MdfOzIbFhamxYsXa+zYsRo0aJA6derk2vb444+rrKxMCQkJ6tevnx5//PEm7xcAAABoCUzNaJGvS0pKshc+F3bfvn2Ki4ur9zlW7crRvPWZOppfrM7twjVrTJ9G3V8LeIK732sAAADAHxhjdlhr61xgyJOLR/mcCQO7EGQBAAAAwMcQbAEAAACgBfOHmasEWwAAAABooVbtytHslbtdjz7NyS/W7JW7Jcmnwi2LRwEAAABACzVvfaYr1NYoLqvQvPWZDnXUMARbAAAAAGihjuYXu1X3VgRbAAAAAGihOrcLd6vurQi2HnTy5EnXM2ajo6PVpUsX1+tz587V6xypqamu4/r37681a9Z4uGsAAAAALcWsMX0UHhxYqxYeHKhZY/o41FHDsHiUB3Xo0EHp6emSqgJqRESEZs6c6fZ5ZsyYoZkzZ2rfvn1KTk7WsWPHFBDA7yQAAAAANE7NAlG+vioy6eh8Ga9JC/pLqe2q/s54rcnfYuPGjRo4cKDi4+N1zz33qLS0VJLUvXt3/exnP1N8fLyGDh2qgwcPXnRsXFycgoKCdOLECb3yyiuKj49X//799cgjj7j2iYiI0IwZM9SvXz+lpKTo+PHjTf4ZAAAAAPiPCQO76INHR+mzuWP1waOjfC7USgTb/8h4TVr7kFRwWJKt+nvtQ00abktKSnT33Xfr1Vdf1e7du1VeXq4//vGPru2RkZHavXu3HnzwQU2fPv2i47du3aqAgACVlZXpkUce0aZNm5Senq7t27dr1apVkqSioiIlJSVpz549uu666/TLX/6yyfoHAAAAAG9EsK2x8Qmp7IKVv8qKq+pNpKKiQj169FDv3r0lSXfddZf+9a9/ubZPnjzZ9fdHH33kqi9YsECJiYmaOXOmXn31VaWlpWnEiBHq2LGjgoKCNGXKFNd5AgICdOedd0qSvv/97+v9999vsv4BAAAAwBsRbGsUHHGv7gHGmDp/njFjhtLT07VlyxYlJyc3+JwAAAAA4I8ItjUiu7pXb4DAwEBlZ2e77p99+eWXdd1117m2v/rqq66/hw8ffsnzDB06VP/85z914sQJVVRU6JVXXnGdp7KyUitWrJAk/e1vf9O1117bZP0DAAAAgDdiVeQaKXOq7qk9fzpycHhVvYmEhYXpL3/5i26//XaVl5dryJAh+slPfuLa/tVXXykhIUGhoaF65ZVXLnmemJgYzZ07VyNHjpS1VmPHjtX48eMlSa1bt9a2bdv0q1/9Sp06dXKFZQAAAADwV8Za63QPTSIpKcmmpaXVqu3bt09xcXH1P0nGa1X31BYcqRqpTZkjJdzRxJ3WrXv37kpLS1NUVFSjzhMREaHCwsIm6greyO3vNQAAAOAHjDE7rLVJdW1jxPZ8CXc0W5AFAAAAADQNgq2XyM7ObpLzMFoLAAAAoKVh8SgAAAAAgE8j2AIAAAAAfBrBFgAAAADg0wi2AAAAAACfRrD1sOzsbPXv379WLTU1VfPnz2/S8/75z3/W4MGD9dVXX+nuu+9Wly5dVFpaKkk6ceKEunfv7jrOGKNnn33WdeyDDz6oJUuWuF7Pnz9fV111lRITEzVkyBAtXbpUkjRixAj16dNHiYmJSkxM1IoVKxr1GQAAAACgKRBsfVB5eXmt1y+//LKeffZZrV+/XpdddpkkKTAwUC+++GKdx3fq1EmLFi3SuXPnLtr2/PPP6x//+Ie2bdum9PR0bdy4Uec/63jZsmVKT09Xenq6brvttib8VAAAAADQMATb86zLWqfRK0Yr4aUEjV4xWuuy1nn0/Z555hn17dtXCQkJmjRpkiSpqKhI99xzj4YOHaqBAwdq9erVkqQlS5bo5ptv1qhRo5SSkuI6x2uvvaa5c+dqw4YNioqKctWnT5+uBQsWXBSCJaljx45KSUnRSy+9dNG2X//61/rjH/+otm3bSpLatm2ru+66q0k/NwAAAAA0JZ5jW21d1jqlfpiqkooSSVJuUa5SP0yVJI2NHeuR95w7d64+++wzhYaGKj8/X5L01FNPadSoUXrxxReVn5+voUOH6vrrr5ck7dy5UxkZGWrfvr2ys7P1+eef68EHH9SuXbsUHR1d69xXXHGFrr32Wr388ssaN27cRe/9yCOP6KabbtI999zjqp0+fVpnzpxRbGzsJXueMmWKwsPDJUkbN25Uhw4dGnsZAAAAAKBRGLGttmjnIleorVFSUaJFOxc16rzGmEvWExISNGXKFP31r39VUFDV7xg2bNiguXPnKjExUSNGjFBJSYm++OILSdINN9yg9u3bu87RsWNHXXHFFXrttdfqfI/Zs2dr3rx5qqysvGhbbGyshg0bpr/97W9ufZ7zpyITagEAAAB4A4JttbyiPLfq9dWhQwd99dVXtWqnTp1SVFSU1q1bpwceeEA7d+7UkCFDVF5eLmutXn/9dVd4/OKLLxQXFydJat26da3ztGrVSm+99Zaef/55LVu27KL37tWrlxITEy8ZfH/+85/rN7/5jese2rZt2yoiIkJZWVmN+swAAAAA0JwIttWiW0e7Va+viIgIxcTEaNOmTZKqQu0777yja6+9VocPH9bIkSP1m9/8RgUFBSosLNSYMWP07LPPusLmrl27vvb8nTp10jvvvKOf//znWr9+/UXbH3vssUuuwHzVVVepb9++Wrt2ras2e/ZsPfDAAzp9+rQkqbCw0LUqMgAAAAB4I4JttWmDpiksMKxWLSwwTNMGTWv0uZcuXaonn3xSiYmJGjVqlH7xi1/oiiuu0Pe//33Fx8dr4MCBeuihh9SuXTs9/vjjKisrU0JCgvr166fHH3/8G8/fo0cPrVmzRvfcc4+2bdtWa1u/fv00aNCgSx772GOP6ciRI67X999/v0aOHKkhQ4aof//+Sk5OVkAAXxMAAAAA3suc/ygXX5aUlGTT0tJq1fbt2+eaxlsf67LWadHORcorylN062hNGzTNYwtHAQ3l7vcaAAAA8AfGmB3W2qS6trEq8nnGxo4lyAIAAACAj2GOKQAAAADApxFsAQAAAAA+jWALAAAAAPBpBFsAAAAAgE8j2AIAAAAAfBrB1sMiIiIkSdnZ2frb3/7WZOdNTU3V/PnzJUklJSW64YYblJqaKkkyxujhhx927Tt//nzXttTUVLVq1UrHjh27qEdJysvL06RJk9SzZ08NHjxY/+///T99+umnys7OVnh4uBITE11/zp0712SfBwAAAAAaimDbTJo62NY4d+6cJk6cqMGDB7vCa2hoqFauXKkTJ07UeUxUVJR+97vfXVS31uqWW27RiBEjdOjQIe3YsUNPP/20vvzyS0lSz549lZ6e7voTEhLS5J8HAAAAANxFsD1Pwdq1OjAqRfvi+urAqBQVrF3bZOd+9NFHtWXLFiUmJmrBggWqqKjQrFmzNGTIECUkJOhPf/qTJOm9997Tddddp/Hjxys2NlaPPvqoli1bpqFDhyo+Pl6HDh1ynbO8vFx33nmnevXqpblz57rqQUFBuu+++7RgwYI6e7nnnnv06quv6tSpU7XqmzdvVnBwsH7yk5+4agMGDFBycnKTXQcAAAAAaGoE22oFa9cq9/E5Kj96VLJW5UePKvfxOU0WbufOnavk5GSlp6drxowZeuGFFxQZGant27dr+/bt+vOf/6zPPvtMkvTxxx/r+eef1759+/Tyyy/r008/1bZt2/SjH/1Izz77rOucv/3tbxUSEqKFCxde9H4PPPCAli1bpoKCgou2RURE6J577tGiRYtq1T/55BMNHjz4kp/h0KFDrmnIDzzwQAOvBAAAAAA0rSCnG/AWxxYslC0pqVWzJSU6tmChIseNa/L327BhgzIyMrRixQpJUkFBgQ4cOKCQkBANGTJEMTExkqqm/44ePVqSFB8fr82bN7vOce211+rDDz/Up59+qt69e9c6f9u2bTV16lQ988wzCg8Pv+j9H3roISUmJmrmzJn17rlmKjIAAAAAeBNGbKuV5+a6VW8sa62effZZ1/2qn332mSvAhoaGuvYLCAhwvQ4ICFB5eblr23e+8x0tXLhQN910k3Lr6HP69Ol64YUXVFRUdNG2du3a6Xvf+56ee+45V61fv37asWNHk31GAAAAAGgOBNtqQdUjpPWtu6tNmzY6c+aM6/WYMWP0xz/+UWVlZZKkTz/9tM4A+k0mTpyomTNn6sYbb1R+fn6tbe3bt9cdd9yhF154oc5j/+d//kd/+tOfXGF51KhRKi0t1eLFi137ZGRkaMuWLW73BQAAAADNhWBbrdOM6TJhYbVqJixMnWZMb5LzJyQkKDAwUAMGDNCCBQv0ox/9SH379tWgQYPUv39//fjHP641GuuO+++/X7fccotuvvlmlVwwnfrhhx/+2tWRb7nlFpWWlkqqekzQG2+8oXfffVc9e/ZUv379NHv2bEVHRzeoLwAAAABoDsZa63QPTSIpKcmmpaXVqu3bt09xcXH1PkfB2rU6tmChynNzFRQTo04zpnvk/lqgMdz9XgMAAAD+wBizw1qbVNc2Fo86T+S4cQRZAAAAAPAxTEUGAAAAAPg0gi0AAAAAwKcRbAEAAAAAPo1gCwAAAADwaQRbAAAAAIBPI9h6WGBgoBITE9W/f3/dfvvtOnv2rEpKSjR06FANGDBA/fr10y9+8YtvPM/dd9+tFStWNHofAAAAAPA3BFsPCw8PV3p6uj755BOFhITo+eefV2hoqDZt2qSPP/5Y6enpeuedd/Tvf//b6VYBAAAAwCcRbM/z6dY8vfTzD/TcTzbppZ9/oE+35jXp+ZOTk3Xw4EEZYxQRESFJKisrU1lZmYwxkqQnnnhCQ4YMUf/+/XXffffJWnvRebp3766f/exnio+P19ChQ3Xw4EHXtn/961+6+uqrFRsb6xq9LSwsVEpKigYNGqT4+HitXr26ST8XAAAAADiJYFvt06152rxsvwpPlUqSCk+VavOy/U0WbsvLy/X2228rPj5eklRRUaHExER16tRJN9xwg4YNGyZJevDBB7V9+3Z98sknKi4u1ptvvlnn+SIjI7V79249+OCDmj59uquem5ur999/X2+++aYeffRRSVJYWJjeeOMN7dy5U5s3b9bDDz9cZ2AGAAAAAF9EsK320epDKj9XWatWfq5SH60+1KjzFhcXKzExUUlJSbriiit07733Sqq69zY9PV1HjhzRtm3b9Mknn0iSNm/erGHDhik+Pl6bNm3Snj176jzv5MmTXX9/9NFHrvqECRMUEBCgvn376ssvv5QkWWv185//XAkJCbr++uuVk5Pj2gYAAAAAvi7I6Qa8Rc1IbX3r9VVzj+2ltGvXTiNHjtQ777yjb33rW/rpT3+qtLQ0devWTampqSopKanzuJqpyxf+HBoa6vq5ZlR22bJlOn78uHbs2KHg4GB17979kucFAAAAAF/DiG21iPahbtUb4/jx48rPz5dUNaL7j3/8Q1dddZUrbEZFRamwsPBrVzh+9dVXXX8PHz78a9+voKBAnTp1UnBwsDZv3qzPP/+8aT4IAAAAAHgBRmyrDR/fU5uX7a81HTkoJEDDx/ds8vfKzc3VXXfdpYqKClVWVuqOO+7Qd7/7XUnSf/3Xf6l///6Kjo7WkCFDLnmOr776SgkJCQoNDdUrr7zyte83ZcoUjRs3TvHx8UpKStJVV13VpJ8HAAAAAJxk/GURoaSkJJuWllartm/fPsXFxdX7HJ9uzdNHqw+p8FSpItqHavj4nuo9LLqpW2207t27Ky0tTVFRUU63Age4+70GAAAA/IExZoe1NqmubYzYnqf3sGivDLIAAAAAgEsj2Pqg7Oxsp1sAAAAAAK/B4lEAAAAAAJ9GsAUAAAAA+DSCLQAAAADApxFsAQAAAAA+jWDrYYGBgUpMTFT//v01btw45efnS6paACo8PFyJiYnq27evfvKTn6iyslKff/65Bg0apMTERPXr10/PP/+861wRERFuvfeqVau0d+/epvw4AAAAAOB1CLYeFh4ervT0dH3yySdq3769nnvuOde2nj17Kj09XRkZGdq7d69WrVqlmJgYffTRR0pPT9fWrVs1d+5cHT16tEHvTbAFAAAA0BIQbM+zb8tmLX7gh/rdpHFa/MAPtW/L5iY9//Dhw5WTk3NRPSgoSFdffbUOHjyokJAQhYaGSpJKS0tVWVlZa98ZM2aoX79+SklJ0fHjxyVJhw4d0o033qjBgwcrOTlZ+/fv14cffqg1a9Zo1qxZSkxM1KFDh/TnP/9ZQ4YM0YABAzRx4kSdPXu2ST8fAAAAADiBYFtt35bN2rD4Dzpz4rhkrc6cOK4Ni//QZOG2oqJCGzdu1M0333zRtrNnz2rjxo2Kj4+XJB0+fFgJCQnq1q2bHnnkEXXu3FmSVFRUpKSkJO3Zs0fXXXedfvnLX0qS7rvvPj377LPasWOH5s+fr5/+9Ke6+uqrdfPNN2vevHlKT09Xz549deutt2r79u36+OOPFRcXpxdeeKFJPhsAAAAAOCnI6Qa8xZblS1V+rrRWrfxcqbYsX6q45JENPm9xcbESExOVk5OjuLg43XDDDa5thw4dUmJioowxGj9+vG666SZJUrdu3ZSRkaGjR49qwoQJuu2223T55ZcrICBAd955pyTp+9//vm699VYVFhbqww8/1O233+46b2lp7c9R45NPPtH//u//Kj8/X4WFhRozZkyDPxcAAAAAeAuCbbUzJ0+4Va+vmntsz549qzFjxui5557TQw89JOk/99heSufOndW/f39t2bJFt91220XbjTGqrKxUu3btvvY8Ne6++26tWrVKAwYM0JIlS/Tee+818FMBAAAAgPdgKnK1Nh2i3Kq7q1WrVnrmmWf0u9/9TuXl5Zfc78iRIyouLpYkffXVV3r//ffVp08fSVJlZaVWrFghSfrb3/6ma6+9Vm3btlWPHj3097//XZJkrdXHH39c1XubNjpz5ozr3GfOnFFMTIzKysq0bNmyJvlcAAAAAOA0gm215ElTFRQSWqsWFBKq5ElTm+w9Bg4cqISEBL3yyiuX3Gffvn0aNmyYBgwYoOuuu04zZ8503XvbunVrbdu2Tf3799emTZs0Z84cSdKyZcv0wgsvaMCAAerXr59Wr14tSZo0aZLmzZungQMH6tChQ3ryySc1bNgwXXPNNbrqqqua7HMBAAAAgJOMtdbpHppEUlKSTUtLq1Xbt2+f4uLi6n2OfVs2a8vypTpz8oTadIhS8qSpjbq/FvAEd7/XAAAAgD8wxuyw1ibVtY17bM8TlzySIAsAAAAAPoapyAAAAAAAn0awBQAAAAD4NIItAAAAAMCnEWwBAAAAAD6NYAsAAAAA8GkeDbbGmBuNMZnGmIPGmEfr2B5qjHm1evtWY0z36nqwMeYlY8xuY8w+Y8xsT/bpSYGBgUpMTFT//v11++236+zZs01y3jfffFMDBw7UgAED1LdvX/3pT3/SP//5Tw0fPrzWfuXl5br88st19OhR3X333erRo4cGDBig3r17a+rUqTpy5EiT9AMAAAAATvFYsDXGBEp6TtJNkvpKmmyM6XvBbvdK+spa+y1JCyT9prp+u6RQa228pMGSflwTen1NeHi40tPT9cknnygkJETPP/98vY+tqKios15WVqb77rtPa9eu1ccff6xdu3ZpxIgRSk5O1pEjR/T555+79n333XfVr18/de7cWZI0b948ffzxx8rMzNTAgQM1atQonTt3rnEfEgAAAAAc5MkR26GSDlprs6y15yQtlzT+gn3GS3qp+ucVklKMMUaSldTaGBMkKVzSOUmnPdirJKlo1zHlzt2mI49uUe7cbSradaxJz5+cnKyDBw/qvffe03e/+11X/cEHH9SSJUskSd27d9cjjzyiQYMG6e9//7s2bNig4cOHa9CgQbr99ttVWFioM2fOqLy8XB06dJAkhYaGqk+fPgoICNAdd9yh5cuXu869fPlyTZ48+aJejDGaMWOGoqOj9fbbbzfp5wQAAACA5uTJYNtF0uHzXh+prtW5j7W2XFKBpA6qCrlFknIlfSFpvrX21IVvYIy5zxiTZoxJO378eKOaLdp1TPkrD6giv1SSVJFfqvyVB5os3JaXl+vtt99WfHz8N+7boUMH7dy5U9dff71+9atf6d1339XOnTuVlJSk3//+92rfvr1uvvlmXXnllZo8ebKWLVumyspKSdLkyZNdwba0tFRvvfWWJk6ceMn3GjRokPbv398knxEAAAAAnOCti0cNlVQhqbOkHpIeNsbEXriTtXaxtTbJWpvUsWPHRr3h6fXZsmWVtc9fVqnT67Mbdd7i4mIlJiYqKSlJV1xxhe69995vPObOO++UJP373//W3r17dc011ygxMVEvvfSSa5rx//3f/2njxo0aOnSo5s+fr3vuuUeSlJSUpMLCQmVmZurtt9/WsGHD1L59+0u+l7W2UZ8PAAAAAJwW5MFz50jqdt7rrtW1uvY5Uj3tOFLSSUnfk/SOtbZM0jFjzAeSkiRlearZmpHa+tbrq+Ye2/MFBQW5RlglqaSkpNb21q1bS6oKnTfccINeeeWVOs8dHx+v+Ph4/eAHP1CPHj1c05lrRm337dtX5zTk8+3atUspKSlufioAAAAA8B6eHLHdLqmXMaaHMSZE0iRJay7YZ42ku6p/vk3SJls1hPiFpFGSZIxpLenbkjw6XzawXahb9ca48sortXfvXpWWlio/P18bN26sc79vf/vb+uCDD3Tw4EFJUlFRkT799FMVFhbqvffec+2Xnp6uK6+80vV68uTJ+utf/6pNmzZp/PgLb2uuYq3VM888o9zcXN14441N9+EAAAAAoJl5bMTWWltujHlQ0npJgZJetNbuMcY8ISnNWrtG0guSXjbGHJR0SlXhV6paTfkvxpg9koykv1hrMzzVqyS1HdNd+SsP1JqObIID1HZM9yZ/r27duumOO+5Q//791aNHDw0cOLDO/Tp27KglS5Zo8uTJKi2tGjn+1a9+pZiYGP32t7/Vj3/8Y4WHh6t169au0VpJiouLU+vWrTV48GDX6G+NWbNm6cknn9TZs2f17W9/W5s3b1ZISEiTf0YAAAAAaC7GX+6xTEpKsmlpabVq+/btU1xcXL3PUbTrmE6vz1ZFfqkC24Wq7Zjuaj2wU1O3CjSKu99rAAAAwB8YY3ZYa5Pq2ubJe2x9TuuBnQiyAAAAAOBjvHVVZAAAAAAA6oVgCwAAAADwaQRbAAAAAIBPI9gCAAAAAHwawRYAAAAA4NMIth5mjNHDDz/sej1//nylpqZKklJTU2WM0cGDB13bFy5cKGOMah5d1L17d8XHxysxMVGJiYn68MMPm7V/AAAAAPB2BFsPCw0N1cqVK3XixIk6t8fHx2v58uWu13//+9/Vr1+/Wvts3rxZ6enpSk9P19VXX+3RfgEAAADA1xBsz5ORkaEFCxYoNTVVCxYsUEZGRqPPGRQUpPvuu08LFiyoc/uECRO0evVqSdKhQ4cUGRmpqKioRr8vAAAAALQUBNtqGRkZWrt2rQoKCiRJBQUFWrt2bZOE2wceeEDLli1znft8bdu2Vbdu3fTJJ59o+fLluvPOOy/aZ+TIkUpMTNSwYcMa3QsAAAAA+BuCbbWNGzeqrKysVq2srEwbN25s9Lnbtm2rqVOn6plnnqlz+6RJk7R8+XKtWrVKt9xyy0Xba6Yib926tdG9AAAAAIC/IdhWq2s09evq7po+fbpeeOEFFRUVXbTtu9/9rl5++WVdccUVatu2bZO8HwAAAAC0FATbapGRkW7V3dW+fXvdcccdeuGFFy7a1qpVK/3mN7/RY4891iTvBQAAAAAtCcG2WkpKioKDg2vVgoODlZKS0mTv8fDDD19ydeRJkyZp0KBBTfZeAAAAANBSBDndgLdISEiQVHWvbUFBgSIjI5WSkuKqN1RhYaHr58svv1xnz551va55nu2F3nvvPdfP2dnZjXp/AAAAAPB3BNvzJCQkNDrIAgAAAACaF1ORAQAAAAA+jWALAAAAAPBpBFsAAAAAgE8j2AIAAAAAfBqLRwEAAABAC7Zvy2ZtWb5UZ06eUJsOUUqeNFVxySOdbsstjNh6WGBgoBITE11/GvL4nuzsbPXv3/+i+tmzZzVlyhTFx8erf//+uvbaa1VYWKiRI0dq/fr1tfZduHCh7r//fmVnZys8PFwDBw5UXFychg4dqiVLljTw0wEAAADwZfu2bNaGxX/QmRPHJWt15sRxbVj8B+3bstnp1tzCiK2HhYeHKz093SPnXrRokS6//HLt3r1bkpSZmang4GBNnjxZy5cv15gxY1z7Ll++XL/97W8lST179tSuXbskSVlZWbr11ltlrdUPf/hDj/QJAAAAwDttWb5U5edKa9XKz5Vqy/KlPjVqy4jteXLzVuuDD5K1cdO39MEHycrNW+2R9+nevbtOnDghSUpLS9OIESMkSampqbrnnns0YsQIxcbG6plnnrno2KysLA0cOFDbt29Xbm6uunTp4trWp08fhYaG6rbbbtO6det07tw5SVUjvkePHlVycvJF54uNjdXvf//7Ot8LAAAAgH87c/KEW3VvxYhttdy81dq//zFVVhZLkkpKj2r//sckSTHR4xt83uLiYiUmJkqSevTooTfeeONr99+/f782b96sM2fOqE+fPrr//vtd2zIzMzVp0iQtWbJEAwYMUHBwsEaPHq0VK1YoJSVFd911l3r16qX27dtr6NChevvttzV+/HgtX75cd9xxh4wxdb7noEGDtH///gZ/RgAAAAC+qU2HqKppyHXUfQkjttWyDs13hdoalZXFyjo0v1HnrZmKnJ6e/o2hVpLGjh2r0NBQRUVFqVOnTvryyy8lScePH9f48eO1bNkyDRgwQJKUmJiorKwszZo1S6dOndKQIUO0b98+SXJNR5aqpiFPnjz5ku9prW3UZwQAAADgm5InTVVQSGitWlBIqJInTXWoo4ZhxLZaSWmuW/XGCAoKUmVlZdX5S0pqbQsN/c+XKjAwUOXl5ZKkyMhIXXHFFXr//ffVt29f1z4RERG69dZbdeuttyogIEBvvfWW4uLiNH78eM2YMUM7d+7U2bNnNXjw4Ev2s2vXLsXFxTXlRwQAAADgA2ruo/X1VZEJttXCQmNUUnq0znpT6969u3bs2KGbbrpJr7/+er2OCQkJ0RtvvKExY8YoIiJC3/ve9/TBBx+ob9++uuyyy3Tu3Dnt3bvXdb9uRESERo4cqXvuuedrR2uzs7M1c+ZM/fd//3dTfDQAAAAAPiYueaTPBdkLEWyrxfacWeseW0kKCAhXbM+ZTf5ev/jFL3Tvvffq8ccfdwXR+mjdurXefPNN3XDDDYqIiFB+fr7uv/9+WWtVWVmpsWPHauLEia79J0+erFtuucU1JbnGoUOHNHDgQJWUlKhNmzZ66KGHdPfddzfRpwMAAACA5mX85f7KpKQkm5aWVqu2b98+t6bY5uatVtah+SopzVVYaIxie85s1MJRgCe4+70GAAAA/IExZoe1NqmubYzYnicmejxBFgAAAAB8DKsiAwAAAAB8GsEWAAAAAFqwgrVrdWBUivbF9dWBUSkqWLvW6ZbcxlRkAAAAAGihCtauVe7jc2SrH0NafvSoch+fI0mKHDfOydbcwogtAAAAALRQxxYsdIXaGrakRMcWLHSmoQYi2AIAAABAC1Wem+tW3VsRbJvBU089pX79+ikhIUGJiYnaunWrRowYoQsfT/Tee+/pu9/9riRpyZIlMsbo3XffdW1ftWqVjDFasWKFJGnEiBHq06ePEhMTFRcXp8WLF7v27d69u+Lj45WQkKDRo0crLy/vovp1112nzz//3NMfHwAAAICXCoqJcavurQi2HvbRRx/pzTff1M6dO5WRkaF3331X3bp1q9ex8fHxWr58uev1K6+8ogEDBtTaZ9myZUpPT9cHH3ygRx55ROfOnXNt27x5szIyMpSUlKRf//rXF9VHjBihX/3qV438hAAAAAB8VacZ02XCwmrVTFiYOs2Y7kxDDUSwPc/reaeU9OEexWxOV9KHe/R63qlGnzM3N1dRUVEKDQ2VJEVFRalz5871OjY5OVnbtm1TWVmZCgsLdfDgQSUmJta5b2FhoVq3bq3AwMCLtn3nO9/RwYMHL6oPHz5cOTk59f8wAAAAAPxK5LhxinnyCQV17iwZo6DOnRXz5BM+tXCUxKrILq/nndLMzMMqrrSSpCOlZZqZeViSNDG6fYPPO3r0aD3xxBPq3bu3rr/+et1555267rrr6nWsMUbXX3+91q9fr4KCAt1888367LPPau0zZcoUhYaG6sCBA1q4cGGdwfbNN99UfHz8RfV33nlHEyZMaNDnAgAAAOAfIseN87kgeyFGbKs9nZXrCrU1iiutns5q3E3TERER2rFjhxYvXqyOHTvqzjvv1JIlS+p9/KRJk7R8+XItX75ckydPvmj7smXLlJGRoS+++ELz58+vdc/syJEjlZiYqNOnT2v27Nm16l26dNHbb79d5zkBAAAAwJcwYlstp7TMrbo7AgMDNWLECI0YMULx8fF66aWX6n3s0KFDtXv3brVq1Uq9e/e+5H4dO3bUoEGDtHXrVl155ZWSqu6ljYqKumjfzZs3q127dpoyZYp+8Ytf6Pe//737HwoAAAAAvAQjttW6hAa7Va+vzMxMHThwwPU6PT3dFTzra+7cubUWf6rL2bNntWvXLvXs2bNe5wwKCtLChQu1dOlSnTrV+HuJAQAAAMApBNtqs2NjFB5gatXCA4xmxzZumevCwkLddddd6tu3rxISErR3716lpqZKksaOHauuXbuqa9euuv322y95jptuukkjR46sc9uUKVOUmJiowYMH6+6779bgwYPr3VtMTIwmT56s5557zq3PBAAAAADexFhrv3kvH5CUlGQvfC7svn37FBcXV+9zvJ53Sk9n5SqntExdQoM1OzamUQtHAZ7g7vcaAAAA8AfGmB3W2qS6tnGP7XkmRrcnyAIAAACAj2EqMgAAAADApxFsAQAAAAA+ze+Drb/cQwxIfJ8BAACAuvh1sA0LC9PJkycJA/AL1lqdPHlSYWFhTrcCAAAAeBW/Xjyqa9euOnLkiI4fP+50K0CTCAsLU9euXZ1uAwAAAPAqfh1sg4OD1aNHD6fbAAAAAAB4kF9PRQYAAAAA+D+CLQAAAADApxFsAQAAAAA+jWALAAAAAPBpBFsAAAAAgE8j2AIAAAAAfBrBFgAAAADg0wi2AAAAAACfRrAFAAAAAPg0gi0AAAAAwKcRbAEAAAAAPo1gCwAAAADwaQRbAAAAAIBPC3K6AQAAAAC40KpdOZq3PlNH84vVuV24Zo3powkDuzjdFrwUwRYAAACAV1m1K0ezV+5WcVmFJCknv1izV+6WJMIt6sRUZAAAAABeZd76TFeorVFcVqF56zMd6gjejmALAAAAwKsczS92qw4QbAEAAAB4lc7twt2qAwRbAAAAAF5l1pg+Cg8OrFULDw7UrDF9HOoI3o7FowAAAAB4lZoFolgVGfVFsAUAAADgdSYM7EKQRb0xFRkAAAAA4NMItgAAAADQgq3LWqfRK0Yr4aUEjV4xWuuy1jndktuYigwAAAAALdS6rHVK/TBVJRUlkqTcolylfpgqSRobO9bBztzDiC0AAAAAtFCLdi5yhdoaJRUlWrRzkUMdNQzBFgAAAABaqLyiPLfq3opgCwAAAAAtVHTraLfq3opgCwAAAAAt1LRB0xQWGFarFhYYpmmDpjnUUcOweBQAAAAAtFA1C0Qt2rlIeUV5im4drWmDpvnUwlESwRYAAAAAWrSxsWN9LsheyKNTkY0xNxpjMo0xB40xj9axPdQY82r19q3GmO7nbUswxnxkjNljjNltjAm78HgAAAAAADwWbI0xgZKek3STpL6SJhtj+l6w272SvrLWfkvSAkm/qT42SNJfJf3EWttP0ghJZZ7qFQAAAADguzw5YjtU0kFrbZa19pyk5ZLGX7DPeEkvVf+8QlKKMcZIGi0pw1r7sSRZa09aays82CsAAAAAwEd5Mth2kXT4vNdHqmt17mOtLZdUIKmDpN6SrDFmvTFmpzHmZ3W9gTHmPmNMmjEm7fjx403+AQAAAAAA3s9bH/cTJOlaSVOq/77FGJNy4U7W2sXW2iRrbVLHjh2bu0cAAAAAgBfwZLDNkdTtvNddq2t17lN9X22kpJOqGt39l7X2hLX2rKS3JA3yYK8AAAAAAB/lyWC7XVIvY0wPY0yIpEmS1lywzxpJd1X/fJukTdZaK2m9pHhjTKvqwHudpL0e7BUAAAAAWqTX804p6cM9itmcrqQP9+j1vFNOt+Q2jz3H1lpbbox5UFUhNVDSi9baPcaYJySlWWvXSHpB0svGmIOSTqkq/Mpa+5Ux5veqCsdW0lvW2nWe6hUAAAAAWqLX805pZuZhFVdaSdKR0jLNzKxaKmlidHsnW3OLqRog9X1JSUk2LS3N6TYAAAAAwGckfbhHR0ovfrJq19BgpV3dz4GOLs0Ys8Nam1TXNm9dPAoAAAAA4GE5dYTar6t7K4ItAAAAALRQXUKD3ap7K4ItAAAAALRQs2NjFB5gatXCA4xmx8Y41FHDeGzxKAAAAACAd6tZIOrprFzllJapS2iwZsfG+NTCURLBFgAAAABatInR7X0uyF6IqcgAAAAAAJ9GsAUAAAAA+DSCLQAAAADApxFsAQAAAAA+jWALAAAAAPBpBFsAAAAAgE8j2AIAAAAAfBrBFgAAAADg0wi2AAAAAACfRrAFAAAAAPg0gi0AAAAAwKcRbAEAAAAAPo1gCwAAAADwaQRbAAAAAIBPI9gCAAAAAHwawRYAAAAA4NMItgAAAAAAn0awBQAAAAD4NIItAAAAAMCnEWwBAAAAAD6NYAsAAAAA8GkEWwAAAACATyPYAgAAAAB8GsEWAAAAAODTCLYAAAAAAJ9GsAUAAAAA+DSCLQAAAADApxFsAQAAAAA+LcjpBgAAAADgQvu2bNaW5Ut15uQJtekQpeRJUxWXPNLptuClCLYAAAAAvMq+LZu1YfEfVH6uVJJ05sRxbVj8B0ki3KJOTEUGAAAA4FW2LF/qCrU1ys+VasvypQ51BG9HsAUAAADgVc6cPOFWHahXsDXGRHm6EQAAAACQpDYd6o4fl6oDXxtsjTHjjDHHJe02xhwxxlzdTH0BAAAAaKGSJ01VUEhorVpQSKiSJ011qCN4u29aPOopScnW2v3GmGGSfivpOs+3BQAAAKClqlkgilWRUV/fFGzLrbX7Jclau9UY06YZegIAAADQwsUljyTIot6+Kdh2Msb8z6VeW2t/75m2AAAAAACon28Ktn+W1OYSr61HOgIAAAAAwA1fG2yttb+81DZjzJCmbwcAAAAAAPe49RxbY0xfY8yTxpiDkv7ooZ4AAAAAtHAFa9fqwKgU7YvrqwOjUlSwdq3TLcGLfdNUZBljukuaXP2nTNKVkpKstdke7QwAAABAi1Swdq1yH58jW1IiSSo/elS5j8+RJEWOG+dka/BS3/Qc248krVNVAJ5orR0s6QyhFgAAAICnHFuw0BVqa9iSEh1bsNCZhuD1vmkq8peqWizqckkdq2ssGgUAAADAY8pzc92qA18bbK21EyTFS9ohKdUY85mky4wxQ5uhNwAAAAAtUFBMjFt14BsXj7LWFlhr/2KtHS3p25LmSFpgjDns8e4AAAAAtDidZkyXCQurVTNhYeo0Y7ozDcHrfePiUeez1n4p6VlJzxpjrvRMSwAAAABaspoFoo4tWKjy3FwFxcSo04zpLByFS/raYGuMWfMNx9/chL0AAAAAgKSqcEuQRX1904jtcEmHJb0iaask4/GOAAAAAABwwzcF22hJN6jqGbbfU9Wjf16x1u7xdGMAAAAAANTH1wZba22FpHckvWOMCVVVwH3PGPNLa+0fmqNBAAAAwJus2pWjeeszdTS/WJ3bhWvWmD6aMLCL020BLdo3Lh5VHWjHqirUdpf0jKQ3PNsWAAAA4H1W7crR7JW7VVxWIUnKyS/W7JW7JYlwCzjomxaPWiqpv6S3JP3SWvtJs3QFAAAAeKF56zNdobZGcVmF5q3PJNjCZxXtOqbT67NVkV+qwHahajumu1oP7OR0W275phHb70sqkjRN0kPGuNaOMpKstbatB3sDAAAAvMrR/GK36oC3K9p1TPkrD8iWVUqSKvJLlb/ygCT5VLgN+LqN1toAa22b6j9tz/vThlALAACAlqZzu3C36oC3O70+2xVqa9iySp1en+1MQw30tcEWAAAAwH/MGtNH4cGBtWrhwYGaNaaPQx0BjVOeX+pW3Vt94+JRAAAAAKrU3EfLqsjwFyWS6ppvUNLcjTQSwRYAAABww4SBXQiy8Bt7isqV2CpQQf9ZT0nl1mrP2Qr1crAvdzEVGQAAAABaqII2IUo/W6GzFVbWWp2tsEo/W6GCNiFOt+YWRmwBAAAAoIUaPr6nNi/br5wz5a5aUEiARo7v6WBX7iPYAgAAAEAL1XtYtCTpo9WHVHiqVBHtQzV8fE9X3VcQbAEAAACgBes9LNrnguyFuMcWAAAAAODTCLYAAAAAAJ9GsAUAAAAA+DSCLQAAAADAp7F4FAAAAAC0YBkZGdq4caMKCgoUGRmplJQUJSQkON2WWwi2AAAAANBCZWRkaO3atSorK5MkFRQUaO3atZLkU+GWqcgAAAAA0EJt3LjRFWprlJWVaePGjQ511DAEWwAAAABooQoKCtyqeyuCLQAAAAC0UJGRkW7VvRXBFgAAAABaqJSUFAUHB9eqBQcHKyUlxaGOGobFowAAAACghapZIIpVkQEAAAAAPishIcHnguyFmIoMAAAAAPBpBFsAAAAAgE8j2AIAAAAAfBrBFgAAAADg0zwabI0xNxpjMo0xB40xj9axPdQY82r19q3GmO4XbL/CGFNojJnpyT4BAAAAeJfX804p6cM9itmcrqQP9+j1vFNOtwQv5rFga4wJlPScpJsk9ZU02RjT94Ld7pX0lbX2W5IWSPrNBdt/L+ltT/UIAAAAwPu8nndKMzMP60hpmaykI6Vlmpl5mHCLS/LkiO1QSQettVnW2nOSlksaf8E+4yW9VP3zCkkpxhgjScaYCZI+k7THgz0CAAAA8DJPZ+WquNLWqhVXWj2dletQR/B2ngy2XSQdPu/1kepanftYa8slFUjqYIyJkPSIpF9+3RsYY+4zxqQZY9KOHz/eZI0DAAAAcE5OaZlbdcBbF49KlbTAWlv4dTtZaxdba5OstUkdO3Zsns4AAAAAeFSX0GC36oAng22OpG7nve5aXatzH2NMkKRISSclDZP0W2NMtqTpkn5ujHnQg70CAAAA8BKzY2MUHmBq1cIDjGbHxjjUEbxdkAfPvV1SL2NMD1UF2EmSvnfBPmsk3SXpI0m3SdpkrbWSkmt2MMakSiq01v7Bg70CAAAA8BITo9tLqrrXNqe0TF1CgzU7NsZVBy7ksWBrrS2vHmVdLylQ0ovW2j3GmCckpVlr10h6QdLLxpiDkk6pKvwCAAAAaOEmRrcnyKLeTNUAqe9LSkqyaWlpTrcBAAAAAPAAY8wOa21SXdu8dfEoAAAAAADqhWALAAAAAPBpBFsAAAAAgE8j2AIAAAAAfBrBFgAAAADg0wi2AAAAAACf5rHn2AIAAAD+aN+WzdqyfKnOnDyhNh2ilDxpquKSRzrdFtCiEWwBAACAetq3ZbM2LP6Dys+VSpLOnDiuDYv/IEmEW8BBTEUGAAAA6mnL8qWuUFuj/Fyptixf6lBHACSCLQAAAFBvZ06ecKsOoHkQbAEAAIB6atMhyq06gOZBsAUAAADqKXnSVAWFhNaqBYWEKnnSVIc6AiCxeBQAAABQbzULRLEqMuBdCLYAAACAGzrnF2rkvi9UnpuroJiz6pRf6HRLQItHsAUAAADqqWDtWuU+Pke2pESSVH70qHIfnyNJihw3zsnWgBaNe2wBAACAejq2YKEr1NawJSU6tmChMw0BkESwBQAAAOqtPDfXrTqA5kGwBQAAAOopKCbGrTqA5kGwBQAAAOqp04zpMmFhtWomLEydZkx3piEAklg8CgAAAKi3mgWiji1YWL0qcow6zZjOwlGAwwi2AAAAfmLVrhzNW5+po/nF6twuXLPG9NGEgV2cbsvvRI4bR5AFvAzBFgAAwA+s2pWj2St3q7isQpKUk1+s2St3SxLhFoDf4x5bAAAAPzBvfaYr1NYoLqvQvPWZDnUEAM2HYAsAAOAHjuYXu1UHAH9CsAUAAPADnduFu1UHvN26rHUavWK0El5K0OgVo7Uua53TLcGLEWwBAAD8wKwxfRQeHFirFh4cqFlj+jjUEdBw67LWKfXDVOUW5crKKrcoV6kfphJucUkEWwAAAD8wYWAXPX1rvLq0C5eR1KVduJ6+NZ6Fo+CTFu1cpJKKklq1kooSLdq5yKGO4O1YFRkAAMBPTBjYhSALv5BXlOdWHWDEFgAAAIBXiW4d7VYdINgCAAAA8CrTBk1TWGBYrVpYYJimDZrmUEfwdkxFBgAAAOBVxsaOlVR1r21eUZ6iW0dr2qBprjpwIYItAAAAAK8zNnYsQRb1xlRkAAAAAIBPI9gCAAAAAHwawRYAAAAA4NMItgAAAAAAn8biUQAAAAC8zqdb8/TR6kMqPFWqiPahGj6+p3oP4zm2qBvBFgAAAIBX+XRrnjYv26/yc5WSpMJTpdq8bL8kEW5RJ6YiAwAAAPAqH60+5Aq1NcrPVeqj1Ycc6gjejhFbAAAAwA2v553S01m5yiktU5fQYM2OjdHE6PZOt+VXCk+VulUHGLEFAAAA6un1vFOamXlYR0rLZCUdKS3TzMzDej3vlNOt+ZWI9qFu1QGCLQAAAFBPT2flqrjS1qoVV1o9nZXrUEf+afj4ngoKqR1VgkICNHx8T4c6grdjKjIAAABQTzmlZW7V0TA1C0SxKnIzyXhN2viEVHBEiuwqpcyREu5wuiu3EGwBAACAeuoSGqwjdYTYLqHBDnTj37qEBGh022BVVFYqsG2w2oYw2dQjMl6T1j4klRVXvS44XPVa8qlwy7cDAAAAqKfZsTEKDzC1auEBRrNjYxzqyD8V7Tqm/JUHVJFftVhURX6p8lceUNGuYw535oc2PvGfUFujrLiq7kMItgAAAEA9TYxur/l9uqlraLCMpK6hwZrfpxurIjex0+uzZctqP+7HllXq9PpsZxryZwVH3Kt7KaYiAwAAAG6YGN2eIOthNSO19a2jESK7Vk0/rqvuQxixBQAAAOBVAtvV/VifS9XRCClzpODw2rXg8Kq6DyHYAgAAAPAqbcd0lwmuHVVMcIDajunuTEP+LOEOadwzUmQ3Sabq73HP+NTCURJTkQEAQDNYtStH89Zn6mh+sTq3C9esMX00YWAXp9sC4KVaD+ykvYc/1T93fqBCW6IIE6brBl2rLgM7Od2af0q4w+eC7IUYsQUAAB61aleOZq/crZz8YllJOfnFmr1yt1btynG6NQBeKiMjQxt2/1OFKpGMVKgSbdj9T2VkZDjdGrwUwRYAAHjUvPWZKi6rqFUrLqvQvPWZDnUEwNtt3LhRZWW1nxdcVlamjRs3OtQRvB3BFgAAeNTR/GK36gBQUFDgVh3gHlsAAOBRnduFK6eOENu5XXgde6Mx9m3ZrC3Ll+rMyRNq0yFKyZOmKi55pNNtAW6LjIysM8RGRkY60A18ASO2AADAo2aN6aPw4MBatfDgQM0a08ehjvzTvi2btWHxH3TmxHHJWp05cVwbFv9B+7Zsdro1wG0pKSkKDg6uVQsODlZKSopDHcHbEWwBAIBHTRjYRU/fGq8u7cJlJHVpF66nb41nVeQmtmX5UpWfK61VKz9Xqi3LlzrUEdBwCQkJGjdunGuENjIyUuPGjVNCQoLDncFbMRUZAAB43ISBXQiyHnbm5Am36oC3S0hIIMii3hixBQAA8ANtOkS5VQcAf0KwBQAA8APJk6YqKCS0Vi0oJFTJk6Y61BEANB+CLQAAgB+ISx6pa4dco/CKSslahVdU6toh17AqMoAWgXtsAQAA/EDB2rVq/eLLGllS4qqZgy+roEcvRY4b52BnAOB5jNgCAAD4gWMLFsqeF2olyZaU6NiChc40BADNiGALAADgB8pzc92qA4A/IdgCAAD4gaCYGLfqAOBPCLYAAAB+oNOM6TJhYbVqJixMnWZMd6YhAGhGLB4FAADgB2oWiDq2YKHKc3MVFBOjTjOms3AUgBaBYAsAAOAnIseNI8gCaJGYigwAAAAA8GkEWwAAAACATyPYAgAAAAB8GvfYAgAA+InX807p6axc5ZSWqUtosGbHxmhidHun2wIAjyPYAgAA+IHX805pZuZhFVdaSdKR0jLNzDwsSYRbAH6PqcgAAAB+4OmsXFeorVFcafV0Vq5DHQFA8yHYAgAA+IGc0jK36gDgTwi2AAAAfqBLaLBbdQDwJwRbAAAAPzA7NkbhAaZWLTzAaHZsjEMdAUDzYfEoAAAAP1CzQBSrIgNoiQi2AAAAfmJidHuCLIAWianIAADA4wrWrtWBUSnaF9dXB0alqGDtWqdbAgD4EUZsAQCARxWsXavcx+fIlpRIksqPHlXu43MkSZHjxjnZGtAgn27N00erD6nwVKki2odq+Pie6j0s2um2gBaNEVsAAOBRxxYsdIXaGrakRMcWLHSmIaARPt2ap8zlmRpeVqGbI4M0vKxCmcsz9enWPKdbA1o0RmwBAIBHlefmKqddhDJj2qskOEhhZeXqk3tKXXJznW4NcFvWGwcUH2IUZKpWoG4VKMUHSPvfOMCoLeAgj47YGmNuNMZkGmMOGmMerWN7qDHm1ertW40x3avrNxhjdhhjdlf/PcqTfQIAAM/J695Vu7t1VElIsGSMSkKCtbtbR+V17+p0a4DbYiusK9TWCDJGsRXWoY78WMZr0oL+Umq7qr8zXnO6I3gxjwVbY0ygpOck3SSpr6TJxpi+F+x2r6SvrLXfkrRA0m+q6yckjbPWxku6S9LLnuoTAAB41qcx7VUZUPufHJUBAfo0htV74XvCA41bdTRQxmvS2oekgsOSbNXfax8i3OKSPDliO1TSQWttlrX2nKTlksZfsM94SS9V/7xCUooxxlhrd1lrj1bX90gKN8aEerBXAADgIYVni9yqA96sNLTSrToaaOMTUllx7VpZcVUdqIMng20XSYfPe32kulbnPtbackkFkjpcsM9ESTuttaUXvoEx5j5jTJoxJu348eNN1jgAoOVYtStH18zdpB6PrtM1czdp1a4cp1vyO206RLlVB7zZS51XqVRltWqlKtNLnVc505C/KjjiXh0tnlcvHmWM6aeq6cmj69purV0sabEkJSUlcWMDAMAtq3blaPbK3Souq5Ak5eQXa/bK3ZKkCQMv/F0sGip50lRtWPwHlZ/7z++og0JClTxpqoNdAQ2zOmyjznY4p06FnVWsMoUrWMcijuofYVv0pNPN+ZPIrtXTkOuoA3Xw5IhtjqRu573uWl2rcx9jTJCkSEknq193lfSGpKnW2kMe7BMA0ELNW5/pCrU1issqNG99pkMd+ae45JEafd+DahPVUTJGbaI6avR9DyoueaTTrQFu61/WX+FnL1OxKZOMVGzKFH72MvUv6+90a/4lZY4UHF67FhxeVQfq4MkR2+2SehljeqgqwE6S9L0L9lmjqsWhPpJ0m6RN1lprjGknaZ2kR621H3iwRwBAC3Y0v9itOhpub68BWjxlpnJKy9QlNFgdYmMU53RTQAP0/6q/ztlztWpBNkj9vyLYNqmEO6r+3vhE1fTjyK5VobamDlzAY8HWWltujHlQ0npJgZJetNbuMcY8ISnNWrtG0guSXjbGHJR0SlXhV5IelPQtSXOMMTW/lhltrT3mqX4BAC1P53bhyqkjxHZuF17H3mio1/NOaWbmYRVXVt01dKS0TDMzq6YYToxmZWT4lnNF59yqoxES7iDIot48+hxba+1b1tre1tqe1tqnqmtzqkOtrLUl1trbrbXfstYOtdZmVdd/Za1tba1NPO8PoRYA0KRmjemj8ODAWrXw4EDNGtPHoY7809NZua5QW6O40urprFyHOgIaLjIy0q06gObh0WALAGgYVuptHhMGdtHTt8arS7twGUld2oXr6VvjWTiqieWUlrlVB7xZSkqKgoODa9WCg4OVkpLiUEcAJC9fFRkAWiJW6m1eEwZ24bp6WJfQYB2pI8R2CQ2uY2/AuyUkJEiSNm7cqIKCAkVGRiolJcVVB+AMgi0AeJmvW6mXAAZfNDs2ptY9tpIUHmA0OzbGwa6AhktISCDIAl6GYAsAXoaVeuFvahaIejor17Uq8uzYGBaOAgA0GYItAHgZVuqFP5oY3Z4gCwDwGBaPAgAvw0q9AAAA7mHEFgC8TM19tPPWZ+pofrE6twvXrDF9uL8WAADgEgi2AOCFWKkXAACg/piKDAAAAADwaQRbAAAAAIBPI9gCAAAAAHwawRYAAAAA4NMItgAAAAAAn0awBQAAAAD4NIItAAAAAMCnEWwBAAAAAD4tyOkGAPiWVbtyNG99po7mF6tzu3DNGtNHEwZ2cbotoMEK1q7VsQULVZ6bq6CYGHWaMV2R48Y53RYAAHADwRZAva3alaPZK3eruKxCkpSTX6zZK3dLEuEWPqlg7Vql/fZp7Y9qo5KoHgorK9dVv31aSRLhFsClZbwmbXxCKjgiRXaVUuZICXc43RXQojEVGUC9zVuf6Qq1NYrLKjRvfaZDHQGNs/P/e1YZ0e1UEhIsGaOSkGBlRLfTzv/vWadbA+CtMl6T1j4kFRyWZKv+XvtQVR2AYxixBVBvR/OL3aoD3m5vmNEnvRO1ZdgNOh3RTm0L85W89R8K2btDI51uDoB32viEVHbB/+6VFVfVGbUFHEOwBVBvnduFK6eOENu5XbgD3QCNtzNusNZfN0HlwSGSpNNtLtP66yZI1tm+gIYq2nVMp9dnqyK/VIHtQtV2THe1HtjJ6bb8S8ER9+oAmgVTkQHU26wxfRQeHFirFh4cqFlj+jjUEdA47w8f4wq1NcqDQ/T+8DEOdQQ0XNGuY8pfeUAV+aWSpIr8UuWvPKCiXccc7szPRHZ1rw6gWRBsAdTbhIFd9PSt8erSLlxGUpd24Xr61ngWjoLPOh3R1q064M1Or8+WLausVbNllTq9PtuZhvxVyhwp+IKZSsHhVXUAjmEqMgC3TBjYhSDbDP769kbNKzE6FtlOnQryNSvM6vs3pTjdlt/pEhqiI6VlddYBX1ORX6qDAblKC8pSoSlRhA1TUnmsvpUf43Rr/qXmPlpWRQa8CsEWALzMX9/eqMcCIlTaLlSS9GW79nrsXKn09kbCbRObHRujmZmHVVz5n5tqwwOMZscSBOB7DoQd0/t2vypM1ahtoSnRluD9siZQTJJtYgl3EGQBL8NUZADwMvNKjEpDQmvVSkNCNa/EONSR/5oY3V7z+3RT19BgGUldQ4M1v083TYxu73RrgNs+qjzgCrU1KkylPqo84FBHANB8GLEFAC9zLLKdW3U0zsTo9gRZ+IVzpsStOgD4E0ZsAcDLdCrId6sOAJIUaEPdqgOAPyHYNoNVu3J0zdxN6vHoOl0zd5NW7cpxuiUAXmxWmFXIudJatZBzpZoVxsNVAVza0IDDUuUF/7SrDKiqA4CfI9h62KpdOZq9crdy8otlJeXkF2v2yt2EWwCXNDgiQGPef1Ntz3wlWau2Z77SmPff1OAI/pMN4NLG6G8aXnZQgRVBkpUCK4I0vOygxuhvTrcGAB7HPbYeNm99porLKmrVissqNG99Jo9MaWKrduVo3vpMHc0vVud24Zo1pg/XGD5py/KluurEcV316a7a9VNHFJc80qGugMYp2nVMp9dnqyK/VIHtQtV2THe1HtjJ6bb8S2RXjSlYrTE1rwOr/0R2c64nAGgm/Prfw47mF7tVR8MwMg5/cubkCbfqgLcr2nVM+SsPqCK/aop9RX6p8lceUNGuYw535mdS5kjB4bVrweFVdQDwcwRbD+vcLtytOhrm60bGAV/TpkOUW3XA251eny1bVvsxNLasUqfXZzvTkL9KuEMa90z1CK2p+nvcMzxvFUCLQLD1sFlj+ig8OLBWLTw4ULPG9HGoI//EyDj8SfKkqQq64Dm2QSGhSp401aGOgMapGamtbx0Nty6itUZ366yEHldodLfOWhfR2umWAKBZcI+th9Xc48m9n57VuV24cuoIsYyMwxfV3Ee7ZflSnTl5Qm06RCl50lTur4XPCmwXWmeIDWzHY2ia0rqsdUr9MFUlFVXPrc0tylXqh6mSpLGxYx3sDAA8z1jrH4+PSEpKsmlpaU63AYfU3GN7/nTk8OBAPX1rPL9EaGKv553S01m5yiktU5fQYM2OjdHE6PZOtwXAixXtOqa0N/6l7TqoQlOiCBumIfqWkm75DgtINaHRK0Yrtyj3onpM6xhtuG2DAx0BQNMyxuyw1ibVtY0RW/gFRsabx+t5pzQz87CKK6t+IXaktEwzM6uej0i4BXAphwLztCVwj8qrf5leaEq0xezRZYG9lSCCbVPJK8pzqw4A/oRgC78xYWAXgqyHPZ2V6wq1NYorrZ7OyiXYArikjW+vcYXaGuXWauPba5SQkOBQV/4nunV0nSO20a2jHegGAJoXi0cBqLecknNu1QFAkgqKy9yqo2GmDZqmsMCwWrWwwDBNGzTNoY4AoPkwYgug3joVfKUv2108Mtup4CsHugHgK1pVntPZgIsXimpVyS/FmlLNAlGLdi5SXlGeoltHa9qgaSwcBaBFINgCqLcfrfyb5k/5L5WG/ucfqKGlpfrRyr9Jt4xysDMA3qxVYU+djTgqBZz3LNvKALUq7OlcU35qbOxYgiyAFompyADq7caczzRz2WJdfvK4jK3U5SePa+ayxbox5zOnWwPgxczZHmpzupcCykMlKwWUh6rN6V4yZ3s43RoAwE8wYgug3jrNmK4bHp+j67d/6KqZsDB1evIJB7sC4O0i2odJpy5XWMnlF9R5ji0AoGkwYgug3iLHjVPMk08oqHNnyRgFde6smCefUOS4cU63BsCLDR/fU0Ehtf/JERQSoOHjmYoMAGgajNgCcEvkuHEEWfiXjNekjU9IBUekyK5Syhwp4Q6nu/IrvYdVPW7mo9WHVHiqVBHtQzV8fE9XHQCAxiLYAnDLvi2btWX5Up05eUJtOkQpedJUxSWPdLotoGEyXpPWPiSVFVe9Ljhc9Voi3Dax3sOiCbIAAI9hKjKAetu3ZbM2LP6Dzpw4LlmrMyeOa8PiP2jfls1OtwY0zMYn/hNqa5QVV9UBAIDPYMQWQL1tWb5U5edKa9XKz5Vqy/KljNrCNxUc0aGz96lS/09hJlAltkIBeks99WenOwMAAG4g2AKotzMnT7hVRyNw32ezOFQ+Q4FmpEKNkSSFmyCV23E6VB4hljUCAMB3EGzhN17PO6Wns3KVU1qmLqHBmh0bo4nR7Z1uy6+06RBVNQ25jjqaEPd9NpvKshE6HJintKAsFZoSRdgwJZXHqkvZCKdbAwAAbuAeW/iF1/NO6eG92TpSWiYr6UhpmR7em63X80453ZpfSZ40VUEhtZ87GRQSquRJUx3qyE9x32ezORL4pbYE71dhQIlkpMKAEm0J3q8jgV863RoAAHADI7bwC0/tyVJJQO2vc4kJ0FN7shi1bUI199GyKrKHFRxRUfl1Ol1+lyoUpUCdUNugl9S64F9Od+Z3tgdlqcJU1qpVmEptD8oS32oAAHwHwRZ+IdcEulVHw8UljyTIelhRyC3KL/merMIkSRXqpPzy/5ZadVBrh3vzN2cDStyqAwAA78RUZPiFTqfqXrzoUnU0QsZr0oL+Umq7qr8zXnO6I79zunyqK9TWsArT6XKmfDe1yMhIt+pouHVZ6zR6xWglvJSg0StGa13WOqdbAgD4EYIt/MJ9/1yv0NLaj6EJLS3Vff9c71BHfqpmUaOCw5LsfxY1Itw2qYqzdU+muVQdDZeSkqLg4OBateDgYKWkpDjUkX9al7VOqR+mKrcoV1ZWuUW5Sv0wlXALAGgyBFv4he+Pvk4zX1uiy08el7GVuvzkcc18bYm+P/o6p1vzLyxq1Cwqw+sOsJeqo+ESEhI0btw41whtZGSkxo0bp4SEBIc78y+Ldi5SSUXt6d0lFSVatHORQx0BAPwN/0pqBjyGxvMix43TVEk3Lpir8txcBcXEqNOM6YocN87p1vxLwRH36miQvcXlCg7I1a7zHkEzsDxWZcUxusLp5vxQQkICQdbD8ory3KoDAOAugq2HvZ53SjMzD6u40kqqegzNzMzDkkS4bWJH20VoS9wVOtOpVdVqve0ixF1yTSyya/U05DrqaDJ7So6oMPKAbPVqvYWmRO8H71dEQYVudLg3oCGiW0crtyi3zjoAAE2Bqcge9nRWrivU1iiutHo66+L/gUfD7duyWRsW/0FnThyXrNWZE8e1YfEftG/LZqdb8y8pc1Rkb1BuyYs6UrJGuSUvqsjeIKXMcbozv3I28nNXqK1hTaXORn7uUEdA40wbNE1hgbUXRAsLDNO0QdMc6ggA4G8YsfWwnNIyt+pomC3Ll6r8XO3Fo8rPlWrL8qU8mqYJFVWMUH755bLVvxNzPYamIo7H0DShClP3o2YuVQe83djYsZKq7rXNK8pTdOtoTRs0zVUHAKCxCLYeFlNZrqMBF1/mmMpyB7rxX2dOHHerjoY5vT5bB+yXSgv5z72fSeWx6rM+XK0HdnK6Pb8RGRmpgoKCOuuArxobO5YgCwDwGKYie9i9q5bX+Riae1ctd6gj/9QmpO5fFFyqjobJPJ2tLcH7VRhQIhmpMKBEW4L3K/N0ttOt+RUeQQMAAOAegq2HjXp3nWYuW1z7MTTLFmvUuzy7ryklRx1UkKmoVQsyFUqOOuhQR/4pLSRLFRfc+1lhKpUWkuVQR/6JR9AAAAC4h6nIHhbUWrp++4e6fvuHtesRDjXkp+KuCJN0QFuOddeZ8lC1CSpVcqfs6jqaSqHqvsfzUnU0HI+gaT7rstZx7ycAAD6OYOthnfp/pdztkbIV/xkcN4GV6tT/4vvn0Agpc3TF669pXOhkVShKgTqhtqGvSCl3ON2ZX2kVFqGzJYV11gFftC5rnVI/TFVJRdUvZ3KLcpX6YaokEW4BAPAhBFsPixwQJemEjmW0UfnZQAW1qlCnhDPVdTSVoooRSquw2h5yQIVmtyJsmIZU3K6kihGs1tuEWhV219mAvVLAedORKwPUqrC7Yz0BjbFo5yJXqK1RUlGiRTsXEWwBAPAh3GPraSlzdDSqvTbHXam3BvTU5rgrdTSqPc/9bGI71n2gfwVm1lrU6F+Bmdqx7gOnW/Mr5kR7tTndSwHloZKVAspD1eZ0L5kT7Z1uDWiQvKI8t+oAAMA7MWLrYfsKOmpDbi+Vl1ctbHSmPEwbcntJBR0V53Bv/mTbuUxVBFy8qNG2c5n6jkM9+aOI9qHSqcsVVnL5xXXAB0W3jlZuUW6ddQAA4DsYsfWwLcuXukJtjfLyCm1ZvtShjvxTYcAlFjW6RB0NM3x8TwWF1P7PRlBIgIaP7+lQR/5rXdY6jV4xWgkvJWj0itFal8VK6p4wbdA0hQXWXmQuLDBM0wZNc6gjAADQEIzYetiZkyfcqqNh2oZH6HTxxYsatQ1nUaOm1HtY1SjWR6sPqfBUqSLah2r4+J6uOpoGCxo1n5rryarIAAD4NoKth7XpECVjLtdX7dvqrDmnVjZEl506LWu/dLo1v9Kv1zD9++PNsuc9Y9XYAPXrNczBrvxT72HRBFkPY0Gj5jU2dizXFQAAH8dUZA/rnnCj8jq00tmAc5KRzgacU16HVuqecKPTrfmVvLQARRTUXtQooqCX8tL4isP3sKARAACAexix9bCM3M9UYS5e1Cgj9zONdqgnf1R4qlRhunhRo8KSUoc6AhqOBY0AAADcw3CWhxXaSyxqdIk6GuZSq/KyWi98EQsaAQAAuIdg62GtTZhbdTQMq/U2H1br9byxsWOVenWqYlrHyMgopnWMUq9O5T5QAACAS2AqsodFn4tVdvD+WtORA22AostiHezK/7Bab/Ngtd7mw4JGAAAA9Uew9bDTp6LUpXVvnWj9mc6aUrWyoYoq6qHTRVFOt+Z3WK3X81itFwAAAN6IYOthEe1DVXiqk1oXdVLr6lqxuPcTvonVegEAAOCNuMfWw7j3E/7kUqvyslovAAAAnESw9bDew6I1cspVrhHaiPahGjnlKqbMegCLGnkeq/UCAADAGzEVuRkc6LhDywYtUl5RnqJbRyuq4zT1FvcjNiUWNWoeNddy0c7/fJ+nDZrGNQYAAICjjLXW6R6aRFJSkk1LS3O6jYtcGLikqhEuHt3RtEavGK3cotyL6jGtY7Thtg0OdAQAAACgKRljdlhrk+ra5tERW2PMjZIWSQqU9H/W2rkXbA+VtFTSYEknJd1prc2u3jZb0r2SKiQ9ZK1d78lePYVVZJsHixo1n9y81co6NF8lpbkKC41RbM+Zioke73Rbfofr3Hy41s2D69w8uM7Ng+vcPLjOzccfrrXHgq0xJlDSc5JukHRE0nZjzBpr7d7zdrtX0lfW2m8ZYyZJ+o2kO40xfSVNktRPUmdJ7xpjeltrKzzVr6fkFeVpUKsyfTeyXJcFWn1VYfRmQZB2EbiaVHTraMXYLy66zrnmCqdb8yu5eau1f/9jqqwsliSVlB7V/v2PSZLP/cfPm3Gdmw/XunlwnZsH17l5cJ2bB9e5+fjLtfbk4lFDJR201mZZa89JWi7pwiszXtJL1T+vkJRijDHV9eXW2lJr7WeSDlafz+eMvKy1Jl1WpvZBVsZI7YOsJl1WppGXtf7mg1FvM3pfXed1ntH7aqdb8ytZh+a7/qNXo7KyWFmH5jvUkX/iOjcfrnXz4Do3D65z8+A6Nw+uc/Pxl2vtyWDbRdLh814fqa7VuY+1tlxSgaQO9TzWJ3w3skwXPO1HIQFVdTSdtmc21nmd257Z6ExDfqqk9OL7mL+ujobhOjcfrnXz4Do3D65z8+A6Nw+uc/Pxl2vt04/7McbcZ4xJM8akHT9+3Ol26hRQke9WHQ3jL/8P6e3CQmPcqqNhuM7Nh2vdPLjOzYPr3Dy4zs2D69x8/OVaezLY5kjqdt7rrtW1OvcxxgRJilTVIlL1OVbW2sXW2iRrbVLHjh2bsPWm4y9fFG/HdW4esT1nKiAgvFYtICBcsT1nOtSRf+I6Nx+udfPgOjcPrnPz4Do3D65z8/GXa+3JYLtdUi9jTA9jTIiqFoNac8E+ayTdVf3zbZI22arnD62RNMkYE2qM6SGpl6RtHuzVY/zli+LtuM7NIyZ6vK666imFhXaWZBQW2llXXfWUTy0s4Au4zs2Ha908uM7Ng+vcPLjOzYPr3Hz85Vp79Dm2xpj/J2mhqh7386K19iljzBOS0qy1a4wxYZJeljRQ0ilJk6y1WdXHPibpHknlkqZba9/+uvfy1ufYSv6xfLYv4DoDAAAA/uvrnmPr0WDbnLw52AIAAAAAGufrgq1PLx4FAAAAAADBFgAAAADg0wi2AAAAAACfRrAFAAAAAPg0gi0AAAAAwKcRbAEAAAAAPo1gCwAAAADwaQRbAAAAAIBPI9gCAAAAAHwawRYAAAAA4NMItgAAAAAAn0awBQAAAAD4NIItAAAAAMCnEWwBAAAAAD6NYAsAAAAA8GkEWwAAAACATyPYAgAAAAB8mrHWOt1DkzDGHJf0udN9fIMoSSecbgJoInyf4W/4TsOf8H2GP+H7jBpXWms71rXBb4KtLzDGpFlrk5zuA2gKfJ/hb/hOw5/wfYY/4fuM+mAqMgAAAADApxFsAQAAAAA+jWDbvBY73QDQhPg+w9/wnYY/4fsMf8L3Gd+Ie2wBAAAAAD6NEVsAAAAAgE8j2DYDY8yNxphMY8xBY8yjTvcDNIYxppsxZrMxZq8xZo8xZprTPQGNZYwJNMbsMsa86XQvQGMZY9oZY1YYY/YbY/YZY4Y73RPQUMaYGdX/3vjEGPOKMSbM6Z7gnQi2HmaMCZT0nKSbJPWVNNkY09fZroBGKZf0sLW2r6RvS3qA7zT8wDRJ+5xuAmgiiyS9Y629StIA8d2GjzLGdJH0kKQka21/SYGSJjnbFbwVwdbzhko6aK3Nstaek7Rc0niHewIazFqba63dWf3zGVX9g6mLs10BDWeM6SpprKT/c7oXoLGMMZGSviPpBUmy1p6z1uY72hTQOEGSwo0xQZJaSTrqcD/wUgRbz+si6fB5r4+IEAA/YYzpLmmgpK0OtwI0xkJJP5NU6XAfQFPoIem4pL9UT6//P2NMa6ebAhrCWpsjab6kLyTlSiqw1m5wtit4K4ItgAYxxkRIel3SdGvtaaf7ARrCGPNdScestTuc7gVoIkGSBkn6o7V2oKQiSazvAZ9kjLlMVTMde0jqLKm1Meb7znYFb0Ww9bwcSd3Oe921ugb4LGNMsKpC7TJr7Uqn+wEa4RpJNxtjslV1q8goY8xfnW0JaJQjko5Ya2tm0qxQVdAFfNH1kj6z1h631pZJWinpaod7gpci2Hredkm9jDE9jDEhqrrhfY3DPQENZowxqrp3a5+19vdO9wM0hrV2trW2q7W2u6r++7zJWstoAHyWtTZP0mFjTJ/qUoqkvQ62BDTGF5K+bYxpVf3vjxSxGBouIcjpBvydtbbcGPOgpPWqWsntRWvtHofbAhrjGkk/kLTbGJNeXfu5tfYt51oCAJznvyUtq/6FepakHzrcD9Ag1tqtxpgVknaq6qkMuyQtdrYreCtjrXW6BwAAAAAAGoypyAAAAAAAn0awBQAAAAD4NIItAAAAAMCnEWwBAAAAAD6NYAsAAAAA8GkEWwAAmogxprsx5pMLaqnGmJkeeK/LjTF/M8ZkGWN2GGM+Msbc0tTvAwCALyDYAgDg5YwxQRe8NpJWSfqXtTbWWjtY0iRJXb/pWAAA/BHBFgCAZmKMecgYs9cYk2GMWV5da22MedEYs80Ys8sYM766frcxZo0xZpOkjRecapSkc9ba52sK1trPrbXP1nWsMaa9MWZV9fv+2xiTUL1frdFkY8wn1aPO3Y0x+40xy4wx+4wxK4wxrTx7dQAAaDh+iwsAQPN5VFIPa22pMaZdde0xSZustfdU17YZY96t3jZIUoK19tQF5+knaec3vJfrWGPMs5J2WWsnGGNGSVoqKfEbju8j6V5r7QfGmBcl/VTS/G/+iAAAND9GbAEAaDr2G+oZkpYZY74vqby6NlrSo8aYdEnvSQqTdEX1tn/UEWovYox5zhjzsTFm+3nl84+9VtLLkmSt3SSpgzGm7Tec9rC19oPqn/9afQ4AALwSwRYAgKZzUtJlF9TaSzpR/fNYSc+pajR1e/X9r0bSRGttYvWfK6y1+6r3L7rE++ypPockyVr7gKQUSR3P2+dSx56vXLX/LRB23s8XhvRLhXYAABxHsAUAoIlYawsl5VZP95Uxpr2kGyW9b4wJkNTNWrtZ0iOSIiVFSFov6b+rF4SSMWZgPd5qk6QwY8z959W+7h7YLZKmVJ9/hKQT1trTkrJVHZCNMYMk9TjvmCuMMcOrf/6epPfr0RcAAI4g2AIA0LSmSnq8emrxJkm/tNYekhQo6a/GmN2Sdkl6xlqbL+lJScGSMowxe6pffy1rrZU0QdJ1xpjPjDHbJL2kqsBcl1RJg40xGZLmSrqruv66pPbV7/ugpE/POyZT0gPGmH2qGoX+Y70+PQAADjBV/9sIAABQxRjTXdKb1tr+TvcCAEB9MGILAAAAAPBpjNgCAAAAAHwaI7YAAAAAAJ9GsAUAAAAA+DSCLQAAAADApxFsAQAAAAA+jWALAAAAAPBpBFsAAAAAgE/7/wEwfWCzQpgZQgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "_ = plt.figure(figsize=(16, 9))\n",
    "for label, recommender in recommender_object_dict.items():\n",
    "    results = MAP_recommender_per_group[label]\n",
    "    plt.scatter(x=np.arange(0,len(results)), y=results, label=label)\n",
    "plt.ylabel('MAP')\n",
    "plt.xlabel('User Group')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1492.312173,
   "end_time": "2022-11-26T22:07:33.263391",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-26T21:42:40.951218",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
